<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Big Data and Social Science</title>
  <meta name="description" content="Big Data and Social Science">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Big Data and Social Science" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="Coleridge-Initiative/big-data-and-social-science" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Big Data and Social Science" />
  
  
  

<meta name="author" content="Ian Foster">
<meta name="author" content="Rayid Ghani">
<meta name="author" content="Ron S. Jarmin">
<meta name="author" content="Frauke Kreuter">
<meta name="author" content="Julia Lane">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="chap-intro.html">
<link rel="next" href="chap-link.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Big Data and Social Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="chap-intro.html"><a href="chap-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-1"><i class="fa fa-check"></i><b>1.1</b> Why this book?</a></li>
<li class="chapter" data-level="1.2" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-2"><i class="fa fa-check"></i><b>1.2</b> Defining big data and its value</a></li>
<li class="chapter" data-level="1.3" data-path="chap-intro.html"><a href="chap-intro.html#sec:1.3"><i class="fa fa-check"></i><b>1.3</b> Social science, inference, and big data</a></li>
<li class="chapter" data-level="1.4" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-5"><i class="fa fa-check"></i><b>1.4</b> Social science, data quality, and big data</a></li>
<li class="chapter" data-level="1.5" data-path="chap-intro.html"><a href="chap-intro.html#new-tools-for-new-data"><i class="fa fa-check"></i><b>1.5</b> New tools for new data</a></li>
<li class="chapter" data-level="1.6" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-6"><i class="fa fa-check"></i><b>1.6</b> The book’s “use case”</a></li>
<li class="chapter" data-level="1.7" data-path="chap-intro.html"><a href="chap-intro.html#the-structure-of-the-book"><i class="fa fa-check"></i><b>1.7</b> The structure of the book</a><ul>
<li class="chapter" data-level="1.7.1" data-path="chap-intro.html"><a href="chap-intro.html#part-i-capture-and-curation"><i class="fa fa-check"></i><b>1.7.1</b> Part I: Capture and curation</a></li>
<li class="chapter" data-level="1.7.2" data-path="chap-intro.html"><a href="chap-intro.html#part-ii-modeling-and-analysis"><i class="fa fa-check"></i><b>1.7.2</b> Part II: Modeling and analysis</a></li>
<li class="chapter" data-level="1.7.3" data-path="chap-intro.html"><a href="chap-intro.html#part-iii-inference-and-ethics"><i class="fa fa-check"></i><b>1.7.3</b> Part III: Inference and ethics</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="chap-intro.html"><a href="chap-intro.html#sec:intro:resources"><i class="fa fa-check"></i><b>1.8</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chap-web.html"><a href="chap-web.html"><i class="fa fa-check"></i><b>2</b> Working with Web Data and APIs</a><ul>
<li class="chapter" data-level="2.1" data-path="chap-web.html"><a href="chap-web.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="chap-web.html"><a href="chap-web.html#sec:4-1"><i class="fa fa-check"></i><b>2.2</b> Scraping information from the web</a><ul>
<li class="chapter" data-level="2.2.1" data-path="chap-web.html"><a href="chap-web.html#sec:4-1.1"><i class="fa fa-check"></i><b>2.2.1</b> Obtaining data from the HHMI website</a></li>
<li class="chapter" data-level="2.2.2" data-path="chap-web.html"><a href="chap-web.html#sec:4-1.2"><i class="fa fa-check"></i><b>2.2.2</b> Limits of scraping</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="chap-web.html"><a href="chap-web.html#sec:4-2"><i class="fa fa-check"></i><b>2.3</b> New data in the research enterprise</a></li>
<li class="chapter" data-level="2.4" data-path="chap-web.html"><a href="chap-web.html#sec:4-3"><i class="fa fa-check"></i><b>2.4</b> A functional view</a><ul>
<li class="chapter" data-level="2.4.1" data-path="chap-web.html"><a href="chap-web.html#sec:4-3.1"><i class="fa fa-check"></i><b>2.4.1</b> Relevant APIs and resources</a></li>
<li class="chapter" data-level="2.4.2" data-path="chap-web.html"><a href="chap-web.html#sec:4-3.2"><i class="fa fa-check"></i><b>2.4.2</b> RESTful APIs, returned data, and Python wrappers</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="chap-web.html"><a href="chap-web.html#sec:4-4"><i class="fa fa-check"></i><b>2.5</b> Programming against an API</a></li>
<li class="chapter" data-level="2.6" data-path="chap-web.html"><a href="chap-web.html#sec:4-4.1"><i class="fa fa-check"></i><b>2.6</b> Using the ORCID API via a wrapper</a></li>
<li class="chapter" data-level="2.7" data-path="chap-web.html"><a href="chap-web.html#sec:4-5"><i class="fa fa-check"></i><b>2.7</b> Quality, scope, and management</a></li>
<li class="chapter" data-level="2.8" data-path="chap-web.html"><a href="chap-web.html#sec:4-6"><i class="fa fa-check"></i><b>2.8</b> Integrating data from multiple sources</a><ul>
<li class="chapter" data-level="2.8.1" data-path="chap-web.html"><a href="chap-web.html#sec:4-6.1"><i class="fa fa-check"></i><b>2.8.1</b> The Lagotto API</a></li>
<li class="chapter" data-level="2.8.2" data-path="chap-web.html"><a href="chap-web.html#sec:4-6.2"><i class="fa fa-check"></i><b>2.8.2</b> Working with a corpus</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="chap-web.html"><a href="chap-web.html#sec:4-7"><i class="fa fa-check"></i><b>2.9</b> Working with the graph of relationships</a><ul>
<li class="chapter" data-level="2.9.1" data-path="chap-web.html"><a href="chap-web.html#sec:4-7.1"><i class="fa fa-check"></i><b>2.9.1</b> Citation links between articles</a></li>
<li class="chapter" data-level="2.9.2" data-path="chap-web.html"><a href="chap-web.html#sec:4-7.1.2"><i class="fa fa-check"></i><b>2.9.2</b> Categories, sources, and connections</a></li>
<li class="chapter" data-level="2.9.3" data-path="chap-web.html"><a href="chap-web.html#sec:4-7.1.3"><i class="fa fa-check"></i><b>2.9.3</b> Data availability and completeness</a></li>
<li class="chapter" data-level="2.9.4" data-path="chap-web.html"><a href="chap-web.html#sec:4-7.1.4"><i class="fa fa-check"></i><b>2.9.4</b> The value of sparse dynamic data</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="chap-web.html"><a href="chap-web.html#sec:4-8"><i class="fa fa-check"></i><b>2.10</b> Bringing it together: Tracking pathways to impact</a><ul>
<li class="chapter" data-level="2.10.1" data-path="chap-web.html"><a href="chap-web.html#sec:4-8.1"><i class="fa fa-check"></i><b>2.10.1</b> Network analysis approaches</a></li>
<li class="chapter" data-level="2.10.2" data-path="chap-web.html"><a href="chap-web.html#sec:4-8.3"><i class="fa fa-check"></i><b>2.10.2</b> Future prospects and new data sources</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="chap-web.html"><a href="chap-web.html#sec:4-9"><i class="fa fa-check"></i><b>2.11</b> Summary</a></li>
<li class="chapter" data-level="2.12" data-path="chap-web.html"><a href="chap-web.html#resources"><i class="fa fa-check"></i><b>2.12</b> Resources</a></li>
<li class="chapter" data-level="2.13" data-path="chap-web.html"><a href="chap-web.html#acknowledgements-and-copyright"><i class="fa fa-check"></i><b>2.13</b> Acknowledgements and copyright</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chap-link.html"><a href="chap-link.html"><i class="fa fa-check"></i><b>3</b> Record Linkage</a><ul>
<li class="chapter" data-level="3.1" data-path="chap-link.html"><a href="chap-link.html#motivation"><i class="fa fa-check"></i><b>3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2" data-path="chap-link.html"><a href="chap-link.html#sec:recordlinkage"><i class="fa fa-check"></i><b>3.2</b> Introduction to record linkage</a></li>
<li class="chapter" data-level="3.3" data-path="chap-link.html"><a href="chap-link.html#preprocessing-data-for-record-linkage"><i class="fa fa-check"></i><b>3.3</b> Preprocessing data for record linkage</a></li>
<li class="chapter" data-level="3.4" data-path="chap-link.html"><a href="chap-link.html#S:indexing"><i class="fa fa-check"></i><b>3.4</b> Indexing and blocking</a></li>
<li class="chapter" data-level="3.5" data-path="chap-link.html"><a href="chap-link.html#matching"><i class="fa fa-check"></i><b>3.5</b> Matching</a><ul>
<li class="chapter" data-level="3.5.1" data-path="chap-link.html"><a href="chap-link.html#rule-based-approaches"><i class="fa fa-check"></i><b>3.5.1</b> Rule-based approaches</a></li>
<li class="chapter" data-level="3.5.2" data-path="chap-link.html"><a href="chap-link.html#probabilistic-record-linkage"><i class="fa fa-check"></i><b>3.5.2</b> Probabilistic record linkage</a></li>
<li class="chapter" data-level="3.5.3" data-path="chap-link.html"><a href="chap-link.html#machine-learning-approaches-to-linking"><i class="fa fa-check"></i><b>3.5.3</b> Machine learning approaches to linking</a></li>
<li class="chapter" data-level="3.5.4" data-path="chap-link.html"><a href="chap-link.html#disambiguating-networks"><i class="fa fa-check"></i><b>3.5.4</b> Disambiguating networks</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="chap-link.html"><a href="chap-link.html#classification"><i class="fa fa-check"></i><b>3.6</b> Classification</a><ul>
<li class="chapter" data-level="3.6.1" data-path="chap-link.html"><a href="chap-link.html#S:thresholds"><i class="fa fa-check"></i><b>3.6.1</b> Thresholds</a></li>
<li class="chapter" data-level="3.6.2" data-path="chap-link.html"><a href="chap-link.html#one-to-one-links"><i class="fa fa-check"></i><b>3.6.2</b> One-to-one links</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="chap-link.html"><a href="chap-link.html#record-linkage-and-data-protection"><i class="fa fa-check"></i><b>3.7</b> Record linkage and data protection</a></li>
<li class="chapter" data-level="3.8" data-path="chap-link.html"><a href="chap-link.html#summary"><i class="fa fa-check"></i><b>3.8</b> Summary</a></li>
<li class="chapter" data-level="3.9" data-path="chap-link.html"><a href="chap-link.html#resources-1"><i class="fa fa-check"></i><b>3.9</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chap-db.html"><a href="chap-db.html"><i class="fa fa-check"></i><b>4</b> Databases</a><ul>
<li class="chapter" data-level="4.1" data-path="chap-db.html"><a href="chap-db.html#sec:db:intro"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="chap-db.html"><a href="chap-db.html#sec:db:when"><i class="fa fa-check"></i><b>4.2</b> DBMS: When and why</a></li>
<li class="chapter" data-level="4.3" data-path="chap-db.html"><a href="chap-db.html#relational-dbmss"><i class="fa fa-check"></i><b>4.3</b> Relational DBMSs</a><ul>
<li class="chapter" data-level="4.3.1" data-path="chap-db.html"><a href="chap-db.html#structured-query-language-sql"><i class="fa fa-check"></i><b>4.3.1</b> Structured Query Language (SQL)</a></li>
<li class="chapter" data-level="4.3.2" data-path="chap-db.html"><a href="chap-db.html#sec:db:sql"><i class="fa fa-check"></i><b>4.3.2</b> Manipulating and querying data</a></li>
<li class="chapter" data-level="4.3.3" data-path="chap-db.html"><a href="chap-db.html#sec:db:schema"><i class="fa fa-check"></i><b>4.3.3</b> Schema design and definition</a></li>
<li class="chapter" data-level="4.3.4" data-path="chap-db.html"><a href="chap-db.html#loading-data"><i class="fa fa-check"></i><b>4.3.4</b> Loading data</a></li>
<li class="chapter" data-level="4.3.5" data-path="chap-db.html"><a href="chap-db.html#transactions-and-crash-recovery"><i class="fa fa-check"></i><b>4.3.5</b> Transactions and crash recovery</a></li>
<li class="chapter" data-level="4.3.6" data-path="chap-db.html"><a href="chap-db.html#sec:db:index"><i class="fa fa-check"></i><b>4.3.6</b> Database optimizations</a></li>
<li class="chapter" data-level="4.3.7" data-path="chap-db.html"><a href="chap-db.html#caveats-and-challenges"><i class="fa fa-check"></i><b>4.3.7</b> Caveats and challenges</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="chap-db.html"><a href="chap-db.html#linking-dbmss-and-other-tools"><i class="fa fa-check"></i><b>4.4</b> Linking DBMSs and other tools</a></li>
<li class="chapter" data-level="4.5" data-path="chap-db.html"><a href="chap-db.html#sec:db:nosql"><i class="fa fa-check"></i><b>4.5</b> NoSQL databases</a><ul>
<li class="chapter" data-level="4.5.1" data-path="chap-db.html"><a href="chap-db.html#challenges-of-scale-the-cap-theorem"><i class="fa fa-check"></i><b>4.5.1</b> Challenges of scale: The CAP theorem</a></li>
<li class="chapter" data-level="4.5.2" data-path="chap-db.html"><a href="chap-db.html#nosql-and-keyvalue-stores"><i class="fa fa-check"></i><b>4.5.2</b> NoSQL and key–value stores</a></li>
<li class="chapter" data-level="4.5.3" data-path="chap-db.html"><a href="chap-db.html#other-nosql-databases"><i class="fa fa-check"></i><b>4.5.3</b> Other NoSQL databases</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="chap-db.html"><a href="chap-db.html#sec:db:spatial"><i class="fa fa-check"></i><b>4.6</b> Spatial databases</a></li>
<li class="chapter" data-level="4.7" data-path="chap-db.html"><a href="chap-db.html#which-database-to-use"><i class="fa fa-check"></i><b>4.7</b> Which database to use?</a><ul>
<li class="chapter" data-level="4.7.1" data-path="chap-db.html"><a href="chap-db.html#relational-dbmss-1"><i class="fa fa-check"></i><b>4.7.1</b> Relational DBMSs</a></li>
<li class="chapter" data-level="4.7.2" data-path="chap-db.html"><a href="chap-db.html#nosql-dbmss"><i class="fa fa-check"></i><b>4.7.2</b> NoSQL DBMSs</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="chap-db.html"><a href="chap-db.html#summary-1"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
<li class="chapter" data-level="4.9" data-path="chap-db.html"><a href="chap-db.html#resources-2"><i class="fa fa-check"></i><b>4.9</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chap-parallel.html"><a href="chap-parallel.html"><i class="fa fa-check"></i><b>5</b> Programming with Big Data</a><ul>
<li class="chapter" data-level="5.1" data-path="chap-parallel.html"><a href="chap-parallel.html#introduction-1"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="chap-parallel.html"><a href="chap-parallel.html#sec:intro"><i class="fa fa-check"></i><b>5.2</b> The MapReduce programming model</a></li>
<li class="chapter" data-level="5.3" data-path="chap-parallel.html"><a href="chap-parallel.html#apache-hadoop-mapreduce"><i class="fa fa-check"></i><b>5.3</b> Apache Hadoop MapReduce</a><ul>
<li class="chapter" data-level="5.3.1" data-path="chap-parallel.html"><a href="chap-parallel.html#the-hadoop-distributed-file-system"><i class="fa fa-check"></i><b>5.3.1</b> The Hadoop Distributed File System</a></li>
<li class="chapter" data-level="5.3.2" data-path="chap-parallel.html"><a href="chap-parallel.html#hadoop-bringing-compute-to-the-data"><i class="fa fa-check"></i><b>5.3.2</b> Hadoop: Bringing compute to the data</a></li>
<li class="chapter" data-level="5.3.3" data-path="chap-parallel.html"><a href="chap-parallel.html#hardware-provisioning"><i class="fa fa-check"></i><b>5.3.3</b> Hardware provisioning</a></li>
<li class="chapter" data-level="5.3.4" data-path="chap-parallel.html"><a href="chap-parallel.html#programming-language-support"><i class="fa fa-check"></i><b>5.3.4</b> Programming language support</a></li>
<li class="chapter" data-level="5.3.5" data-path="chap-parallel.html"><a href="chap-parallel.html#fault-tolerance"><i class="fa fa-check"></i><b>5.3.5</b> Fault tolerance</a></li>
<li class="chapter" data-level="5.3.6" data-path="chap-parallel.html"><a href="chap-parallel.html#limitations-of-hadoop"><i class="fa fa-check"></i><b>5.3.6</b> Limitations of Hadoop</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="chap-parallel.html"><a href="chap-parallel.html#apache-spark"><i class="fa fa-check"></i><b>5.4</b> Apache Spark</a></li>
<li class="chapter" data-level="5.5" data-path="chap-parallel.html"><a href="chap-parallel.html#summary-2"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
<li class="chapter" data-level="5.6" data-path="chap-parallel.html"><a href="chap-parallel.html#resources-3"><i class="fa fa-check"></i><b>5.6</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chap-ml.html"><a href="chap-ml.html"><i class="fa fa-check"></i><b>6</b> Machine Learning</a><ul>
<li class="chapter" data-level="6.1" data-path="chap-ml.html"><a href="chap-ml.html#introduction-2"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="chap-ml.html"><a href="chap-ml.html#what-is-machine-learning"><i class="fa fa-check"></i><b>6.2</b> What is machine learning?</a></li>
<li class="chapter" data-level="6.3" data-path="chap-ml.html"><a href="chap-ml.html#the-machine-learning-process"><i class="fa fa-check"></i><b>6.3</b> The machine learning process</a></li>
<li class="chapter" data-level="6.4" data-path="chap-ml.html"><a href="chap-ml.html#problem-formulation-mapping-a-problem-to-machine-learning-methods"><i class="fa fa-check"></i><b>6.4</b> Problem formulation: Mapping a problem to machine learning methods</a></li>
<li class="chapter" data-level="6.5" data-path="chap-ml.html"><a href="chap-ml.html#methods"><i class="fa fa-check"></i><b>6.5</b> Methods</a><ul>
<li class="chapter" data-level="6.5.1" data-path="chap-ml.html"><a href="chap-ml.html#unsupervised-learning-methods"><i class="fa fa-check"></i><b>6.5.1</b> Unsupervised learning methods</a></li>
<li class="chapter" data-level="6.5.2" data-path="chap-ml.html"><a href="chap-ml.html#sec:MLchapter:super"><i class="fa fa-check"></i><b>6.5.2</b> Supervised learning</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="chap-ml.html"><a href="chap-ml.html#evaluation"><i class="fa fa-check"></i><b>6.6</b> Evaluation</a><ul>
<li class="chapter" data-level="6.6.1" data-path="chap-ml.html"><a href="chap-ml.html#methodology"><i class="fa fa-check"></i><b>6.6.1</b> Methodology</a></li>
<li class="chapter" data-level="6.6.2" data-path="chap-ml.html"><a href="chap-ml.html#metrics"><i class="fa fa-check"></i><b>6.6.2</b> Metrics</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="chap-ml.html"><a href="chap-ml.html#practical-tips"><i class="fa fa-check"></i><b>6.7</b> Practical tips</a><ul>
<li class="chapter" data-level="6.7.1" data-path="chap-ml.html"><a href="chap-ml.html#features"><i class="fa fa-check"></i><b>6.7.1</b> Features</a></li>
<li class="chapter" data-level="6.7.2" data-path="chap-ml.html"><a href="chap-ml.html#machine-learning-pipeline"><i class="fa fa-check"></i><b>6.7.2</b> Machine learning pipeline</a></li>
<li class="chapter" data-level="6.7.3" data-path="chap-ml.html"><a href="chap-ml.html#multiclass-problems"><i class="fa fa-check"></i><b>6.7.3</b> Multiclass problems</a></li>
<li class="chapter" data-level="6.7.4" data-path="chap-ml.html"><a href="chap-ml.html#skewed-or-imbalanced-classification-problems"><i class="fa fa-check"></i><b>6.7.4</b> Skewed or imbalanced classification problems</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="chap-ml.html"><a href="chap-ml.html#how-can-social-scientists-benefit-from-machine-learning"><i class="fa fa-check"></i><b>6.8</b> How can social scientists benefit from machine learning?</a></li>
<li class="chapter" data-level="6.9" data-path="chap-ml.html"><a href="chap-ml.html#advanced-topics"><i class="fa fa-check"></i><b>6.9</b> Advanced topics</a></li>
<li class="chapter" data-level="6.10" data-path="chap-ml.html"><a href="chap-ml.html#summary-3"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
<li class="chapter" data-level="6.11" data-path="chap-ml.html"><a href="chap-ml.html#ml:res"><i class="fa fa-check"></i><b>6.11</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chap-text.html"><a href="chap-text.html"><i class="fa fa-check"></i><b>7</b> Text Analysis</a><ul>
<li class="chapter" data-level="7.1" data-path="chap-text.html"><a href="chap-text.html#understanding-what-people-write"><i class="fa fa-check"></i><b>7.1</b> Understanding what people write</a></li>
<li class="chapter" data-level="7.2" data-path="chap-text.html"><a href="chap-text.html#how-to-analyze-text"><i class="fa fa-check"></i><b>7.2</b> How to analyze text</a><ul>
<li class="chapter" data-level="7.2.1" data-path="chap-text.html"><a href="chap-text.html#processing-text-data"><i class="fa fa-check"></i><b>7.2.1</b> Processing text data</a></li>
<li class="chapter" data-level="7.2.2" data-path="chap-text.html"><a href="chap-text.html#how-much-is-a-word-worth"><i class="fa fa-check"></i><b>7.2.2</b> How much is a word worth?</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="chap-text.html"><a href="chap-text.html#sec:appapp"><i class="fa fa-check"></i><b>7.3</b> Approaches and applications</a><ul>
<li class="chapter" data-level="7.3.1" data-path="chap-text.html"><a href="chap-text.html#sec:lda"><i class="fa fa-check"></i><b>7.3.1</b> Topic modeling</a></li>
<li class="chapter" data-level="7.3.2" data-path="chap-text.html"><a href="chap-text.html#sec:ir"><i class="fa fa-check"></i><b>7.3.2</b> Information retrieval and clustering</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="chap-text.html"><a href="chap-text.html#sec:eval"><i class="fa fa-check"></i><b>7.4</b> Evaluation</a></li>
<li class="chapter" data-level="7.5" data-path="chap-text.html"><a href="chap-text.html#text-analysis-tools"><i class="fa fa-check"></i><b>7.5</b> Text analysis tools</a></li>
<li class="chapter" data-level="7.6" data-path="chap-text.html"><a href="chap-text.html#summary-4"><i class="fa fa-check"></i><b>7.6</b> Summary</a></li>
<li class="chapter" data-level="7.7" data-path="chap-text.html"><a href="chap-text.html#resources-4"><i class="fa fa-check"></i><b>7.7</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chap-networks.html"><a href="chap-networks.html"><i class="fa fa-check"></i><b>8</b> Networks: The Basics</a><ul>
<li class="chapter" data-level="8.1" data-path="chap-networks.html"><a href="chap-networks.html#introduction-3"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="chap-networks.html"><a href="chap-networks.html#network-data"><i class="fa fa-check"></i><b>8.2</b> Network data</a><ul>
<li class="chapter" data-level="8.2.1" data-path="chap-networks.html"><a href="chap-networks.html#forms-of-network-data"><i class="fa fa-check"></i><b>8.2.1</b> Forms of network data</a></li>
<li class="chapter" data-level="8.2.2" data-path="chap-networks.html"><a href="chap-networks.html#inducing-one-mode-networks-from-two-mode-data"><i class="fa fa-check"></i><b>8.2.2</b> Inducing one-mode networks from two-mode data</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="chap-networks.html"><a href="chap-networks.html#network-measures"><i class="fa fa-check"></i><b>8.3</b> Network measures</a><ul>
<li class="chapter" data-level="8.3.1" data-path="chap-networks.html"><a href="chap-networks.html#reachability"><i class="fa fa-check"></i><b>8.3.1</b> Reachability</a></li>
<li class="chapter" data-level="8.3.2" data-path="chap-networks.html"><a href="chap-networks.html#whole-network-measures"><i class="fa fa-check"></i><b>8.3.2</b> Whole-network measures</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="chap-networks.html"><a href="chap-networks.html#comparing-collaboration-networks"><i class="fa fa-check"></i><b>8.4</b> Comparing collaboration networks</a></li>
<li class="chapter" data-level="8.5" data-path="chap-networks.html"><a href="chap-networks.html#summary-5"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="chap-networks.html"><a href="chap-networks.html#resources-5"><i class="fa fa-check"></i><b>8.6</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chap-viz.html"><a href="chap-viz.html"><i class="fa fa-check"></i><b>9</b> Information Visualization</a><ul>
<li class="chapter" data-level="9.1" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-1"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2"><i class="fa fa-check"></i><b>9.2</b> Developing effective visualizations</a></li>
<li class="chapter" data-level="9.3" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-3"><i class="fa fa-check"></i><b>9.3</b> A data-by-tasks taxonomy</a><ul>
<li class="chapter" data-level="9.3.1" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.1"><i class="fa fa-check"></i><b>9.3.1</b> Multivariate data</a></li>
<li class="chapter" data-level="9.3.2" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.2"><i class="fa fa-check"></i><b>9.3.2</b> Spatial data</a></li>
<li class="chapter" data-level="9.3.3" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.4"><i class="fa fa-check"></i><b>9.3.3</b> Temporal data</a></li>
<li class="chapter" data-level="9.3.4" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.5"><i class="fa fa-check"></i><b>9.3.4</b> Hierarchical data</a></li>
<li class="chapter" data-level="9.3.5" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.6"><i class="fa fa-check"></i><b>9.3.5</b> Network data</a></li>
<li class="chapter" data-level="9.3.6" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.7"><i class="fa fa-check"></i><b>9.3.6</b> Text data</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4"><i class="fa fa-check"></i><b>9.4</b> Challenges</a><ul>
<li class="chapter" data-level="9.4.1" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4.1"><i class="fa fa-check"></i><b>9.4.1</b> Scalability</a></li>
<li class="chapter" data-level="9.4.2" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4.2"><i class="fa fa-check"></i><b>9.4.2</b> Evaluation</a></li>
<li class="chapter" data-level="9.4.3" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4.3"><i class="fa fa-check"></i><b>9.4.3</b> Visual impairment</a></li>
<li class="chapter" data-level="9.4.4" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4.4"><i class="fa fa-check"></i><b>9.4.4</b> Visual literacy</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-5"><i class="fa fa-check"></i><b>9.5</b> Summary</a></li>
<li class="chapter" data-level="9.6" data-path="chap-viz.html"><a href="chap-viz.html#sec:mylabel4"><i class="fa fa-check"></i><b>9.6</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chap-errors.html"><a href="chap-errors.html"><i class="fa fa-check"></i><b>10</b> Errors and Inference</a><ul>
<li class="chapter" data-level="10.1" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-1"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-2"><i class="fa fa-check"></i><b>10.2</b> The total error paradigm</a><ul>
<li class="chapter" data-level="10.2.1" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-2.1"><i class="fa fa-check"></i><b>10.2.1</b> The traditional model</a></li>
<li class="chapter" data-level="10.2.2" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-2.2"><i class="fa fa-check"></i><b>10.2.2</b> Extending the framework to big data</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-3"><i class="fa fa-check"></i><b>10.3</b> Illustrations of errors in big data</a></li>
<li class="chapter" data-level="10.4" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-4"><i class="fa fa-check"></i><b>10.4</b> Errors in big data analytics</a><ul>
<li class="chapter" data-level="10.4.1" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-4.1"><i class="fa fa-check"></i><b>10.4.1</b> Errors resulting from volume, velocity, and variety, assuming perfect veracity</a></li>
<li class="chapter" data-level="10.4.2" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-4.2"><i class="fa fa-check"></i><b>10.4.2</b> Errors resulting from lack of veracity</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-5"><i class="fa fa-check"></i><b>10.5</b> Some methods for mitigating, detecting, and compensating for errors</a></li>
<li class="chapter" data-level="10.6" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-6"><i class="fa fa-check"></i><b>10.6</b> Summary</a></li>
<li class="chapter" data-level="10.7" data-path="chap-errors.html"><a href="chap-errors.html#resources-6"><i class="fa fa-check"></i><b>10.7</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chap-privacy.html"><a href="chap-privacy.html"><i class="fa fa-check"></i><b>11</b> Privacy and Confidentiality</a><ul>
<li class="chapter" data-level="11.1" data-path="chap-privacy.html"><a href="chap-privacy.html#introduction-4"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="chap-privacy.html"><a href="chap-privacy.html#why-is-access-important"><i class="fa fa-check"></i><b>11.2</b> Why is access important?</a></li>
<li class="chapter" data-level="11.3" data-path="chap-privacy.html"><a href="chap-privacy.html#providing-access"><i class="fa fa-check"></i><b>11.3</b> Providing access</a></li>
<li class="chapter" data-level="11.4" data-path="chap-privacy.html"><a href="chap-privacy.html#the-new-challenges"><i class="fa fa-check"></i><b>11.4</b> The new challenges</a></li>
<li class="chapter" data-level="11.5" data-path="chap-privacy.html"><a href="chap-privacy.html#legal-and-ethical-framework"><i class="fa fa-check"></i><b>11.5</b> Legal and ethical framework</a></li>
<li class="chapter" data-level="11.6" data-path="chap-privacy.html"><a href="chap-privacy.html#summary-6"><i class="fa fa-check"></i><b>11.6</b> Summary</a></li>
<li class="chapter" data-level="11.7" data-path="chap-privacy.html"><a href="chap-privacy.html#resources-7"><i class="fa fa-check"></i><b>11.7</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chap-workbooks.html"><a href="chap-workbooks.html"><i class="fa fa-check"></i><b>12</b> Workbooks</a><ul>
<li class="chapter" data-level="12.1" data-path="chap-workbooks.html"><a href="chap-workbooks.html#introduction-5"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="chap-workbooks.html"><a href="chap-workbooks.html#environment"><i class="fa fa-check"></i><b>12.2</b> Environment</a><ul>
<li class="chapter" data-level="12.2.1" data-path="chap-workbooks.html"><a href="chap-workbooks.html#running-workbooks-locally"><i class="fa fa-check"></i><b>12.2.1</b> Running workbooks locally</a></li>
<li class="chapter" data-level="12.2.2" data-path="chap-workbooks.html"><a href="chap-workbooks.html#central-workbook-server"><i class="fa fa-check"></i><b>12.2.2</b> Central workbook server</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="chap-workbooks.html"><a href="chap-workbooks.html#workbook-details"><i class="fa fa-check"></i><b>12.3</b> Workbook details</a><ul>
<li class="chapter" data-level="12.3.1" data-path="chap-workbooks.html"><a href="chap-workbooks.html#social-media-and-apis"><i class="fa fa-check"></i><b>12.3.1</b> Social Media and APIs</a></li>
<li class="chapter" data-level="12.3.2" data-path="chap-workbooks.html"><a href="chap-workbooks.html#database-basics"><i class="fa fa-check"></i><b>12.3.2</b> Database basics</a></li>
<li class="chapter" data-level="12.3.3" data-path="chap-workbooks.html"><a href="chap-workbooks.html#data-linkage"><i class="fa fa-check"></i><b>12.3.3</b> Data Linkage</a></li>
<li class="chapter" data-level="12.3.4" data-path="chap-workbooks.html"><a href="chap-workbooks.html#machine-learning"><i class="fa fa-check"></i><b>12.3.4</b> Machine Learning</a></li>
<li class="chapter" data-level="12.3.5" data-path="chap-workbooks.html"><a href="chap-workbooks.html#text-analysis"><i class="fa fa-check"></i><b>12.3.5</b> Text Analysis</a></li>
<li class="chapter" data-level="12.3.6" data-path="chap-workbooks.html"><a href="chap-workbooks.html#networks"><i class="fa fa-check"></i><b>12.3.6</b> Networks</a></li>
<li class="chapter" data-level="12.3.7" data-path="chap-workbooks.html"><a href="chap-workbooks.html#visualization"><i class="fa fa-check"></i><b>12.3.7</b> Visualization</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="chap-workbooks.html"><a href="chap-workbooks.html#resources-8"><i class="fa fa-check"></i><b>12.4</b> Resources</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Big Data and Social Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chap:web" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Working with Web Data and APIs</h1>
<p> <strong>Cameron Neylon</strong></p>
<p>This chapter will show you how to extract information from social media about the transmission of knowledge. The particular application will be to develop links to authors’ articles on Twitter using PLOS articles and to pull information using an API. You will get link data from bookmarking services, citations from Crossref, links from Facebook, and information from news coverage. The examples that will be used are from Twitter. In keeping with the social science grounding that is a core feature of the book, it will discuss what can be captured, what is potentially reliable, and how to manage data quality issues.</p>
<div id="introduction" class="section level2">
<h2><span class="header-section-number">2.1</span> Introduction</h2>
<p>A tremendous lure of the Internet is the availability of vast amounts of data on businesses, people, and their activity on social media. But how can we capture the information and make use of it as we might make use of more traditional data sources? In this chapter, we begin by describing how web data can be collected, using the use case of UMETRICS and research output as a readily available example, and then discuss how to think about the scope, coverage, and integration issues associated with its collection.</p>
<p>Often a big data exploration starts with information on people or on a group of people. The web can be a rich source of additional information. It can also act as pointers to new sources of information, allowing a pivot from one perspective to another, from one kind of query to another. Often this is exploratory. You have an existing core set of data and are looking to augment it. But equally this exploration can open up whole new avenues. Sometimes the data are completely unstructured, existing as web pages spread across a site, and sometimes they are provided in a machine-readable form. The challenge is in having a sufficiently diverse toolkit to bring all of this information together.</p>
<p>Using the example of data on researchers and research outputs, we will explore obtaining information directly from web pages (<em>web scraping</em>) as well as explore the uses of APIs— web services that allow an interaction with, and retrieval of, structured data. You will see how the crucial pieces of integration often lie in making connections between disparate data sets and how in turn making those connections requires careful quality control. The emphasis throughout this chapter is on the importance of focusing on the purpose for which the data will be used as a guide for data collection. While much of this is specific to data about research and researchers, the ideas are generalizable to wider issues of data and public policy.</p>
</div>
<div id="sec:4-1" class="section level2">
<h2><span class="header-section-number">2.2</span> Scraping information from the web</h2>
<p>With the range of information available on the web, our first question is how to access it. The simplest approach is often to manually go directly to the web and look for data files or other information. For instance, on the NSF website <span class="citation">[@nsfweb]</span> it is possible to obtain data dumps of all grant information. Sometimes data are available only on web pages or we only want a subset of this information. In this case web scraping is often a viable approach.</p>
<p>Web scraping involves using a program to download and process web pages directly. This can be highly effective, particularly where tables of information are made available online. It is also useful in cases where it is desirable to make a series of very similar queries. In each case we need to look at the website, identify how to get the information we want, and then process it. Many websites deliberately make this difficult to prevent easy access to their underlying data.</p>
<div id="sec:4-1.1" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Obtaining data from the HHMI website</h3>
<p>Let us suppose we are interested in obtaining information on those investigators that are funded by the Howard Hughes Medical Institute (HHMI). HHMI has a website that includes a search function for funded researchers, including the ability to filter by field, state, and role. But there does not appear to be a downloadable data set of this information. However, we can automate the process with code to create a data set that you might compare with other data.</p>
<p>This process involves first understanding how to construct a URL that will do the search we want. This is most easily done by playing with search functionality and investigating the URL structures that are returned. Note that in many cases websites are not helpful here. However, with HHMI if we do a general search and play with the structure of the URL, we can see some of the elements of the URL that we can think of as a query. As we want to see <em>all</em> investigators, we do not need to limit the search, and so with some fiddling we come up with a URL like the following. (We have broken the one-line URL into three lines for ease of presentation.)</p>

<p><a href="http://www.hhmi.org/scientists/browse?kw" class="uri">http://www.hhmi.org/scientists/browse?kw</a>=&amp;sort_by=field_scientist_last_name&amp;sort_order=ASC&amp;items_per_page=20&amp;page=0</p>
<p>The <code>requests</code> module, available natively in Jupyter Python notebooks, is a useful set of tools for handling interactions with websites. It lets us construct the request that we just presented in terms of a base URL and query terms, as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>BASE_URL =<span class="st"> &quot;http://www.hhmi.org/scientists/browse&quot;</span>
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>query =<span class="st"> </span>{
            <span class="st">&quot;kw&quot;</span> <span class="op">:</span><span class="st"> &quot;&quot;</span>,
            <span class="st">&quot;sort_by&quot;</span> <span class="op">:</span><span class="st"> &quot;field_scientist_last_name&quot;</span>,
            <span class="st">&quot;sort_order&quot;</span> <span class="op">:</span><span class="st"> &quot;ASC&quot;</span>,
            <span class="st">&quot;items_per_page&quot;</span> <span class="op">:</span><span class="st"> </span><span class="dv">20</span>,
            <span class="st">&quot;page&quot;</span> <span class="op">:</span><span class="st"> </span>None
           }</code></pre></div>
<p>With our request constructed we can then make the call to the web page to get a response.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>import requests
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>response =<span class="st"> </span><span class="kw">requests.get</span>(BASE_URL, <span class="dt">params=</span>query)</code></pre></div>
<p>The first thing to do when building a script that hits a web page is to make sure that your call was successful. This can be checked by looking at the response code that the web server sent—and, obviously, by checking the actual HTML that was returned. A <code>200</code> code means success and that everything should be OK. Other codes may mean that the URL was constructed wrongly or that there was a server error.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>response.status_code
<span class="dv">200</span></code></pre></div>
<p>With the page successfully returned, we now need to process the text it contains into the data we want. This is not a trivial exercise. It is possible to search through and find things, but there are a range of tools that can help with processing HTML and XML data. Among these one of the most popular is a module called BeautifulSoup<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a> <span class="citation">[@bsoup]</span>, which provides a number of useful functions for this kind of processing. The module documentation provides more details.</p>
<p>We need to check the details of the page source to find where the information we are looking for is kept (see, for example, <a href="chap-web.html#fig:fig2-1">2.1</a>). Here, all the details on HHMI investigators can be found in a <code>&lt;div&gt;</code> element with the class attribute <code>view-content</code>. This structure is not something that can be determined in advance. It requires knowledge of the structure of the page itself. Nested inside this <code>&lt;div&gt;</code> element are another series of <code>div</code>s, each of which corresponds to one investigator. These have the class attribute <code>view-rows</code>. Again, there is nothing obvious about finding these, it requires a close examination of the page HTML itself for any specific case you happen to be looking at.</p>
<div class="figure" style="text-align: center"><span id="fig:fig2-1"></span>
<img src="ChapterWeb/figures/fig2-1.png" alt="Source HTML from the portion of an HHMI results page containing information on HHMI investigators; note that the webscraping results in badly formatted html which is difficult to read." width="70%" />
<p class="caption">
Figure 2.1: Source HTML from the portion of an HHMI results page containing information on HHMI investigators; note that the webscraping results in badly formatted html which is difficult to read.
</p>
</div>
<p> We first process the page using the BeautifulSoup module (into the variable <code>soup</code>) and then find the <code>div</code> element that holds the information on investigators (<code>investigator_list</code>). As this element is unique on the page (I checked using my web browser), we can use the find method. We then process that <code>div</code> (using <code>find_all</code>) to create an iterator object that contains each of the page segments detailing a single investigator (<code>investigators</code>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>from bs4 import BeautifulSoup
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>soup =<span class="st"> </span><span class="kw">BeautifulSoup</span>(response.text, <span class="st">&quot;html5lib&quot;</span>)
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>investigator_list =<span class="st"> </span><span class="kw">soup.find</span>(<span class="st">&#39;div&#39;</span>, <span class="dt">class_ =</span> <span class="st">&quot;view-content&quot;</span>)
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>investigators =<span class="st"> </span><span class="kw">investigator_list.find_all</span>(<span class="st">&quot;div&quot;</span>, <span class="dt">class_ =</span> <span class="st">&quot;views-row&quot;</span>)</code></pre></div>
<p> As we specified in our query parameters that we wanted 20 results per page, we should check whether our list of page sections has the right length.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="kw">len</span>(investigators)
<span class="dv">20</span></code></pre></div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Given a request response object, parse for HHMI investigators</span>
def <span class="kw">scrape</span>(page_response)<span class="op">:</span>
<span class="st">   </span><span class="co"># Obtain response HTML and the correct &lt;div&gt; from the page</span>
<span class="st">   </span>soup =<span class="st"> </span><span class="kw">BeautifulSoup</span>(response.text, <span class="st">&quot;html5lib&quot;</span>)
   inv_list =<span class="st"> </span><span class="kw">soup.find</span>(<span class="st">&#39;div&#39;</span>, <span class="dt">class_ =</span> <span class="st">&quot;view-content&quot;</span>)

   <span class="co"># Create a list of all the investigators on the page</span>
   investigators =<span class="st"> </span><span class="kw">inv_list.find_all</span>(<span class="st">&quot;div&quot;</span>, <span class="dt">class_ =</span> <span class="st">&quot;views-row&quot;</span>)

   data =<span class="st"> </span>[] <span class="co"># Make the data object to store scraping results</span>

   <span class="co"># Scrape needed elements from investigator list</span>
   <span class="cf">for</span> investigator <span class="cf">in</span> investigators<span class="op">:</span>
<span class="st">       </span>inv =<span class="st"> </span>{} <span class="co"># Create a dictionary to store results</span>

       <span class="co"># Name and role are in same HTML element; this code</span>
       <span class="co"># separates them into two data elements</span>
       name_role_tag =<span class="st"> </span><span class="kw">investigator.find</span>(<span class="st">&quot;div&quot;</span>,
           <span class="dt">class_ =</span> <span class="st">&quot;views-field-field-scientist-classification&quot;</span>)
       strings =<span class="st"> </span>name_role_tag.stripped_strings
       <span class="cf">for</span> string,a <span class="cf">in</span> <span class="kw">zip</span>(strings, [<span class="st">&quot;name&quot;</span>, <span class="st">&quot;role&quot;</span>])<span class="op">:</span>
<span class="st">           </span>inv[a] =<span class="st"> </span>string

       <span class="co"># Extract other elements from text of specific divs or from</span>
       <span class="co"># class attributes of tags in the page (e.g., URLs)</span>
       research_tag =<span class="st"> </span><span class="kw">investigator.find</span>(<span class="st">&quot;div&quot;</span>,
          <span class="dt">class_ =</span> <span class="st">&quot;views-field-field-scientist-research-abs-nod&quot;</span>)
       inv[<span class="st">&quot;research&quot;</span>] =<span class="st"> </span><span class="kw">research_tag.text.lstrip</span>()
       inv[<span class="st">&quot;research_url&quot;</span>] =<span class="st"> &quot;http://hhmi.org&quot;</span>
          <span class="op">+</span><span class="st"> </span><span class="kw">research_tag.find</span>(<span class="st">&quot;a&quot;</span>)<span class="kw">.get</span>(<span class="st">&quot;href&quot;</span>)
       institution_tag =<span class="st"> </span><span class="kw">investigator.find</span>(<span class="st">&quot;div&quot;</span>,
          <span class="dt">class_ =</span> <span class="st">&quot;views-field-field-scientist-academic-institu&quot;</span>)
       inv[<span class="st">&quot;institute&quot;</span>] =<span class="st"> </span><span class="kw">institution_tag.text.lstrip</span>()
       town_state_tag =<span class="st"> </span><span class="kw">investigator.find</span>(<span class="st">&quot;div&quot;</span>,
           <span class="dt">class_ =</span> <span class="st">&quot;views-field-field-scientist-institutionstate&quot;</span>)
       inv[<span class="st">&quot;town&quot;</span>], inv[<span class="st">&quot;state&quot;</span>] =<span class="st"> </span><span class="kw">town_state_tag.text.split</span>(<span class="st">&quot;,&quot;</span>)
       inv[<span class="st">&quot;town&quot;</span>] =<span class="st"> </span><span class="kw">inv.get</span>(<span class="st">&quot;town&quot;</span>)<span class="kw">.lstrip</span>()
       inv[<span class="st">&quot;state&quot;</span>] =<span class="st"> </span><span class="kw">inv.get</span>(<span class="st">&quot;state&quot;</span>)<span class="kw">.lstrip</span>()

       thumbnail_tag =<span class="st"> </span><span class="kw">investigator.find</span>(<span class="st">&quot;div&quot;</span>,
          <span class="dt">class_ =</span> <span class="st">&quot;views-field-field-scientist-image-thumbnail&quot;</span>)
       inv[<span class="st">&quot;thumbnail_url&quot;</span>] =<span class="st"> </span><span class="kw">thumbnail_tag.find</span>(<span class="st">&quot;img&quot;</span>)[<span class="st">&quot;src&quot;</span>]
       inv[<span class="st">&quot;url&quot;</span>] =<span class="st"> &quot;http://hhmi.org&quot;</span>
          <span class="op">+</span><span class="st"> </span><span class="kw">thumbnail_tag.find</span>(<span class="st">&quot;a&quot;</span>)<span class="kw">.get</span>(<span class="st">&quot;href&quot;</span>)

       <span class="co"># Add the new data to the list</span>
       <span class="kw">data.append</span>(inv)
   return data</code></pre></div>
<div style="text-align: center">
Listing 2.1. Python code to parse for HHMI investigators
</div>
<p><br></p>
<p>Finally, we need to process each of these segments to obtain the data we are looking for. This is the actual “scraping” of the page to get the information we want. Again, this involves looking closely at the HTML itself, identifying where the information is held, what tags can be used to find it, and often doing some postprocessing to clean it up (removing spaces, splitting different elements up).</p>
<p>Listing 2.1 provides a function to handle all of this. The function accepts the response object from the requests module as its input, processes the page text to soup, and then finds the <code>investigator_list</code> as above and processes it into an actual list of the investigators. For each investigator it then processes the HTML to find and clean up the information required, converting it to a dictionary and adding it to our growing list of data.</p>
<p>Let us check what the first two elements of our data set now look like. You can see two dictionaries, one relating to Laurence Abbott, who is a senior fellow at the HHMI Janelia Farm Campus, and one for Susan Ackerman, an HHMI investigator based at the Jackson Laboratory in Bar Harbor, Maine. Note that we have also obtained URLs that give more details on the researcher and their research program (<code>research_url</code> and <code>url</code> keys in the dictionary) that could provide a useful input to textual analysis or topic modeling (see <a href="chap-text.html#chap:text">Text Analysis</a>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>data =<span class="st"> </span><span class="kw">scrape</span>(response)
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>data[<span class="dv">0</span><span class="op">:</span><span class="dv">2</span>]
[{<span class="st">&#39;institute&#39;</span><span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Janelia Research Campus &#39;</span>,
  <span class="st">&#39;name&#39;</span><span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Laurence Abbott, PhD&#39;</span>,
  <span class="st">&#39;research&#39;</span><span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Computational and Mathematical Modeling of Neurons and Neural... &#39;</span>,
  <span class="st">&#39;research_url&#39;</span><span class="op">:</span><span class="st"> </span>u<span class="st">&#39;http://hhmi.org/research/computational-and-mathematical-modeling-neurons-and-neural-networks&#39;</span>,
  <span class="st">&#39;role&#39;</span><span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Janelia Senior Fellow&#39;</span>,
  <span class="st">&#39;state&#39;</span><span class="op">:</span><span class="st"> </span>u<span class="st">&#39;VA &#39;</span>,
  <span class="st">&#39;thumbnail_url&#39;</span><span class="op">:</span><span class="st"> </span>u<span class="st">&#39;http://www.hhmi.org/sites/default/files/Our%20Scientists/Janelia/Abbott-112x112.jpg&#39;</span>,
  <span class="st">&#39;town&#39;</span><span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Ashburn&#39;</span>,
  <span class="st">&#39;url&#39;</span><span class="op">:</span><span class="st"> </span>u<span class="st">&#39;http://hhmi.org/scientists/laurence-f-abbott&#39;</span>},
 {<span class="st">&#39;institute&#39;</span><span class="op">:</span><span class="st"> </span>u<span class="st">&#39;The Jackson Laboratory &#39;</span>,
  <span class="st">&#39;name&#39;</span><span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Susan Ackerman, PhD&#39;</span>,
  <span class="st">&#39;research&#39;</span><span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Identification of the Molecular Mechanisms Underlying... &#39;</span>,
  <span class="st">&#39;research_url&#39;</span><span class="op">:</span><span class="st"> </span>u<span class="st">&#39;http://hhmi.org/research/identification-molecular-mechanisms-underlying-neurodegeneration&#39;</span>,
  <span class="st">&#39;role&#39;</span><span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Investigator&#39;</span>,
  <span class="st">&#39;state&#39;</span><span class="op">:</span><span class="st"> </span>u<span class="st">&#39;ME &#39;</span>,
  <span class="st">&#39;thumbnail_url&#39;</span><span class="op">:</span>
u<span class="st">&#39;http://www.hhmi.org/sites/default/files/Our%20Scientists/Investigators/Ackerman-112x112.jpg&#39;</span>,
  <span class="st">&#39;town&#39;</span><span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Bar Harbor&#39;</span>,
  <span class="st">&#39;url&#39;</span><span class="op">:</span><span class="st"> </span>u<span class="st">&#39;http://hhmi.org/scientists/susan-l-ackerman&#39;</span>}]</code></pre></div>
<p>So now we know we can process a page from a website to generate usefully structured data. However, this was only the first page of results. We need to do this for each page of results if we want to capture all the HHMI investigators. We could just look at the number of pages that our search returned manually, but to make this more general we can actually scrape the page to find that piece of information and use that to calculate how many pages we need to work through.</p>
<p>The number of results is found in a <code>div</code> with the class “view-headers” as a piece of free text (“Showing 1–20 of 493 results”). We need to grab the text, split it up (I do so based on spaces), find the right number (the one that is before the word “results”) and convert that to an integer. Then we can divide by the number of items we requested per page (20 in our case) to find how many pages we need to work through. A quick mental calculation confirms that if page 0 had results 1–20, page 24 would give results 481–493.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="co"># Check total number of investigators returned</span>
<span class="er">&gt;&gt;</span><span class="st"> </span>view_header =<span class="st"> </span><span class="kw">soup.find</span>(<span class="st">&quot;div&quot;</span>, <span class="dt">class_ =</span> <span class="st">&quot;view-header&quot;</span>)
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>words =<span class="st"> </span><span class="kw">view_header.text.split</span>(<span class="st">&quot; &quot;</span>)
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>count_index =<span class="st"> </span><span class="kw">words.index</span>(<span class="st">&quot;results.&quot;</span>) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>count =<span class="st"> </span><span class="kw">int</span>(words[count_index])

<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="co"># Calculate number of pages, given count &amp; items_per_page</span>
<span class="er">&gt;&gt;</span><span class="st"> </span>num_pages =<span class="st"> </span>count<span class="op">/</span><span class="kw">query.get</span>(<span class="st">&quot;items_per_page&quot;</span>)
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>num_pages
<span class="dv">24</span></code></pre></div>
<p>Then it is a simple matter of putting the function we constructed earlier into a loop to work through the correct number of pages. As we start to hit the website repeatedly, we need to consider whether we are being polite. Most websites have a file in the root directory called robots.txt that contains guidance on using programs to interact with the website. In the case of <a href="http://hhmi.org" class="uri">http://hhmi.org</a> the file states first that we are allowed (or, more properly, not forbidden) to query <a href="http://www.hhmi.org/scientists/" class="uri">http://www.hhmi.org/scientists/</a> programmatically. Thus, you can pull down all of the more detailed biographical or research information, if you so desire. The file also states that there is a requested “Crawl-delay” of 10. This means that if you are making repeated queries (as we will be in getting the 24 pages), you should wait for 10 seconds between each query. This request is easily accommodated by adding a timed delay between each page request.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="cf">for</span> page_num <span class="cf">in</span> <span class="kw">range</span>(num_pages)<span class="op">:</span>
<span class="er">&gt;&gt;</span><span class="st"> </span><span class="co"># We already have page zero and we need to go to 24:</span>
<span class="er">&gt;&gt;</span><span class="st"> </span><span class="co"># range(24) is [0,1,...,23]</span>
<span class="er">&gt;&gt;</span><span class="st">    </span>query[<span class="st">&quot;items_per_page&quot;</span>] =<span class="st"> </span>page_num <span class="op">+</span><span class="st"> </span><span class="dv">1</span>
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st">    </span>page =<span class="st"> </span><span class="kw">requests.get</span>(BASE_URL, <span class="dt">params=</span>query)
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="co"># We use extend to add list for each page to existing list</span>
<span class="er">&gt;&gt;</span><span class="st">    </span><span class="kw">data.extend</span>(<span class="kw">scrape</span>(page))
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>print <span class="st">&quot;Retrieved and scraped page number:&quot;</span>, <span class="kw">query.get</span>(<span class="st">&quot;items_per_page&quot;</span>)
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="kw">time.sleep</span>(<span class="dv">10</span>) <span class="co"># robots.txt at hhmi.org specifies a crawl delay of 10 seconds</span>
Retrieved and scraped page number<span class="op">:</span><span class="st"> </span><span class="dv">1</span>
Retrieved and scraped page number<span class="op">:</span><span class="st"> </span><span class="dv">2</span>
...
Retrieved and scraped page number<span class="op">:</span><span class="st"> </span><span class="dv">24</span></code></pre></div>
<p>Finally we can check that we have the right number of results after our scraping. This should correspond to the 493 records that the website reports.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="kw">len</span>(data)
<span class="dv">493</span></code></pre></div>
</div>
<div id="sec:4-1.2" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Limits of scraping</h3>
<p>While scraping websites is often necessary, is can be a fragile and messy way of working. It is problematic for a number of reasons: for example, many websites are designed in ways that make scraping difficult or impossible, and other sites explicitly prohibit this kind of scripted analysis. (Both reasons apply in the case of the NSF and Grants.gov websites, which is why we use the HHMI website in our example.)</p>
<p>In many cases a better choice is to process a data dump from an organization. For example, the NSF and Wellcome Trust both provide data sets for each year that include structured data on all their awarded grants. In practice, integrating data is a continual challenge of figuring out what is the easiest way to proceed, what is allowed, and what is practical and useful. The selection of data will often be driven by pragmatic rather than theoretical concerns.</p>
<p>Increasingly, however, good practice is emerging in which organizations provide APIs to enable scripted and programmatic access to the data they hold. These tools are much easier and generally more effective to work with. They are the focus of much of the rest of this chapter.</p>
</div>
</div>
<div id="sec:4-2" class="section level2">
<h2><span class="header-section-number">2.3</span> New data in the research enterprise</h2>
<p>The new forms of data we are discussing in this chapter are largely available because so many human activities—in this case, discussion, reading, and bookmarking—are happening online. All sorts of data are generated as a side effect of these activities. Some of that data is public (social media conversations), some private (IP addresses requesting specific pages), and some intrinsic to the service (the identity of a user who bookmarks an article). What exactly are these new forms of data? There are broadly two new directions that data availability is moving in. The first is information on new forms of research output, data sets, software, and in some cases physical resources. There is an interest across the research community in expanding the set of research outputs that are made available and, to drive this, significant efforts are being made to ensure that these nontraditional outputs are seen as legitimate outputs. In particular there has been a substantial policy emphasis on data sharing and, coupled with this, efforts to standardize practice around data citation. This is applying a well-established measure (citation) to a new form of research output.</p>
<p>The second new direction, which is more developed, takes the alternate route, providing <em>new forms of information on existing types of output</em>, specifically research articles. The move online of research activities, including discovery, reading, writing, and bookmarking, means that many of these activities leave a digital trace. Often these traces are public or semi-public and can be collected and tracked. This certainly raises privacy issues that have not been comprehensively addressed but also provides a rich source of data on who is doing what with research articles.</p>
<div class="figure" style="text-align: center"><span id="fig:fig2-2"></span>
<img src="ChapterWeb/figures/fig2-2.png" alt="Classes of online activity related to research journal articles. Reproduced from Lin and Fenner [237], under a Creative Commons Attribution v 3.0 license" width="70%" />
<p class="caption">
Figure 2.2: Classes of online activity related to research journal articles. Reproduced from Lin and Fenner [237], under a Creative Commons Attribution v 3.0 license
</p>
</div>
<p>There are a wide range of potential data sources, so it is useful to categorize them. Figure <a href="chap-web.html#fig:fig2-2">2.2</a> shows one possible categorization, in which data sources are grouped based on the level of engagement and the stage of use. It starts from the left with “views,” measures of online views and article downloads, followed by “saves” where readers actively collect articles into a library of their own, through online <em>discussion</em> forums such as blogs, social media and new commentary, formal scholarly <em>recommendations</em>, and, finally, formal <em>citations</em>.</p>
<p>These categories are a useful way to understand the classes of information available and to start digging into the sources they can be obtained from. For each category we will look at the <em>kind</em> of usage that the indicator is a proxy for, which <em>users</em> are captured by the indicator, the <em>limitations</em> that the indicator has as a measure, and the <em>sources</em> of data. We start with the familiar case of formal literature citations to provide context.</p>
<hr />
<p><strong>Example: Citations</strong></p>
<p>Most quantitative analyses of research have focused on citations from research articles to other research articles. Many familiar measures—such as Impact Factors, Scimago Journal Rank, or Eigenfactor—are actually measures of journal rather than article performance. However, information on citations at the article level is increasingly the basis for much bibliometric analysis.</p>
<ul>
<li><p>Kind of usage</p>
<ul>
<li><p>Citing a scholarly work is a signal from a researcher that a specific work has relevance to, or has influenced, the work they are describing.</p></li>
<li><p>It implies significant engagement and is a measure that carries some weight.</p></li>
</ul></li>
<li><p>Users</p>
<ul>
<li><p>Researchers, which means usage by a specific group for a fairly small range of purposes.</p></li>
<li><p>With high-quality data, there are some geographical, career, and disciplinary demographic details.</p></li>
</ul></li>
<li><p>Limitations</p>
<ul>
<li><p>The citations are slow to accumulate, as they must pass through a peer-review process.</p></li>
<li><p>It is seldom clear from raw data why a paper is being cited.</p></li>
<li><p>It provides a limited view of usage, as it only reflects reuse in research, not application in the community.</p></li>
</ul></li>
<li><p>Sources</p>
<ul>
<li><p>Public sources of citation data include PubMed Central and Europe PubMed Central, which mine publicly available full text to find citations.</p></li>
<li><p>Proprietary sources of citation data include Thomson Reuters’ Web of Knowledge and Elsevier’s Scopus.</p></li>
<li><p>Some publishers make citation data collected by Crossref available.</p></li>
</ul></li>
</ul>
<hr />
<hr />
<p><strong>Example: Page views and downloads</strong></p>
<p>A major new source of data online is the number of times articles are viewed. Page views and downloads can be defined in different ways and can be reached via a range of paths. Page views are an immediate measure of usage. Viewing a paper may involve less engagement than citation or bookmarking, but it can capture interactions with a much wider range of users.</p>
<p>The possibility of drawing demographic information from downloads has significant potential for the future in providing detailed information on who is reading an article, which may be valuable for determining, for example, whether research is reaching a target audience.</p>
<ul>
<li><p>Kind of usage</p>
<ul>
<li>It counts the number of people who have clicked on an article page or downloaded an article.</li>
</ul></li>
<li><p>Users</p>
<ul>
<li>Page views and downloads report on use by those who have access to articles. For publicly accessible articles this could be anyone; for subscription articles it is likely to be researchers.</li>
</ul></li>
<li><p>Limitations</p>
<ul>
<li><p>Page views are calculated in different ways and are not directly comparable across publishers. Standards are being developed but are not yet widely applied.</p></li>
<li><p>Counts of page views cannot easily distinguish between short-term visitors and those who engage more deeply with an article.</p></li>
<li><p>There are complications if an article appears in multiple places, for example at the journal website and a repository.</p></li>
</ul>
</li>
<li><p>Sources</p>
<ul>
<li><p>Some publishers and many data repositories make page view data available in some form. Publishers with public data include PLOS, Nature Publishing Group, Ubiquity Press, Co-Action Press, and Frontiers.</p></li>
<li><p>Data repositories, including Figshare and Dryad, provide page view and download information.</p></li>
<li><p>PubMed Central makes page views of articles hosted on that site available to depositing publishers. PLOS and a few other publishers make this available.</p></li>
</ul></li>
</ul>
<hr />
<hr />
<p><strong>Example: Analyzing bookmarks</strong></p>
<p>Tools for collecting and curating personal collections of literature, or web content, are now available online. They make it easy to make copies and build up indexes of articles. Bookmarking services can choose to provide information on the number of people who have bookmarked a paper.</p>
<p>Two important services targeted at researchers are Mendeley and CiteULike. Mendeley has the larger user base and provides richer statistics. Data include the number of users that who bookmarked a paper, groups that have collected a paper, and in some cases demographics of users, which can include discipline, career stage, and geography.</p>
<p>Bookmarks accumulate rapidly after publication and provide evidence of scholarly interest. They correlate quite well with the eventual number of citations. There are also public bookmarking services that provide a view onto wider interest in research articles.</p>
<ul>
<li><p>Kind of usage</p>
<ul>
<li><p>Bookmarking is a purposeful act. It may reveal more interest than a page view, but less than a citation.</p></li>
<li><p>Its uses are different from those captured by citations.</p></li>
<li><p>The bookmarks may include a variety of documents, such as papers for background reading, introductory material, position or policy papers, or statements of community positions.</p></li>
</ul></li>
<li><p>Users</p>
<ul>
<li><p>Academic-focused services provide information on use by researchers.</p></li>
<li><p>Each service has a different user profile in, for instance, sciences or social sciences.</p></li>
<li><p>All services have a geographical bias towards North America and Europe.</p></li>
<li><p>There is some demographic information, for instance, on countries where users are bookmarking the most.</p></li>
</ul></li>
<li><p>Limitations</p>
<ul>
<li><p>There is bias in coverage of services; for instance, Mendeley has good coverage of biomedical literature.</p></li>
<li><p>It can only report on activities of signed-up users.</p></li>
<li><p>It is not usually possible to determine why a bookmark has been created.</p></li>
</ul></li>
<li><p>Sources</p>
<ul>
<li><p>Mendeley and CiteULike both have public APIs that provide data that are freely available for reuse.</p></li>
<li><p>Most consumer bookmarking services provide some form of API, but this often has restrictions or limitations.</p></li>
</ul></li>
</ul>
<hr />
<hr />
<p><strong>Example: Discussions on social media</strong></p>
<p>Social media are one of the most valuable new services producing information about research usage. A growing number of researchers, policymakers, and technologists are on these services discussing research.</p>
<p>There are three major features of social media as a tool. First, among a large set of conversations, it is possible to discover a discussion about a specific paper. Second, Twitter makes it possible to identify groups discussing research and to learn whether they were potential targets of the research. Third, it is possible to reconstruct discussions to understand what paths research takes to users.</p>
<p>In the future it will be possible to identify target audiences and to ask whether they are being reached and how modified distribution might maximize that reach. This could be a powerful tool, particularly for research with social relevance.</p>
<p>Twitter provides the most useful data because discussions and the identity of those involved are public. Connections between users and the things they say are often available, making it possible to identify communities discussing work. However, the 140-character limit on Twitter messages (“tweets”) does not support extended critiques. Facebook has much less publicly available information— but being more private, it can be a site for frank discussion of research.</p>
<ul>
<li><p>Kind of usage</p>
<ul>
<li><p>Those discussing research are showing interest potentially greater than page views.</p></li>
<li><p>Often users are simply passing on a link or recommending an article.</p></li>
<li><p>It is possible to navigate to tweets and determine the level and nature of interest.</p></li>
<li><p>Conversations range from highly technical to trivial, so numbers should be treated with caution.</p></li>
<li><p>Highly tweeted or Facebooked papers also tend to have significant bookmarking and citation.</p></li>
<li><p>Professional discussions can be swamped when a piece of research captures public interest.</p></li>
</ul></li>
<li><p>Users</p>
<ul>
<li><p>The user bases and data sources for Twitter and Facebook are global and public.</p></li>
<li><p>There are strong geographical biases.</p></li>
<li><p>A rising proportion of researchers use Twitter and Facebook for professional activities.</p></li>
<li><p>Many journalists, policymakers, public servants, civil society groups, and others use social media.</p></li>
</ul></li>
<li><p>Limitations</p>
<ul>
<li><p>Frequent lack of explicit links to papers is a serious limitation.</p></li>
<li><p>Use of links is biased towards researchers and against groups not directly engaged in research.</p></li>
<li><p>There are demographic issues and reinforcement effects—retweeting leads to more retweeting in preference to other research—so analysis of numbers of tweets or likes is not always useful.</p></li>
</ul></li>
</ul>
<p>-–</p>
<hr />
<p><strong>Example: Recommendations</strong></p>
<p>A somewhat separate form of usage is direct expert recommendations. The best-known case of this is the F1000 service on which experts offer recommendations with reviews of specific research articles. Other services such as collections or personal recommendation services may be relevant here as well.</p>
<ul>
<li><p>Kind of usage</p>
<ul>
<li><p>Recommendations from specific experts show that particular outputs are worth looking at in detail, are important, or have some other value.</p></li>
<li><p>Presumably, recommendations are a result of in-depth reading and a high level of engagement.</p></li>
</ul>
</li>
<li><p>Users</p>
<ul>
<li><p>Recommendations are from a selected population of experts depending on the service in question.</p></li>
<li><p>In some cases this might be an algorithmic recommendation service.</p></li>
</ul></li>
<li><p>Limitations</p>
<ul>
<li><p>Recommendations are limited to the interests of the selected population of experts.</p></li>
<li><p>The recommendation system may be biased in terms of the interests of recommenders (e.g., towards—or away from—new theories vs. developing methodology) as well as their disciplines.</p></li>
<li><p>Recommendations are slow to build up.</p></li>
</ul></li>
</ul>
<hr />
</div>
<div id="sec:4-3" class="section level2">
<h2><span class="header-section-number">2.4</span> A functional view</h2>
<p>The descriptive view of data types and sources is a good place to start, but it is subject to change. Sources of data come and go, and even the classes of data types may expand and contract in the medium to long term. We also need a more functional perspective to help us understand how these sources of data relate to activities in the broader research enterprise.</p>
<p>Consider Figure <a href="chap-intro.html#fig:fig2">1.2</a> in Chapter <a href="chap-intro.html#chap:intro">Introduction</a>. The research enterprise has been framed as being made up of people who are generating outputs. The data that we consider in this chapter relate to connections between outputs, such as citations between research articles and tweets referring to articles. These connections are themselves created by people, as shown in Figure <a href="chap-web.html#fig:fig2-3">2.3</a>. The people in turn may be classed as belonging to certain categories or communities. What is interesting, and expands on the simplified picture of Figure <a href="chap-intro.html#fig:fig2">1.2</a>, is that many of these people are not professional researchers. Indeed, in some cases they may not be people at all but automated systems of some kind. This means we need to expand the set of actors we are considering. As described above, we are also expanding the range of outputs (or objects) that we are considering as well.</p>
<div class="figure" style="text-align: center"><span id="fig:fig2-3"></span>
<img src="ChapterWeb/figures/fig2-3.png" alt="A simplified model of online interactions between research outputs and the objects that refer to them" width="70%" />
<p class="caption">
Figure 2.3: A simplified model of online interactions between research outputs and the objects that refer to them
</p>
</div>
<p>In the simple model of Figure <a href="chap-web.html#fig:fig2-3">2.3</a>, there are three categories of things (nodes on the graph): objects, people, and the communities they belong to. Then there are the relationships between these elements (connections between nodes). Any given data source may provide information on different parts of this graph, and the information available is rarely complete or comprehensive. Data from different sources can also be difficult to integrate<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a>. As with any data integration, combining sources relies on being able to confidently identify those nodes that are common between data sources. Therefore identifying unique objects and people is critical to making progress.</p>
<p>These data are not necessarily public but many services choose to make some data available. An important characteristic of these data sources is that they are completely in the gift of the service provider. Data availability, its presentation, and upstream analysis can change without notice. Data are sometimes provided as a dump but is also frequently provided through an API.</p>
<p>An API is simply a tool that allows a program to interface with a service. APIs can take many different forms and be of varying quality and usefulness. In this section we will focus on one common type of API and examples of important publicly available APIs relevant to research communications. We will also cover combining APIs and the benefits and challenges of bringing multiple data sources together.</p>
<div id="sec:4-3.1" class="section level3">
<h3><span class="header-section-number">2.4.1</span> Relevant APIs and resources</h3>
<p>There is a wide range of other sources of information that can be used in combination with the APIs featured above to develop an overview of research outputs and of where and how they are being used. There are also other tools that can allow deeper analysis of the outputs themselves. Table 2.1 gives a partial list of key data sources and APIs that are relevant to the analysis of research outputs.</p>
<table style="width:100%;">
<colgroup>
<col width="11%" />
<col width="83%" />
<col width="2%" />
<col width="2%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Source</strong></th>
<th><strong>Description</strong></th>
<th align="center"><strong>API</strong></th>
<th align="center"><strong>Free</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td><strong>Bibliographic Data</strong></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td>PubMed</td>
<td>An online index that combines bibliographic data from Medline and PubMed Central. PubMed Central and Europe PubMed Central also provide information.</td>
<td align="center">Y</td>
<td align="center">Y</td>
</tr>
<tr class="odd">
<td>Web of Science</td>
<td>The bibliographic database provided by Thomson Reuters. The ISI Citation Index is also available.</td>
<td align="center">Y</td>
<td align="center">N</td>
</tr>
<tr class="even">
<td>Scopus</td>
<td>The bibliographic database provided by Elsevier. It also provides citation information.</td>
<td align="center">Y</td>
<td align="center">N</td>
</tr>
<tr class="odd">
<td>Crossref</td>
<td>Provides a range of bibliographic metadata and information obtained from members registering DOIs.</td>
<td align="center">Y</td>
<td align="center">Y</td>
</tr>
<tr class="even">
<td>Google Scholar</td>
<td>Provides a search index for scholarly objects and aggregates citation information.</td>
<td align="center">N</td>
<td align="center">Y</td>
</tr>
<tr class="odd">
<td>Microsoft Academic Search</td>
<td>Provides a search index for scholarly objects and aggregates citation information. Not as complete as Google Scholar, but has an API.</td>
<td align="center">Y</td>
<td align="center">Y</td>
</tr>
<tr class="even">
<td></td>
<td><strong>Social Media</strong></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td>Altmetric.com</td>
<td>A provider of aggregated data on social media and mainstream media attention of research outputs. Most comprehensive source of information across different social media and mainstream media conversations.</td>
<td align="center">Y</td>
<td align="center">N</td>
</tr>
<tr class="even">
<td>Twitter</td>
<td>Provides an API that allows a user to search for recent tweets and obtain some information on specific accounts.</td>
<td align="center">Y</td>
<td align="center">Y</td>
</tr>
<tr class="odd">
<td>Facebook</td>
<td>The Facebook API gives information on the number of pages, likes, and posts associated with specific web pages</td>
<td align="center">Y</td>
<td align="center">Y</td>
</tr>
<tr class="even">
<td></td>
<td><strong>Author Profiles</strong></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td>ORCID</td>
<td>Unique identifiers for research authors. Profiles include information on publication lists, grants, and affiliations.</td>
<td align="center">Y</td>
<td align="center">Y</td>
</tr>
<tr class="even">
<td>LinkedIn</td>
<td>CV-based profiles, projects, and publications.</td>
<td align="center">Y</td>
<td align="center">*</td>
</tr>
<tr class="odd">
<td></td>
<td><strong>Funder Information</strong></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td>Gateway to Research</td>
<td>A database of funding decisions and related outputs from Research Councils UK.</td>
<td align="center">Y</td>
<td align="center">Y</td>
</tr>
<tr class="odd">
<td>NIH Reporter</td>
<td>Online search for information on National Institutes of Health grants. Does not provide an API but a downloadable data set is available.</td>
<td align="center">N</td>
<td align="center">Y</td>
</tr>
<tr class="even">
<td>NSF Award Search</td>
<td>Online search for information on NSF grants. Does not provide an API but downloadable data sets by year are available.</td>
<td align="center">N</td>
<td align="center">Y</td>
</tr>
</tbody>
</table>
<p>*The data are restricted: sometimes fee based, other times not.</p>
<div style="text-align: center">
Table 2.1. Popular sources of data relevant to the analysis of research outputs
</div>
</div>
<div id="sec:4-3.2" class="section level3">
<h3><span class="header-section-number">2.4.2</span> RESTful APIs, returned data, and Python wrappers</h3>
<p>The APIs we will focus on here are all examples of RESTful services. REST stands for Representational State Transfer <span class="citation">[@RESTwiki; @fielding2002principled]</span>, but for our purposes it is most easily understood as a means of transferring data using web protocols. Other forms of API require additional tools or systems to work with, but RESTful APIs work directly over the web. This has the advantage that a human user can also with relative ease play with the API to understand how it works. Indeed, some websites work simply by formatting the results ofAPI calls.</p>
<p>As an example let us look at the Crossref API. This provides a range of information associated with Digital Object Identifiers (DOIs) registered with Crossref. DOIs uniquely identify an object, and Crossref DOIs refer to research objects, primarily (but not entirely) research articles. If you use a web browser to navigate to <a href="http://api.crossref.org/works/10.1093/nar/gni170" class="uri">http://api.crossref.org/works/10.1093/nar/gni170</a>, you should receive back a webpage that looks something like the following. (We have laid it out nicely to make it more readable.)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">{ <span class="st">&quot;status&quot;</span> <span class="op">:</span><span class="st"> &quot;ok&quot;</span>,
  <span class="st">&quot;message-type&quot;</span> <span class="op">:</span><span class="st"> &quot;work&quot;</span>,
  <span class="st">&quot;message-version&quot;</span> <span class="op">:</span><span class="st"> &quot;1.0.0&quot;</span>,
  <span class="st">&quot;message&quot;</span> <span class="op">:</span>
<span class="st">   </span>{ <span class="st">&quot;subtitle&quot;</span><span class="op">:</span><span class="st"> </span>[],
     <span class="st">&quot;subject&quot;</span> <span class="op">:</span><span class="st"> </span>[<span class="st">&quot;Genetics&quot;</span>],
     <span class="st">&quot;issued&quot;</span> <span class="op">:</span><span class="st"> </span>{ <span class="st">&quot;date-parts&quot;</span> <span class="op">:</span><span class="st"> </span>[[<span class="dv">2005</span>,<span class="dv">10</span>,<span class="dv">24</span>]] },
     <span class="st">&quot;score&quot;</span> <span class="op">:</span><span class="st"> </span><span class="fl">1.0</span>,
     <span class="st">&quot;prefix&quot;</span> <span class="op">:</span><span class="st"> &quot;http://id.crossref.org/prefix/10.1093&quot;</span>,
     <span class="st">&quot;author&quot;</span> <span class="op">:</span><span class="st"> </span>[ <span class="st">&quot;affiliation&quot;</span> <span class="op">:</span><span class="st"> </span>[],
                   <span class="st">&quot;family&quot;</span> <span class="op">:</span><span class="st"> &quot;Whiteford&quot;</span>,
                   <span class="st">&quot;given&quot;</span> <span class="op">:</span><span class="st"> &quot;N.&quot;</span>}],
     <span class="st">&quot;container-title&quot;</span> <span class="op">:</span><span class="st"> </span>[<span class="st">&quot;Nucleic Acids Research&quot;</span>],
     <span class="st">&quot;reference-count&quot;</span> <span class="op">:</span><span class="st"> </span><span class="dv">0</span>,
     <span class="st">&quot;page&quot;</span> <span class="op">:</span><span class="st"> &quot;e171-e171&quot;</span>,
     <span class="st">&quot;deposited&quot;</span> <span class="op">:</span><span class="st"> </span>{<span class="st">&quot;date-parts&quot;</span> <span class="op">:</span><span class="st"> </span>[[<span class="dv">2013</span>,<span class="dv">8</span>,<span class="dv">8</span>]],
                    <span class="st">&quot;timestamp&quot;</span> <span class="op">:</span><span class="st"> </span><span class="dv">1375920000000</span>},
     <span class="st">&quot;issue&quot;</span> <span class="op">:</span><span class="st"> &quot;19&quot;</span>,
     <span class="st">&quot;title&quot;</span> <span class="op">:</span>
<span class="st">       </span>[<span class="st">&quot;An analysis of the feasibility of short read sequencing&quot;</span>],
     <span class="st">&quot;type&quot;</span> <span class="op">:</span><span class="st"> &quot;journal-article&quot;</span>,
     <span class="st">&quot;DOI&quot;</span> <span class="op">:</span><span class="st"> &quot;10.1093/nar/gni170&quot;</span>,
     <span class="st">&quot;ISSN&quot;</span> <span class="op">:</span><span class="st"> </span>[<span class="st">&quot;0305-1048&quot;</span>,<span class="st">&quot;1362-4962&quot;</span>],
     <span class="st">&quot;URL&quot;</span> <span class="op">:</span><span class="st"> &quot;http://dx.doi.org/10.1093/nar/gni170&quot;</span>,
     <span class="st">&quot;source&quot;</span> <span class="op">:</span><span class="st"> &quot;Crossref&quot;</span>,
     <span class="st">&quot;publisher&quot;</span> <span class="op">:</span><span class="st"> &quot;Oxford University Press (OUP)&quot;</span>,
     <span class="st">&quot;indexed&quot;</span> <span class="op">:</span><span class="st"> </span>{<span class="st">&quot;date-parts&quot;</span> <span class="op">:</span><span class="st"> </span>[[<span class="dv">2015</span>,<span class="dv">6</span>,<span class="dv">8</span>]],
                  <span class="st">&quot;timestamp&quot;</span> <span class="op">:</span><span class="st"> </span><span class="dv">1433777291246</span>},
     <span class="st">&quot;volume&quot;</span> <span class="op">:</span><span class="st"> &quot;33&quot;</span>,
     <span class="st">&quot;member&quot;</span> <span class="op">:</span><span class="st"> &quot;http://id.crossref.org/member/286&quot;</span>
   }
<span class="er">}</span></code></pre></div>
<p>This is a package of JavaScript Object Notation (JSON)<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a> data returned in response to a query. The query is contained entirely in the URL, which can be broken up into pieces: the root URL (<a href="http://api.crossref.org" class="uri">http://api.crossref.org</a>) and a data “query,” in this case made up of a “field” (<code>works</code>) and an identifier (the DOI <code>10.1093/nar/gni170</code>). The Crossref API provides information about the article identified with this specific DOI.</p>
</div>
</div>
<div id="sec:4-4" class="section level2">
<h2><span class="header-section-number">2.5</span> Programming against an API</h2>
<p>Programming against an API involves constructing HTTP requests and parsing the data that are returned. Here we use the Crossref API to illustrate how this is done. Crossref is the provider of DOIs used by many publishers to uniquely identify scholarly works. Crossref is not the only organization to provide DOIs. The scholarly communication space DataCite is another important provider. The documentation is available at the Crossref website <span class="citation">[@crossref]</span>.</p>
<p>Once again the <code>requests</code> Python library provides a series of convenience functions that make it easier to make HTTP calls and to process returned JSON. Our first step is to import the module and set a base URL variable.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>import requests
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>BASE_URL =<span class="st"> &quot;http://api.crossref.org/&quot;</span></code></pre></div>
<p>A simple example is to obtain metadata for an article associated with a specific DOI. This is a straightforward call to the Crossref API, similar to what we saw earlier.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>doi =<span class="st"> &quot;10.1093/nar/gni170&quot;</span>
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>query =<span class="st"> &quot;works/&quot;</span>
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>url =<span class="st"> </span>BASE_URL <span class="op">+</span><span class="st"> </span>query <span class="op">+</span><span class="st"> </span>doi
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>response =<span class="st"> </span><span class="kw">requests.get</span>(url)
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>url
http<span class="op">:</span><span class="er">//</span>api.crossref.org<span class="op">/</span>works<span class="op">/</span><span class="fl">10.1093</span><span class="op">/</span>nar<span class="op">/</span>gni170
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>response.status_code
<span class="dv">200</span></code></pre></div>
<p>The <code>response</code> object that the <code>requests</code> library has created has a range of useful information, including the URL called and the response code from the web server (in this case 200, which means everything is OK). We need the JSON body from the response object (which is currently text from the perspective of our script) converted to a Python dictionary. The <code>requests</code> module provides a convenient function for performing this conversion, as the following code shows. (All strings in the output are in Unicode, hence the <code>u´</code> notation.)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>response_dict =<span class="st"> </span><span class="kw">response.json</span>()
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>response_dict
{ u<span class="st">&#39;message&#39;</span> <span class="op">:</span>
<span class="st">  </span>{ u<span class="st">&#39;DOI&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;10.1093/nar/gni170&#39;</span>,
    u<span class="st">&#39;ISSN&#39;</span> <span class="op">:</span><span class="st"> </span>[ u<span class="st">&#39;0305-1048&#39;</span>, u<span class="st">&#39;1362-4962&#39;</span> ],
    u<span class="st">&#39;URL&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;http://dx.doi.org/10.1093/nar/gni170&#39;</span>,
    u<span class="st">&#39;author&#39;</span> <span class="op">:</span><span class="st"> </span>[ {u<span class="st">&#39;affiliation&#39;</span> <span class="op">:</span><span class="st"> </span>[],
                   u<span class="st">&#39;family&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Whiteford&#39;</span>,
                   u<span class="st">&#39;given&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;N.&#39;</span>} ],
    u<span class="st">&#39;container-title&#39;</span> <span class="op">:</span><span class="st"> </span>[ u<span class="st">&#39;Nucleic Acids Research&#39;</span> ],
    u<span class="st">&#39;deposited&#39;</span> <span class="op">:</span><span class="st"> </span>{ u<span class="st">&#39;date-parts&#39;</span> <span class="op">:</span><span class="st"> </span>[[<span class="dv">2013</span>, <span class="dv">8</span>, <span class="dv">8</span>]],
                     u<span class="st">&#39;timestamp&#39;</span> <span class="op">:</span><span class="st"> </span><span class="dv">1375920000000</span> },
    u<span class="st">&#39;indexed&#39;</span> <span class="op">:</span><span class="st"> </span>{ u<span class="st">&#39;date-parts&#39;</span> <span class="op">:</span><span class="st"> </span>[[<span class="dv">2015</span>, <span class="dv">6</span>, <span class="dv">8</span>]],
                   u<span class="st">&#39;timestamp&#39;</span> <span class="op">:</span><span class="st"> </span><span class="dv">1433777291246</span> },
    u<span class="st">&#39;issue&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;19&#39;</span>,
    u<span class="st">&#39;issued&#39;</span> <span class="op">:</span><span class="st"> </span>{ u<span class="st">&#39;date-parts&#39;</span> <span class="op">:</span><span class="st"> </span>[[<span class="dv">2005</span>, <span class="dv">10</span>, <span class="dv">24</span>]] },
    u<span class="st">&#39;member&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;http://id.crossref.org/member/286&#39;</span>,
    u<span class="st">&#39;page&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;e171-e171&#39;</span>,
    u<span class="st">&#39;prefix&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;http://id.crossref.org/prefix/10.1093&#39;</span>,
    u<span class="st">&#39;publisher&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Oxford University Press (OUP)&#39;</span>,
    u<span class="st">&#39;reference-count&#39;</span> <span class="op">:</span><span class="st"> </span><span class="dv">0</span>,
    u<span class="st">&#39;score&#39;</span> <span class="op">:</span><span class="st"> </span><span class="fl">1.0</span>,
    u<span class="st">&#39;source&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Crossref&#39;</span>,
    u<span class="st">&#39;subject&#39;</span> <span class="op">:</span><span class="st"> </span>[u<span class="st">&#39;Genetics&#39;</span>],
    u<span class="st">&#39;subtitle&#39;</span> <span class="op">:</span><span class="st"> </span>[],
    u<span class="st">&#39;title&#39;</span> <span class="op">:</span><span class="st"> </span>[u<span class="st">&#39;An analysis of the feasibility of short read sequencing&#39;</span>],
    u<span class="st">&#39;type&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;journal-article&#39;</span>,
    u<span class="st">&#39;volume&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;33&#39;</span>
  },
  u<span class="st">&#39;message-type&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;work&#39;</span>,
  u<span class="st">&#39;message-version&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;1.0.0&#39;</span>,
  u<span class="st">&#39;status&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;ok&#39;</span>
}</code></pre></div>
<p>This data object can now be processed in whatever way the user wishes, using standard manipulation techniques.</p>
<p>The Crossref API can, of course, do much more than simply look up article metadata. It is also valuable as a search resource and for cross-referencing information by journal, funder, publisher, and other criteria. More details can be found at the Crossref website.</p>
</div>
<div id="sec:4-4.1" class="section level2">
<h2><span class="header-section-number">2.6</span> Using the ORCID API via a wrapper</h2>
<p>ORCID, which stands for “Open Research and Contributor Identifier” (see <a href="orcid.org" class="uri">orcid.org</a>; see also <span class="citation">[@haak2012orcid]</span>), is a service that provides unique identifiers for researchers. Researchers can claim an ORCID profile and populate it with references to their research works, funding and affiliations. ORCID provides an API for interacting with this information. For many APIs there is a convenient Python wrapper that can be used. The ORCID–Python wrapper works with the ORCID v1.2 API to make various API calls straightforward. This wrapper only works with the public ORCID API and can therefore only access publicly available data.</p>
<p>Using the API and wrapper together provides a convenient means of getting this information. For instance, given an ORCID, it is straightforward to get profile information. Here we get a list of publications associated with my ORCID and look at the the first item on the list.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>import orcid
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>cn =<span class="st"> </span><span class="kw">orcid.get</span>(<span class="st">&quot;0000-0002-0068-716X&quot;</span>)
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>cn
<span class="op">&lt;</span>Author Cameron Neylon, ORCID <span class="dv">0000</span><span class="op">-</span><span class="dv">0002</span><span class="op">-</span><span class="dv">0068</span><span class="op">-</span>716X<span class="op">&gt;</span>
<span class="er">&gt;&gt;</span><span class="st"> </span>cn.publications[<span class="dv">0</span>]
<span class="op">&lt;</span>Publication <span class="st">&quot;Principles for Open Scholarly Infrastructures-v1&quot;</span><span class="op">&gt;</span></code></pre></div>
<p>The wrapper has created Python objects that make it easier to work with and manipulate the data. It is common to take the return from an API and create objects that behave as would be expected in Python. For instance, the <code>publications</code> object is a list populated with publications (which are also Python-like objects). Each publication in the list has its own attributes, which can then be examined individually. In this case the external IDs attribute is a list of further objects that include a DOI for the article and the ISSN of the journal the article was published in.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="kw">len</span>(cn.publications)
<span class="dv">70</span>
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>cn.publications[<span class="dv">12</span>].external_ids
[<span class="op">&lt;</span>ExternalID DOI<span class="op">:</span><span class="fl">10.1371</span><span class="op">/</span>journal.pbio.<span class="dv">1001677</span><span class="op">&gt;</span>, <span class="op">&lt;</span>ExternalID ISSN<span class="op">:</span><span class="dv">1545</span><span class="op">-</span><span class="dv">7885</span><span class="op">&gt;</span>]</code></pre></div>
<p>As a simple example of data processing, we can iterate over the list of publications to identify those for which a DOI has been provided. In this case we can see that of the 70 publications listed in this ORCID profile (at the time of testing), 66 have DOIs.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>exids =<span class="st"> </span>[]
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="cf">for</span> pub <span class="cf">in</span> cn.publications<span class="op">:</span>
<span class="st">        </span><span class="cf">if</span> pub.external_ids<span class="op">:</span>
<span class="st">        </span>exids =<span class="st"> </span>exids <span class="op">+</span><span class="st"> </span>pub.external_ids
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>DOIs =<span class="st"> </span>[exid.id <span class="cf">for</span> exid <span class="cf">in</span> exids <span class="cf">if</span> exid.type <span class="op">==</span><span class="st"> &quot;DOI&quot;</span>]
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="kw">len</span>(DOIs)
<span class="dv">66</span></code></pre></div>
<p>Wrappers generally make operating with an API simpler and cleaner by abstracting away the details of making HTTP requests. Achieving the same by directly interacting with the ORCID API would require constructing the appropriate URLs and parsing the returned data into a usable form. Where a wrapper is available it is generally much easier to use. However, wrappers may not be actively developed and may lag the development of the API. Where possible, use a wrapper that is directly supported or recommended by the API provider.</p>
</div>
<div id="sec:4-5" class="section level2">
<h2><span class="header-section-number">2.7</span> Quality, scope, and management</h2>
<p>The examples in the previous section are just a small dip into the surface of the data available, but we already can see a number of issues that are starting to surface. A great deal of care needs to be taken when using these data, and a researcher will need to apply subject matter knowledge as well as broader data management expertise<a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a>. Some of the core issues are as follows:</p>
<p> <strong>Integration</strong></p>
<p>In the examples given above with Crossref and ORCID, we used a known identifier (a DOI or an ORCID). Integrating data from Crossref to supplement the information from an ORCID profile is possible, but it depends on the linking of identifiers. Note that for the profile data we obtained, only 66 or the 70 items had DOIs. Data integration across multiple data sources that reference DOIs is straightforward for those objects that have DOIs, and messy or impossible for those that do not. In general, integration is possible, but it depends on a means of cross-referencing between data sets. Unique identifiers that are common to both are extremely powerful but only exist in certain cases (see also Chapter <a href="chap-link.html#chap:link">Record Linkage</a>).</p>
<p> <strong>Coverage</strong></p>
<p> Without a population frame, it is difficult to know whether the information that can be captured is comprehensive. For example, “the research literature” is at best a vague concept. A variety of indexes, some openly available (PubMed, Crossref), some proprietary (Scopus, Web of Knowledge, many others), cover different partially overlapping segments of this corpus of work. Each index has differing criteria for inclusion and differing commitments to completeness. Sampling of “the literature” is therefore impossible, and the choice of index used for any study can make a substantial difference to the conclusions.</p>
<p> <strong>Completeness</strong></p>
<p>Alongside the question of coverage (how broad is a data source?), with web data and opt-in services we also need to probe the completeness of a data set. In the example above, 66 of 70 objects have a DOI <em>registered</em>. This does not mean that those four other objects do not have a DOI, just that there are none included in the ORCID record. Similarly, ORCID profiles only exist for a subset of researchers at this stage. Completeness feeds into integration challenges. While many researchers have a Twitter profile and many have an ORCID profile, only a small subset of ORCID profiles provide a link to a Twitter profile. See below for a worked example.</p>
<p> <strong>Scope</strong></p>
<p>In survey data sets, the scope is defined by the question being asked. This is not the case with much of these new data. For example, the challenges listed above for research articles, traditionally considered the bedrock of research outputs, at least in the natural sciences, are much greater for other forms of research outputs. Increasingly, the data generated from research projects, software, materials, and tools, as well as reports and presentations, are being shared by researchers in a variety of settings. Some of these are formal mechanisms for publication, such as large disciplinary databases, books, and software repositories, and some are highly informal. Any study of (a subset of) these outputs has as its first challenge the question of how to limit the corpus to be studied.</p>
<p> <strong>Source and validity</strong></p>
<p>The challenges described above relate to the identification and counting of outputs. As we start to address questions of how these outputs are being used, the issues are compounded. To illustrate some of the difficulties that can arise, we examine the number of citations that have been reported for a single sample article on a biochemical methodology <span class="citation">[@chan2007covalent]</span>. This article has been available for eight years and has accumulated a reasonable number of citations for such an article over that time.</p>
<p>However, the exact number of citations identified varies radically, depending on the data source. Scopus finds 40, while Web of Science finds only 38. A Google Scholar search performed on the same date identified 59. These differences relate to the size of the corpus from which inward citations are being counted. Web of Science has the smallest database, with Scopus being larger and Google Scholar substantially larger again. Thus the size of the index not only affects output counting, it can also have a substantial effect on any analysis that uses that corpus. Alongside the size of the corpus, the means of analysis can also have an effect. For the same article, PubMed Central reports 10 citations but Europe PubMed Central reports 18, despite using a similar corpus. The distinction lies in differences in the methodology used to mine the corpus for citations.</p>
<p> <strong>Identifying the underlying latent variable</strong></p>
<p>These issues multiply as we move into newer forms of data. These sparse and incomplete sources of data require different treatment than more traditional structured and comprehensive forms of data. They are more useful as a way of identifying activities than of quantifying or comparing them. Nevertheless, they can provide new insight into the processes of knowledge dissemination and community building that are occurring online.</p>
</div>
<div id="sec:4-6" class="section level2">
<h2><span class="header-section-number">2.8</span> Integrating data from multiple sources</h2>
<p>We often must work across multiple data sources to gather the information needed to answer a research question. A common pattern is to search in one location to create a list of identifiers and then use those identifiers to query another API. In the ORCID example above, we created a list of DOIs from a single ORCID profile. We could use those DOIs to obtain further information from the Crossref API and other sources. This models a common path for analysis of research outputs: identifying a corpus and then seeking information on its performance.</p>
<p>In this example, we will build on the ORCID and Crossref examples to collect a set of work identifiers from an ORCID profile and use a range of APIs to identify additional metadata as well as information on the performance of those articles. In addition to the ORCID API, we will use the PLOS Lagotto API. Lagotto is the software that was built to support the Article Level Metrics program at PLOS, the open access publisher, and its API provides information on various metrics of PLOS articles. A range of other publishers and service providers, including Crossref, also provide an instance of this API, meaning the same tools can be used to collect information on articles from a range of sources.</p>
<div id="sec:4-6.1" class="section level3">
<h3><span class="header-section-number">2.8.1</span> The Lagotto API</h3>
<p>The module <code>pyalm</code> is a wrapper for the Lagotto API, which is served from a range of hosts. We will work with two instances in particular: one run by PLOS, and the Crossref DOI Event Tracker (DET, recently renamed Crossref Event Data) pilot service. We first need to provide the details of the URLs for these instances to our wrapper. Then we can obtain some information for a single DOI to see what the returned data look like.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>import pyalm
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>pyalm.config.APIS =<span class="st"> </span>{<span class="st">&#39;plos&#39;</span> <span class="op">:</span><span class="st"> </span>{<span class="st">&#39;url&#39;</span> <span class="op">:</span>
<span class="er">&gt;&gt;</span><span class="st">         &#39;http://alm.plos.org/api/v5/articles&#39;</span>},
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st">         &#39;det&#39;</span> <span class="op">:</span><span class="st"> </span>{<span class="st">&#39;url&#39;</span> <span class="op">:</span>
<span class="er">&gt;&gt;</span><span class="st">         &#39;http://det.labs.crossref.org/api/v5/articles&#39;</span>}
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st">         </span>}
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>det_alm_test =<span class="st"> </span><span class="kw">pyalm.get_alm</span>(<span class="st">&#39;10.1371/journal.pbio.1001677&#39;</span>,
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st">         </span><span class="dt">info=</span><span class="st">&#39;detail&#39;</span>, <span class="dt">instance=</span><span class="st">&#39;det&#39;</span>)
det_alm_test
{ <span class="st">&#39;articles&#39;</span> <span class="op">:</span><span class="st"> </span>[<span class="op">&lt;</span>ArticleALM Expert Failure<span class="op">:</span><span class="st"> </span>Re<span class="op">-</span>evaluating Research Assessment, DOI <span class="fl">10.1371</span><span class="op">/</span>journal.pbio.<span class="dv">1001677</span><span class="op">&gt;</span>],
  <span class="st">&#39;meta&#39;</span> <span class="op">:</span><span class="st"> </span>{u<span class="st">&#39;error&#39;</span> <span class="op">:</span><span class="st"> </span>None, u<span class="st">&#39;page&#39;</span> <span class="op">:</span><span class="st"> </span><span class="dv">1</span>,
            u<span class="st">&#39;total&#39;</span> <span class="op">:</span><span class="st"> </span><span class="dv">1</span>, u<span class="st">&#39;total_pages&#39;</span> <span class="op">:</span><span class="st"> </span><span class="dv">1</span>}
}</code></pre></div>
<p>The library returns a Python dictionary containing two elements. The articles key contains the actual data and the meta key includes general information on the results of the interaction with the API. In this case the library has returned one page of results containing one object (because we only asked about one DOI). If we want to collect a lot of data, this information helps in the process of paging through results. It is common for APIs to impose some limit on the number of results returned, so as to ensure performance. By default the Lagotto API has a limit of 50 results.</p>
<p>The articles key holds a list of ArticleALM objects as its value. Each ArticleALM object has a set of internal attributes that contain information on each of the metrics that the Lagotto instance collects. These are derived from various data providers and are called <em>sources</em>. Each can be accessed by name from a dictionary called “sources.” The <code>iterkeys()</code> function provides an iterator that lets us loop over the set of keys in a dictionary. Within the source object there is a range of information that we will dig into.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>article =<span class="st"> </span><span class="kw">det_alm_test.get</span>(<span class="st">&#39;articles&#39;</span>)[<span class="dv">0</span>]
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>article.title
u<span class="st">&#39;Expert Failure: Re-evaluating Research Assessment&#39;</span>
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="cf">for</span> source <span class="cf">in</span> <span class="kw">article.sources.iterkeys</span>()<span class="op">:</span>
<span class="er">&gt;&gt;</span><span class="st">    </span>print source, article.sources[source].metrics.total
reddit <span class="dv">0</span>
datacite <span class="dv">0</span>
pmceuropedata <span class="dv">0</span>
wikipedia <span class="dv">1</span>
pmceurope <span class="dv">0</span>
citeulike <span class="dv">0</span>
pubmed <span class="dv">0</span>
facebook <span class="dv">0</span>
wordpress <span class="dv">0</span>
pmc <span class="dv">0</span>
mendeley <span class="dv">0</span>
crossref <span class="dv">0</span></code></pre></div>
<p> The DET service only has a record of citations to this article from Wikipedia. As we will see below, the PLOS service returns more results. This is because some of the sources are not yet being queried by DET.</p>
<p>Because this is a PLOS paper we can also query the PLOS Lagotto instance for the same article.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>plos_alm_test =<span class="st"> </span><span class="kw">pyalm.get_alm</span>(<span class="st">&#39;10.1371/journal.pbio.1001677&#39;</span>, <span class="dt">info=</span><span class="st">&#39;detail&#39;</span>, <span class="dt">instance=</span><span class="st">&#39;plos&#39;</span>)
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>article_plos =<span class="st"> </span><span class="kw">plos_alm_test.get</span>(<span class="st">&#39;articles&#39;</span>)[<span class="dv">0</span>]
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>article_plos.title
u<span class="st">&#39;Expert Failure: Re-evaluating Research Assessment&#39;</span>
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="cf">for</span> source <span class="cf">in</span> <span class="kw">article_plos.sources.iterkeys</span>()<span class="op">:</span>
<span class="er">&gt;&gt;</span><span class="st">    </span>print source, article_plos.sources[source].metrics.total
datacite <span class="dv">0</span>
twitter <span class="dv">130</span>
pmc <span class="dv">610</span>
articlecoveragecurated <span class="dv">0</span>
pmceurope <span class="dv">1</span>
pmceuropedata <span class="dv">0</span>
researchblogging <span class="dv">0</span>
scienceseeker <span class="dv">0</span>
copernicus <span class="dv">0</span>
f1000 <span class="dv">0</span>
wikipedia <span class="dv">1</span>
citeulike <span class="dv">0</span>
wordpress <span class="dv">2</span>
openedition <span class="dv">0</span>
reddit <span class="dv">0</span>
nature <span class="dv">0</span>
relativemetric <span class="dv">125479</span>
figshare <span class="dv">0</span>
facebook <span class="dv">1</span>
mendeley <span class="dv">14</span>
crossref <span class="dv">3</span>
plos_comments <span class="dv">2</span>
articlecoverage <span class="dv">0</span>
counter <span class="dv">12551</span>
scopus <span class="dv">2</span>
pubmed <span class="dv">1</span>
orcid <span class="dv">3</span></code></pre></div>
<p>The PLOS instance provides a greater range of information but also seems to be giving larger numbers than the DET instance in many cases. For those sources that are provided by both API instances, we can compare the results returned.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="cf">for</span> source <span class="cf">in</span> <span class="kw">article.sources.iterkeys</span>()<span class="op">:</span>
<span class="er">&gt;&gt;</span><span class="st">    </span>print source, article.sources[source].metrics.total,
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st">    </span>article_plos.sources[source].metrics.total
reddit <span class="dv">0</span> <span class="dv">0</span>
datacite <span class="dv">0</span> <span class="dv">0</span>
pmceuropedata <span class="dv">0</span> <span class="dv">0</span>
wikipedia <span class="dv">1</span> <span class="dv">1</span>
pmceurope <span class="dv">0</span> <span class="dv">1</span>
citeulike <span class="dv">0</span> <span class="dv">0</span>
pubmed <span class="dv">0</span> <span class="dv">1</span>
facebook <span class="dv">0</span> <span class="dv">1</span>
wordpress <span class="dv">0</span> <span class="dv">2</span>
pmc <span class="dv">0</span> <span class="dv">610</span>
mendeley <span class="dv">0</span> <span class="dv">14</span>
crossref <span class="dv">0</span> <span class="dv">3</span></code></pre></div>
<p>The PLOS Lagotto instance is collecting more information and has a wider range of information sources. Comparing the results from the PLOS and DET instances illustrates the issues of coverage and completeness discussed previously. The data may be sparse for a variety of reasons, and it is important to have a clear idea of the strengths and weaknesses of a particular data source or aggregator. In this case the DET instance is returning information for some sources for which it is does not yet have data.</p>
<p>We can dig deeper into the events themselves that the metrics.total count aggregates. The API wrapper collects these into an event object within the source object. These contain the JSON returned from the API in most cases. For instance, the Crossref source is a list of JSON objects containing information on an article that cites our article of interest. The first citation event in the list is a citation from the <em>Journal of the Association for Information Science and Technology</em> by Du et al.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>article_plos.sources[<span class="st">&#39;crossref&#39;</span>].events[<span class="dv">0</span>]
{u<span class="st">&#39;event&#39;</span> <span class="op">:</span>
<span class="st">   </span>{u<span class="st">&#39;article_title&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;The effects of research level and article type on the differences between citation metrics and F1000 recommendations&#39;</span>,
    u<span class="st">&#39;contributors&#39;</span> <span class="op">:</span>
<span class="st">      </span>{u<span class="st">&#39;contributor&#39;</span> <span class="op">:</span>
<span class="st">         </span>[ { u<span class="st">&#39;contributor_role&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;author&#39;</span>,
             u<span class="st">&#39;first_author&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;true&#39;</span>,
             u<span class="st">&#39;given_name&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Jian&#39;</span>,
             u<span class="st">&#39;sequence&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;first&#39;</span>,
             u<span class="st">&#39;surname&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Du&#39;</span> },
           { u<span class="st">&#39;contributor_role&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;author&#39;</span>,
             u<span class="st">&#39;first_author&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;false&#39;</span>,
             u<span class="st">&#39;given_name&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Xiaoli&#39;</span>,
             u<span class="st">&#39;sequence&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;additional&#39;</span>,
             u<span class="st">&#39;surname&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Tang&#39;</span>},
           { u<span class="st">&#39;contributor_role&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;author&#39;</span>,
             u<span class="st">&#39;first_author&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;false&#39;</span>,
             u<span class="st">&#39;given_name&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Yishan&#39;</span>,
             u<span class="st">&#39;sequence&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;additional&#39;</span>,
             u<span class="st">&#39;surname&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Wu&#39;</span>} ]
         },
     u<span class="st">&#39;doi&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;10.1002/asi.23548&#39;</span>,
     u<span class="st">&#39;first_page&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;n/a&#39;</span>,
     u<span class="st">&#39;fl_count&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;0&#39;</span>,
     u<span class="st">&#39;issn&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;23301635&#39;</span>,
     u<span class="st">&#39;journal_abbreviation&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;J Assn Inf Sci Tec&#39;</span>,
     u<span class="st">&#39;journal_title&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Journal of the Association for Information Science and Technology&#39;</span>,
     u<span class="st">&#39;publication_type&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;full_text&#39;</span>,
     u<span class="st">&#39;year&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;2015&#39;</span>
   },
   u<span class="st">&#39;event_csl&#39;</span> <span class="op">:</span><span class="st"> </span>{
     u<span class="st">&#39;author&#39;</span> <span class="op">:</span>
<span class="st">           </span>[ { u<span class="st">&#39;family&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Du&#39;</span>, u<span class="st">&#39;given&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Jian&#39;</span>},
             {u<span class="st">&#39;family&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Tang&#39;</span>, u<span class="st">&#39;given&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Xiaoli&#39;</span>},
             {u<span class="st">&#39;family&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Wu&#39;</span>, u<span class="st">&#39;given&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Yishan&#39;</span>} ],
     u<span class="st">&#39;container-title&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Journal of the Association for Information Science and Technology&#39;</span>,
     u<span class="st">&#39;issued&#39;</span> <span class="op">:</span><span class="st"> </span>{u<span class="st">&#39;date-parts&#39;</span> <span class="op">:</span><span class="st"> </span>[[<span class="dv">2015</span>]]},
     u<span class="st">&#39;title&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;The Effects Of Research Level And Article Type On The Differences Between Citation Metrics And F1000 Recommendations&#39;</span>,
     u<span class="st">&#39;type&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;article-journal&#39;</span>,
     u<span class="st">&#39;url&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;http://doi.org/10.1002/asi.23548&#39;</span>
    },
  u<span class="st">&#39;event_url&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;http://doi.org/10.1002/asi.23548&#39;</span>
}</code></pre></div>
<p>Another source in the PLOS data is Twitter. In the case of the Twitter events (individual tweets), this provides the text of the tweet, user IDs, user names, URL of the tweet, and the date. We can see from the length of the events list that there are at least 130 tweets that link to this article.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="kw">len</span>(article_plos.sources[<span class="st">&#39;twitter&#39;</span>].events)
<span class="dv">130</span></code></pre></div>
<p>Again, noting the issues of coverage, scope, and completeness, it is important to consider the limitations of these data. This is a lower bound as it represents search results returned by searching the Twitter API for the DOI or URL of the article. Other tweets that discuss the article may not include a link, and the Twitter search API also has limitations that can lead to incomplete results. The number must therefore be seen as both incomplete and a lower bound.</p>
<p>We can look more closely at data on the first tweet on the list. Bear in mind that the order of the list is not necessarily special. This is not the first tweet about this article chronologically.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>article_plos.sources[<span class="st">&#39;twitter&#39;</span>].events[<span class="dv">0</span>]
{ u<span class="st">&#39;event&#39;</span> <span class="op">:</span><span class="st"> </span>{u<span class="st">&#39;created_at&#39;</span><span class="op">:</span><span class="st"> </span>u<span class="st">&#39;2013-10-08T21:12:28Z&#39;</span>,
  u<span class="st">&#39;id&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;387686960585641984&#39;</span>,
  u<span class="st">&#39;text&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;We have identified the Higgs boson; it is surely not beyond our reach to make research assessment useful http://t.co/Odcm8dVRSU#PLOSBiology&#39;</span>,
  u<span class="st">&#39;user&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;catmacOA&#39;</span>,
  u<span class="st">&#39;user_name&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;Catriona MacCallum&#39;</span>,
  u<span class="st">&#39;user_profile_image&#39;</span> <span class="op">:</span>
<span class="st">  </span>u<span class="st">&#39;http://a0.twimg.com/profile_images/1779875975/CM_photo_reduced_normal.jpg&#39;</span>},
  u<span class="st">&#39;event_time&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;2013-10-08T21:12:28Z&#39;</span>,
  u<span class="st">&#39;event_url&#39;</span> <span class="op">:</span><span class="st"> </span>u<span class="st">&#39;http://twitter.com/catmacOA/status/387686960585641984&#39;</span>
}</code></pre></div>
<p>We could use the Twitter API to understand more about this person. For instance, we could look at their Twitter followers and whom they follow, or analyze the text of their tweets for topic modeling. Much work on social media interactions is done with this kind of data, using forms of network and text analysis described elsewhere in this book<a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a>.</p>
<p>A different approach is to integrate these data with information from another source. We might be interested, for instance, in whether the author of this tweet is a researcher, or whether they have authored research papers. One thing we could do is search the ORCID API to see if there are any ORCID profiles that link to this Twitter handle.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>twitter_search =<span class="st"> </span><span class="kw">orcid.search</span>(<span class="st">&quot;catmacOA&quot;</span>)
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="cf">for</span> result <span class="cf">in</span> twitter_search<span class="op">:</span>
<span class="er">&gt;&gt;</span><span class="st">     </span>print <span class="kw">unicode</span>(result)
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st">     </span>print result.researcher_urls<span class="er">}</span>
<span class="op">&lt;</span>Author Catriona MacCallum, ORCID <span class="dv">0000</span><span class="op">-</span><span class="dv">0001</span><span class="op">-</span><span class="dv">9623</span><span class="op">-</span><span class="dv">2225</span><span class="op">&gt;</span>
[<span class="op">&lt;</span>Website twitter [http<span class="op">:</span><span class="er">//</span>twitter.com<span class="op">/</span>catmacOA]<span class="op">&gt;</span>]</code></pre></div>
<p>So the person with this Twitter handle seems to have an ORCID profile. That means we can also use ORCID to gather more information on their outputs. Perhaps they have authored work which is relevant to our article?</p>
<pre><code>&gt;&gt; cm = orcid.get(&quot;0000-0001-9623-2225&quot;)
&gt;&gt; for pub in cm.publications[0:5]:
&gt;&gt;     print pub.title
The future is open: opportunities for publishers and institutions
Open Science and Reporting Animal Studies: Who&#39;s Accountable?
Expert Failure: Re-evaluating Research Assessment
Why ONE Is More Than 5
Reporting Animal Studies: Good Science and a Duty of Care</code></pre>
<p>From this analysis we can show that this tweet is actually from one of my co-authors of the article.</p>
<p>To make this process easier we write the convenience function shown in Listing 2.2, to go from a Twitter user handle to try and find an ORCID for that person.</p>
<pre id="fig:web:py2" style="PythonStyle" caption="Python code to find ORCID for Twitter handle" label="fig:web:py2"><code># Take a twitter handle or user name and return an ORCID
def twitter2orcid(twitter_handle,
                  resp = &#39;orcid&#39;, search_depth = 10):
    search = orcid.search(twitter_handle)
    s = [r for r in search]
    orc = None
    i = 0
    while i &lt; search_depth and orc == None and i &lt; len(s):
        arr = [(&#39;twitter.com&#39; in website.url)
               for website in s[i].researcher_urls]
        if True in arr:
            index = arr.index(True)
            url = s[i].researcher_urls[index].url
            if url.lower().endswith(twitter_handle.lower()):
                orc = s[i].orcid
                return orc
        i+=1
     return None</code></pre>
<div style="text-align: center">
Listing 2.2. Python code to find ORCID for Twitter handle
</div>
<p><br></p>
<p>Let us do a quick test of the function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="kw">twitter2orcid</span>(<span class="st">&#39;catmacOA&#39;</span>)
u<span class="st">&#39;0000-0001-9623-2225&#39;</span></code></pre></div>
</div>
<div id="sec:4-6.2" class="section level3">
<h3><span class="header-section-number">2.8.2</span> Working with a corpus</h3>
<p>In this case we will continue as previously to collect a set of works from a single ORCID profile. This collection could just as easily be a date range, or subject search at a range of other APIs. The target is to obtain a set of identifiers (in this case DOIs) that can be used to precisely query other data sources. This is a general pattern that reflects the issues of scope and source discussed above. The choice of how to construct a corpus to analyze will strongly affect the results and the conclusions that can be drawn.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="co"># As previously, collect DOIs available from an ORCID profile</span>
<span class="er">&gt;&gt;</span><span class="st"> </span>cn =<span class="st"> </span><span class="kw">orcid.get</span>(<span class="st">&quot;0000-0002-0068-716X&quot;</span>)
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>exids =<span class="st"> </span>[]
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="cf">for</span> pub <span class="cf">in</span> cn.publications<span class="op">:</span>
<span class="er">&gt;&gt;</span><span class="st">     </span><span class="cf">if</span> pub.external_ids<span class="op">:</span>
<span class="er">&gt;&gt;</span><span class="st">         </span>exids =<span class="st"> </span>exids <span class="op">+</span><span class="st"> </span>pub.external_ids
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>DOIs =<span class="st"> </span>[exid.id <span class="cf">for</span> exid <span class="cf">in</span> exids <span class="cf">if</span> exid.type <span class="op">==</span><span class="st"> &quot;DOI&quot;</span>]
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="kw">len</span>(DOIs)
<span class="dv">66</span></code></pre></div>
<p>We have recovered 66 DOIs from the ORCID profile. Note that we have not obtained an identifier for every work, as not all have DOIs. This result illustrates an important point about data integration. In practice it is generally not worth the effort of attempting to integrate data on objects unless they have a unique identifier or key that can be used in multiple data sources, hence the focus on DOIs and ORCIDs in these examples. Even in our search of the ORCID API for profiles that are associated with a Twitter account, we used the Twitter handle as a unique ID to search on.</p>
<p>While it is possible to work with author names or the titles of works directly, disambiguating such names and titles is substantially more difficult than working with unique identifiers. Other chapters (in particular, Chapter <a href="chap-link.html#chap:link">Record Linkage</a>) deal with issues of data cleaning and disambiguation. Much work has been done on this basis, but increasingly you will see that the first step in any analysis is simply to discard objects without a unique ID that can be used across data sources.</p>
<p>We can obtain data for these from the DET API. As is common with many APIs, there is a limit to how many queries can be simultaneously run, in this case 50, so we divide our query into batches.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>batches =<span class="st"> </span>[DOIs[<span class="dv">0</span><span class="op">:</span><span class="dv">50</span>], DOIs[<span class="dv">51</span><span class="op">:-</span><span class="dv">1</span>]]
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>det_alms =<span class="st"> </span>[]
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="cf">for</span> batch <span class="cf">in</span> batches<span class="op">:</span>
<span class="er">&gt;&gt;</span><span class="st">     </span>alms_response =<span class="st"> </span><span class="kw">pyalm.get_alm</span>(batch, <span class="dt">info=</span><span class="st">&quot;detail&quot;</span>, <span class="dt">instance=</span><span class="st">&quot;det&quot;</span>)
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st">     </span><span class="kw">det_alms.extend</span>(<span class="kw">alms_response.get</span>(<span class="st">&#39;articles&#39;</span>))
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="kw">len</span>(det_alms)
<span class="dv">24</span></code></pre></div>
<p>The DET API only provides information on a subset of Crossref DOIs. The process that Crossref has followed to populate its database has focused on more recently published articles, so only 24 responses are received in this case for the 66 DOIs we queried on. A good exercise would be to look at which of the DOIs are found and which are not. Let us see how much interesting data is available in the subset of DOIs for which we have data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">    <span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="cf">for</span> r <span class="cf">in</span> [d <span class="cf">for</span> d <span class="cf">in</span> det_alms <span class="cf">if</span> d.sources[<span class="st">&#39;wikipedia&#39;</span>].metrics.total <span class="op">!=</span><span class="st"> </span><span class="dv">0</span>]<span class="op">:</span>
<span class="st">    </span><span class="er">&gt;&gt;</span><span class="st">     </span>print r.title
    <span class="op">&gt;</span><span class="er">&gt;</span><span class="st">     </span>print <span class="st">&#39;     &#39;</span>, r.sources[<span class="st">&#39;pmceurope&#39;</span>].metrics.total, <span class="st">&#39;pmceurope citations&#39;</span>
    <span class="op">&gt;</span><span class="er">&gt;</span><span class="st">     </span>print <span class="st">&#39;     &#39;</span>, r.sources[<span class="st">&#39;wikipedia&#39;</span>].metrics.total, <span class="st">&#39;wikipedia citations&#39;</span>
    Architecting the Future of Research Communication<span class="op">:</span><span class="st"> </span>Building the Models and Analytics <span class="cf">for</span> an Open Access Future
          <span class="dv">1</span> pmceurope citations
          <span class="dv">1</span> wikipedia citations
    Expert Failure<span class="op">:</span><span class="st"> </span>Re<span class="op">-</span>evaluating Research Assessment
          <span class="dv">0</span> pmceurope citations
          <span class="dv">1</span> wikipedia citations
    LabTrove<span class="op">:</span><span class="st"> </span>A Lightweight, Web Based, Laboratory <span class="st">&quot;Blog&quot;</span> as a Route towards a Marked Up Record of Work <span class="cf">in</span> a Bioscience Research Laboratory
          <span class="dv">0</span> pmceurope citations
          <span class="dv">1</span> wikipedia citations
    The lipidome and proteome of oil bodies from Helianthus <span class="kw">annuus</span> (common sunflower)
          <span class="dv">2</span> pmceurope citations
          <span class="dv">1</span> wikipedia citations</code></pre></div>
<p>As discussed above, this shows that the DET instance, while it provides information on a greater number of DOIs, has less complete data on each DOI at this stage. Only four of the 24 responses have Wikipedia references. You can change the code to look at the full set of 24, which shows only sparse data. The PLOS Lagotto instance provides more data but only on PLOS articles. However, it does provide data on all PLOS articles, going back earlier than the set returned by the DET instance. We can collect the set of articles from the profile published by PLOS.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">    <span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>plos_dois =<span class="st"> </span>[]
    <span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="cf">for</span> doi <span class="cf">in</span> DOIs<span class="op">:</span>
<span class="st">    </span><span class="er">&gt;&gt;</span><span class="st">     </span><span class="co"># Quick and dirty, should check Crossref API for publisher</span>
<span class="st">    </span><span class="er">&gt;&gt;</span><span class="st">     </span><span class="cf">if</span> <span class="kw">doi.startswith</span>(<span class="st">&#39;10.1371&#39;</span>)<span class="op">:</span>
<span class="st">    </span><span class="er">&gt;&gt;</span><span class="st">         </span><span class="kw">plos_dois.append</span>(doi)
    <span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="kw">len</span>(plos_dois)
    <span class="dv">7</span>

    <span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>plos_alms =<span class="st"> </span><span class="kw">pyalm.get_alm</span>(plos_dois, <span class="dt">info=</span><span class="st">&#39;detail&#39;</span>, <span class="dt">instance=</span><span class="st">&#39;plos&#39;</span>)<span class="kw">.get</span>(<span class="st">&#39;articles&#39;</span>)
    <span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="cf">for</span> article <span class="cf">in</span> plos_alms<span class="op">:</span>
<span class="st">    </span><span class="er">&gt;&gt;</span><span class="st">     </span>print article.title
    <span class="op">&gt;</span><span class="er">&gt;</span><span class="st">     </span>print <span class="st">&#39;     &#39;</span>, article.sources[<span class="st">&#39;crossref&#39;</span>].metrics.total, <span class="st">&#39;Crossref citations&#39;</span>
    <span class="op">&gt;</span><span class="er">&gt;</span><span class="st">     </span>print <span class="st">&#39;     &#39;</span>, article.sources[<span class="st">&#39;twitter&#39;</span>].metrics.total, <span class="st">&#39;tweets&#39;</span>
    Architecting the Future of Research Communication<span class="op">:</span><span class="st"> </span>Building the Models and Analytics <span class="cf">for</span> an Open Access Future
          <span class="dv">2</span> Crossref citations
          <span class="dv">48</span> tweets
    Expert Failure<span class="op">:</span><span class="st"> </span>Re<span class="op">-</span>evaluating Research Assessment
          <span class="dv">3</span> Crossref citations
          <span class="dv">130</span> tweets
    LabTrove<span class="op">:</span><span class="st"> </span>A Lightweight, Web Based, Laboratory <span class="st">&quot;Blog&quot;</span> as a Route towards a Marked Up Record of Work <span class="cf">in</span> a Bioscience Research Laboratory
          <span class="dv">6</span> Crossref citations
          <span class="dv">1</span> tweets
    More Than Just Access<span class="op">:</span><span class="st"> </span>Delivering on a Network<span class="op">-</span>Enabled Literature
          <span class="dv">4</span> Crossref citations
          <span class="dv">95</span> tweets
    Article<span class="op">-</span>Level Metrics and the Evolution of Scientific Impact
          <span class="dv">24</span> Crossref citations
          <span class="dv">5</span> tweets
    Optimal Probe Length Varies <span class="cf">for</span> Targets with High Sequence Variation<span class="op">:</span><span class="st"> </span>Implications <span class="cf">for</span> Probe
    Library Design <span class="cf">for</span> Resequencing Highly Variable Genes
          <span class="dv">2</span> Crossref citations
          <span class="dv">1</span> tweets
    Covalent Attachment of Proteins to Solid Supports and Surfaces via Sortase<span class="op">-</span>Mediated Ligation
          <span class="dv">40</span> Crossref citations
          <span class="dv">0</span> tweets</code></pre></div>
<p>From the previous examples we know that we can obtain information on citing articles and tweets associated with these 66 articles. From that initial corpus we now have a collection of up to 86 related articles (cited and citing), a few hundred tweets that refer to (some of) those articles, and perhaps 500 people if we include authors of both articles and tweets. Note how for each of these links our query is limited, so we have a subset of all the related objects and agents. At this stage we probably have duplicate articles (one article might cite multiple in our set of seven) and duplicate people (authors in common between articles and authors who are also tweeting).</p>
<p> These data could be used for network analysis, to build up a new corpus of articles (by following the citation links), or to analyze the links between authors and those tweeting about the articles. We do not pursue an in-depth analysis here, but will gather the relevant objects, deduplicate them as far as possible, and count how many we have in preparation for future analysis.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="co"># Collect all citing DOIs &amp; author names from citing articles</span>
<span class="er">&gt;&gt;</span><span class="st"> </span>citing_dois =<span class="st"> </span>[]
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>citing_authors =<span class="st"> </span>[]
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="cf">for</span> article <span class="cf">in</span> plos_alms<span class="op">:</span>
<span class="er">&gt;&gt;</span><span class="st">     </span><span class="cf">for</span> cite <span class="cf">in</span> article.sources[<span class="st">&#39;crossref&#39;</span>].events<span class="op">:</span>
<span class="er">&gt;&gt;</span><span class="st">         </span><span class="kw">citing_dois.append</span>(cite[<span class="st">&#39;event&#39;</span>][<span class="st">&#39;doi&#39;</span>])
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st">         </span><span class="co"># Use &#39;extend&#39; because the element is a list</span>
<span class="er">&gt;&gt;</span><span class="st">         </span><span class="kw">citing_authors.extend</span>(cite[<span class="st">&#39;event_csl&#39;</span>][<span class="st">&#39;author&#39;</span>])
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>print <span class="st">&#39;</span><span class="ch">\n</span><span class="st">Before de-deduplication:&#39;</span>
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>print <span class="st">&#39;  &#39;</span>, <span class="kw">len</span>(citing_dois), <span class="st">&#39;DOIs&#39;</span>
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>print <span class="st">&#39;  &#39;</span>, <span class="kw">len</span>(citing_authors), <span class="st">&#39;citing authors&#39;</span>
<span class="op">&gt;</span><span class="er">&gt;</span>
<span class="er">&gt;&gt;</span><span class="st"> </span><span class="co"># Easiest way to deduplicate is to convert to a Python set</span>
<span class="er">&gt;&gt;</span><span class="st"> </span>citing_dois =<span class="st"> </span><span class="kw">set</span>(citing_dois)
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>citing_authors =<span class="st"> </span><span class="kw">set</span>([author[<span class="st">&#39;given&#39;</span>] <span class="op">+</span><span class="st"> </span>author[<span class="st">&#39;family&#39;</span>] <span class="cf">for</span> author <span class="cf">in</span> citing_authors])
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>print <span class="st">&#39;</span><span class="ch">\n</span><span class="st">After de-deduplication:&#39;</span>
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>print <span class="st">&#39;  &#39;</span>, <span class="kw">len</span>(citing_dois), <span class="st">&#39;DOIs&#39;</span>
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>print <span class="st">&#39;  &#39;</span>, <span class="kw">len</span>(citing_authors), <span class="st">&#39;citing authors&#39;</span>

Before de<span class="op">-</span>deduplication<span class="op">:</span>
<span class="st">  </span><span class="dv">81</span> DOIs
  <span class="dv">346</span> citing authors

After de<span class="op">-</span>deduplication<span class="op">:</span>
<span class="st">  </span><span class="dv">78</span> DOIs
  <span class="dv">278</span> citing authors</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="co"># Collect all tweets, usernames; check for ORCIDs</span>
<span class="er">&gt;&gt;</span><span class="st"> </span>tweet_urls =<span class="st"> </span><span class="kw">set</span>()
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>twitter_handles =<span class="st"> </span><span class="kw">set</span>()
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="cf">for</span> article <span class="cf">in</span> plos_alms<span class="op">:</span>
<span class="er">&gt;&gt;</span><span class="st">     </span><span class="cf">for</span> tweet <span class="cf">in</span> article.sources[<span class="st">&#39;twitter&#39;</span>].events<span class="op">:</span>
<span class="er">&gt;&gt;</span><span class="st">         </span><span class="kw">tweet_urls.add</span>(tweet[<span class="st">&#39;event_url&#39;</span>])
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st">         </span><span class="kw">twitter_handles.add</span>(tweet[<span class="st">&#39;event&#39;</span>][<span class="st">&#39;user&#39;</span>])
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="co"># No need to explicitly deduplicate as we created sets directly</span>
<span class="er">&gt;&gt;</span><span class="st"> </span>print <span class="kw">len</span>(tweet_urls), <span class="st">&#39;tweets&#39;</span>
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>print <span class="kw">len</span>(twitter_handles), <span class="st">&#39;Twitter users&#39;</span>

<span class="dv">280</span> tweets
<span class="dv">210</span> Twitter users</code></pre></div>
<p>It could be interesting to look at which Twitter users interact most with the articles associated with this ORCID profile. To do that we would need to create not a set but a list, and then count the number of duplicates in the list. The code could be easily modified to do this. Another useful exercise would be to search ORCID for profiles corresponding to citing authors. The best way to do this would be to obtain ORCIDs associated with each of the citing articles. However, because ORCID data are sparse and incomplete, there are two limitations here. First, the author may not have an ORCID. Second, the article may not be explicitly linked to another article. Try searching ORCID for the DOIs associated with each of the citing articles.</p>
<p> In this case we will look to see how many of the Twitter handles discussing these articles are associated with an ORCID profile we can discover. This in turn could lead to more profiles and more cycles of analysis to build up a network of researchers interacting through citation and on Twitter. Note that we have inserted a delay between calls. This is because we are making a larger number of API calls (one for each Twitter handle). It is considered polite to keep the pace at which calls are made to an API to a reasonable level. The ORCID API does not post suggested limits at the moment, but delaying for a second between calls is reasonable.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>tweet_orcids =<span class="st"> </span>[]
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="cf">for</span> handle <span class="cf">in</span> twitter_handles<span class="op">:</span>
<span class="er">&gt;&gt;</span><span class="st">     </span>orc =<span class="st"> </span><span class="kw">twitter2orcid</span>(handle)
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st">     </span><span class="cf">if</span> orc<span class="op">:</span>
<span class="er">&gt;&gt;</span><span class="st">         </span><span class="kw">tweet_orcids.append</span>(orc)
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st">     </span><span class="kw">time.sleep</span>(<span class="dv">1</span>) <span class="co"># wait one second between each call to the ORCID API</span>
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>print <span class="kw">len</span>(tweet_orcids)
<span class="dv">12</span></code></pre></div>
<p>In this case we have identified 12 ORCID profiles that we can link positively to tweets about this set of articles. This is a substantial underestimate of the likely number of ORCIDs associated with these tweets. However, relatively few ORCIDs have Twitter accounts registered as part of the profile. To gain a broader picture a search and matching strategy would need to be applied. Nevertheless, for these 12 we can look more closely into the profiles.</p>
<p>The first step is to obtain the actual profile information for each of the 12 ORCIDs that we have found. Note that at the moment what we have is the ORCIDs themselves, not the retrieved profiles.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span>orcs =<span class="st"> </span>[]
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="cf">for</span> id <span class="cf">in</span> tweet_orcids<span class="op">:</span>
<span class="er">&gt;&gt;</span><span class="st">     </span><span class="kw">orcs.append</span>(<span class="kw">orcid.get</span>(id))</code></pre></div>
<p>With the profiles retrieved we can then take a look at who they are, and check that we do in fact have sensible Twitter handles associated with them. We could use this to build up the network of related authors and Twitter users for further analysis.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="er">&gt;</span><span class="st"> </span><span class="cf">for</span> orc <span class="cf">in</span> orcs<span class="op">:</span>
<span class="er">&gt;&gt;</span><span class="st">     </span>i =<span class="st"> </span>[(<span class="st">&#39;twitter.com&#39;</span> <span class="cf">in</span> website.url) <span class="cf">for</span> website <span class="cf">in</span> orc.researcher_urls]<span class="kw">.index</span>(True)
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st">     </span>twitter_url =<span class="st"> </span>orc.researcher_urls[i].url
<span class="op">&gt;</span><span class="er">&gt;</span><span class="st">     </span>print orc.given_name, orc.family_name, orc.orcid, twitter_url
Catriona MacCallum <span class="dv">0000</span><span class="op">-</span><span class="dv">0001</span><span class="op">-</span><span class="dv">9623</span><span class="op">-</span><span class="dv">2225</span> http<span class="op">:</span><span class="er">//</span>twitter.com<span class="op">/</span>catmacOA
John Dupuis <span class="dv">0000</span><span class="op">-</span><span class="dv">0002</span><span class="op">-</span><span class="dv">6066</span><span class="op">-</span>690X https<span class="op">:</span><span class="er">//</span>twitter.com<span class="op">/</span>dupuisj
Johannes Velterop <span class="dv">0000</span><span class="op">-</span><span class="dv">0002</span><span class="op">-</span><span class="dv">4836</span><span class="op">-</span><span class="dv">6568</span> https<span class="op">:</span><span class="er">//</span>twitter.com<span class="op">/</span>Villavelius
Stuart Lawson <span class="dv">0000</span><span class="op">-</span><span class="dv">0002</span><span class="op">-</span><span class="dv">1972</span><span class="op">-</span><span class="dv">8953</span> https<span class="op">:</span><span class="er">//</span>twitter.com<span class="op">/</span>Lawsonstu
Nelson Piedra <span class="dv">0000</span><span class="op">-</span><span class="dv">0003</span><span class="op">-</span><span class="dv">1067</span><span class="op">-</span><span class="dv">8707</span> http<span class="op">:</span><span class="er">//</span>www.twitter.com<span class="op">/</span>nopiedra
Iryna Kuchma <span class="dv">0000</span><span class="op">-</span><span class="dv">0002</span><span class="op">-</span><span class="dv">2064</span><span class="op">-</span><span class="dv">3439</span> https<span class="op">:</span><span class="er">//</span>twitter.com<span class="op">/</span>irynakuchma
Frank Huysmans <span class="dv">0000</span><span class="op">-</span><span class="dv">0002</span><span class="op">-</span><span class="dv">3468</span><span class="op">-</span><span class="dv">9032</span> https<span class="op">:</span><span class="er">//</span>twitter.com<span class="op">/</span>fhuysmans
Salvatore Salvi VICIDOMINI <span class="dv">0000</span><span class="op">-</span><span class="dv">0001</span><span class="op">-</span><span class="dv">5086</span><span class="op">-</span><span class="dv">7401</span> https<span class="op">:</span><span class="er">//</span>twitter.com<span class="op">/</span>SalViVicidomini
William Gunn <span class="dv">0000</span><span class="op">-</span><span class="dv">0002</span><span class="op">-</span><span class="dv">3555</span><span class="op">-</span><span class="dv">2054</span> http<span class="op">:</span><span class="er">//</span>twitter.com<span class="op">/</span>mrgunn
Stephen Curry <span class="dv">0000</span><span class="op">-</span><span class="dv">0002</span><span class="op">-</span><span class="dv">0552</span><span class="op">-</span><span class="dv">8870</span> https<span class="op">:</span><span class="er">//</span>twitter.com<span class="op">/</span>Stephen_Curry
Cameron Neylon <span class="dv">0000</span><span class="op">-</span><span class="dv">0002</span><span class="op">-</span><span class="dv">0068</span><span class="op">-</span>716X http<span class="op">:</span><span class="er">//</span>twitter.com<span class="op">/</span>cameronneylon
Graham Steel <span class="dv">0000</span><span class="op">-</span><span class="dv">0003</span><span class="op">-</span><span class="dv">4681</span><span class="op">-</span><span class="dv">8011</span> https<span class="op">:</span><span class="er">//</span>twitter.com<span class="op">/</span>McDawg</code></pre></div>
</div>
</div>
<div id="sec:4-7" class="section level2">
<h2><span class="header-section-number">2.9</span> Working with the graph of relationships</h2>
<p>In the above examples we started with the profile of an individual, used this to create a corpus of works, which in turn led us to other citing works (and their authors) and commentary about those works on Twitter (and the people who wrote those comments)<a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a>. Along the way we built up a graph of relationships between objects and people. In this section we will look at this model of the data and how it reveals limitations and strengths of these forms of data and what can be done with them.</p>
<div id="sec:4-7.1" class="section level3">
<h3><span class="header-section-number">2.9.1</span> Citation links between articles</h3>
<p>A citation in a research article (or a policy document or working paper) defines a relationship between that citing article and the cited article. The exact form of the relationship is generally poorly defined, at least at the level of large-scale data sets. A citation might be referring to previous work, indicating the source of data, or supporting (or refuting) an idea. While efforts have been made to codify citation types, they have thus far gained little traction.</p>
<p>In our example we used a particular data source (Crossref) for information about citations. As previously discussed, this will give different results than other sources (such as Thomson Reuters, Scopus, or Google Scholar) because other sources look at citations from a different set of articles and collect them in a different way. The completeness of the data will always be limited. We could use the data to clearly connect the citing articles and their authors because author information is generally available in bibliographic metadata. However, we would have run into problems if we had only had names. ORCIDs can provide a way to uniquely identify authors and ensure that our graph of relationships is clean.</p>
<p>A <em>citation</em> is a reference from an object of one type to an object of the same type. We also sought to link social media activity with specific articles. Rather than a link between objects that are the same (articles) we started to connect different kinds of objects together. We are also expanding the scope of the communities (i.e., people) that might be involved. While we focused on the question of which Twitter handles were connected with researchers, we could just as easily have focused on trying to discover which comments came from people who are <em>not</em> researchers.</p>
<p>We used the Lagotto API at PLOS to obtain this information. The PLOS API in turn depends on the Twitter Search API. A tweet that refers explicitly to a research article, perhaps via a Crossref DOI, can be discovered, and a range of services do these kinds of checks. These services generally rely either on Twitter Search or, more generally, on a search of “the firehose,” a dump of all Twitter data that are available for purchase. The distinction is important because Twitter Search does not provide either a complete or a consistent set of results. In addition, there will be many references to research articles that do not contain a unique identifier, or even a link. These are more challenging to discover. As with citations, the completeness of any data set will always be limited.</p>
<p>However, the set of all tweets is a more defined set of objects than the set of all “articles.” Twitter is a specific social media service with a defined scope. “Articles” is a broad class of objects served by a very wide range of services. Twitter is clearly a subset of all discussions and is highly unlikely to be representative of “all discussions.” Equally the set of all objects with a Crossref DOI, while defined, is unlikely to be representative of all articles.</p>
<div class="figure" style="text-align: center"><span id="fig:fig2-4"></span>
<img src="ChapterWeb/figures/fig2-4.png" alt="A functional view of proxies and relationships" width="70%" />
<p class="caption">
Figure 2.4: A functional view of proxies and relationships
</p>
</div>
<p>Expanding on Figure <a href="chap-web.html#fig:fig2-3">2.3</a>, we show in Figure <a href="chap-web.html#fig:fig2-4">2.4</a> agents and actors (people) and outputs. We place both agents and outputs into categories that may be more or less well defined. In practice our analysis is limited to those objects that we discover by using some “selector” (circles in this diagram), which may or may not have a close correspondence with the “real” categories (shown with graded shapes). Our aim is to identify, aggregate, and in some cases count the relationships between and within categories of objects; for instance, citations are relationships between formal research outputs. A tweet may have a relationship (“links to”) with a specific formally published research output. Both tweets and formal research outputs relate to specific agents (“authors”) of the content.</p>
</div>
<div id="sec:4-7.1.2" class="section level3">
<h3><span class="header-section-number">2.9.2</span> Categories, sources, and connections</h3>
<p>We can see in this example a distinction between categories of objects of interest (articles, discussions, people) and sources of information on subsets of those categories (Crossref, Twitter, ORCID). Any analysis will depend on one or more data sources, and in turn be limited by the coverage of those data sources. The selectors used to generate data sets from these sources will have their own limitations.</p>
<p>Similar to a query on a structured data set, the selector itself may introduce bias. The crucial difference between filtering on a comprehensive (or at least representative) data set and the data sources we are discussing here is that these data sources are by their very nature incomplete. Survey data may include biases introduced in the way that the survey itself is structured or the sampling is designed, but the intent is to be comprehensive. Many of these new forms of data make no attempt to be comprehensive or avowedly avoid such an attempt.</p>
<p>Understanding this incompleteness is crucial to understanding the forms of inference that can be made from these data. Sampling is only possible within a given source or corpus, and this limits the conclusions that can be drawn to the scope of that corpus. It is frequently possible to advance a plausible argument to claim that such findings are more broadly applicable, but it is crucial to avoid assuming that this is the case. In particular, it is important to be clear about what data sources a finding applies to and where the boundary between the strongly evidenced finding and a claim about its generalization lies. Much of the literature on scholarly communications and research impact is poor on this point.</p>
<p>If this is an issue in identifying the objects of interest, it is even more serious when seeking to identify the relationships between them, which are, after all generally the thing of interest. In some cases there are reasonably good sources of data between objects of the same class (at least those available from the same data sources) such as citations between journal articles or links between tweets. However, as illustrated in this chapter, detecting relationships between tweets and articles is much more challenging.</p>
<p>These issues can arise both due to the completeness of the data source itself (e.g., ORCID currently covers only a subset of researchers; therefore, the set of author–article relationships is limited) or due to the challenges of identification (e.g., in the Twitter case above) or due to technical limitations at source (the difference between the Twitter search API and the firehose). In addition, because the source data and the data services are both highly dynamic and new, there is often a mismatch. Many services tracking Twitter data only started collecting data relatively recently. There is a range of primary and secondary data sources working to create more complete data sets. However, once again it is important to treat all of these data as sparse and limited as well as highly dynamic and changeable.</p>
</div>
<div id="sec:4-7.1.3" class="section level3">
<h3><span class="header-section-number">2.9.3</span> Data availability and completeness</h3>
<p>With these caveats in hand and the categorization discussed above, we can develop a mapping of what data sources exist, what objects those data sources inform us about, the completeness of those data sources, and how well the relationships between the different data sources are tracked. Broadly speaking, data sources concern themselves with either agents (mostly people) or objects (articles, books, tweets, posts), while additionally providing additional data about the relationships of the agents or objects that they describe with other objects or agents.</p>
<p>The five broad types of data described above are often treated as ways of categorizing the data source. They are more properly thought of as relationships between objects, or between objects and agents. Thus, for example, citations are relationships between articles; the tweets that we are considering are actually relationships between specific Twitter posts and articles; and “views” are an event associating a reader (agent) with an article. The last case illustrates that often we do not have detailed information on the relationship but merely a count of them. Relationships between agents (such as co-authorship or group membership) can also be important.</p>
<p>With this framing in hand, we can examine which types of relationships we can obtain data on. We need to consider both the <em>quality</em> of data available and the <em>completeness</em> of the data availability. These metrics are necessarily subjective and any analysis will be a personal view of a particular snapshot in time. Nevertheless, some major trends are available.</p>
<p>We have growing and improving data on the relationships between a wide range of objects and agents and traditional scholarly outputs. Although it is sparse and incomplete in many places, <em>nontraditional</em> information on <em>traditional</em> outputs is becoming more available and increasingly rich. By contrast, references from traditional outputs to nontraditional outputs are weaker and data that allow us to understand the relationships between nontraditional outputs is very sparse.</p>
<p>In the context of the current volume, a major weakness is our inability to triangulate around people and communities. While it may be possible to collect a set of co-authors from a bibliographic data source and to identify a community of potential research users on Twitter or Facebook, it is extremely challenging to connect these different sets. If a community is discussing an article or book on social media, it is almost impossible to ascertain whether the authors (or, more generically, interested parties such as authors of cited works or funders) are engaged in that conversation.</p>
</div>
<div id="sec:4-7.1.4" class="section level3">
<h3><span class="header-section-number">2.9.4</span> The value of sparse dynamic data</h3>
<p>Two clear messages arise from our analysis. These new forms of data are incomplete or sparse, both in quality and in coverage, and they change. A data source that is poor today may be much improved tomorrow. A query performed one minute may give different results the next. This can be both a strength and a weakness: data are up to the minute, giving a view of relationships as they form (and break), but it makes ensuring consistency within analyses and across analyses challenging. Compared to traditional surveys, these data sources cannot be relied on as either representative samples or to be stable.</p>
<p>A useful question to ask, therefore, is what kind of statements these data <em>can</em> support. Questions like this will be necessarily different from the questions that can be posed with high-quality survey data. More often they provide an existence proof that something has happened—but they cannot, conversely, show that it has not. They enable some forms of comparison and determination of the characteristics of activity in some cases.</p>
<p><strong>Provide evidence that …</strong></p>
<p>Because much of the data that we have is sparse, the absence of an indicator cannot reliably be taken to mean an absence of activity. For example, a lack of Mendeley bookmarks may not mean that a paper is not being saved by researchers, just that those who do save the article are not using Mendeley to do it. Similarly, a lack of tweets about an article does not mean the article is not being discussed. But we can use the data that do exist to show that some activity is occurring. Here are some examples:</p>
<ul>
<li><p>Provide evidence that relevant communities are aware of a specific paper. I identified the fact that a paper by Jewkes et al. <span class="citation">[@jewkes2011relationship]</span> was mentioned by crisis centers, sexual health organizations, and discrimination support groups in South Africa when I was looking for University of Cape Town papers that had South African Twitter activity using Altmetric.com.</p></li>
<li><p>Provide evidence that a relatively under-cited paper is having a research impact. There is a certain kind of research article, often a method description or a position paper, that is influential without being (apparently) heavily cited. For instance, the <em>PLoS One</em> article by Shen et al. <span class="citation">[@shen2009simplified]</span> has a respectable 14,000 views and 116 Mendeley bookmarks, but a relatively (for the number of views) small number of WoS citations (19) compared to, say, another article, by Leahy et al. <span class="citation">[@leahy2010genome]</span> and also in <em>PLoS One</em>, that is similar in age and number of views but has many more citations.</p></li>
<li><p>Provide evidence of public interest in some topic. Many articles at the top of lists ordered by views or social media mentions are of ephemeral (or prurient) interest—the usual trilogy of sex, drugs, and rock and roll. However, if we dig a little deeper, a wide range of articles surface, often not highly cited but clearly of wider interest. For example, an article on Y-chromosome distribution in Afghanistan <span class="citation">[@haber2012afghanistan]</span> has high page views and Facebook activity among papers with a Harvard affiliation but is not about sex, drugs, nor rock and roll. Unfortunately, because this is Facebook data we cannot see <em>who</em> is talking about it, which limits our ability to say <em>which</em> groups are talking about it, which could be quite interesting.</p></li>
</ul>
<p>  <strong>Compare …</strong></p>
<p>Comparisons using social media or download statistics need real care. As noted above, the data are sparse so it is important that comparisons are fair. Also, comparisons need to be on the basis of something that the data can actually tell you: for example, “which article is discussed more by this online community,” <em>not</em> “which article is discussed more.”</p>
<ul>
<li><p>Compare the extent to which these articles are discussed by this online patient group, or possibly specific online communities in general. Here the online communities might be a proxy for a broader community, or there might be a specific interest in knowing whether the dissemination strategy reaches this community. It is clear that in the longer term social media will be a substantial pathway for research to reach a wide range of audiences, and understanding which communities are discussing what research will help us to optimize the communication.</p></li>
<li><p>Compare the readership of these articles in these countries. One thing that most data sources are weak on at the moment is demographics, but in principle the data are there. Are these articles that deal with diseases of specific areas actually being viewed by readers in those areas? If not, why not? Do they have Internet access, could lay summaries improve dissemination, are they going to secondary online sources instead?</p></li>
<li><p>Compare the communities discussing these articles online. Is most conversation driven by science communicators or by researchers? Are policymakers, or those who influence them, involved? What about practitioner communities? These comparisons require care, and simple counting rarely provides useful information. But understanding which people within which networks are driving conversations can give insight into who is aware of the work and whether it is reaching target audiences.</p></li>
</ul>
<p><strong>What flavor is it?</strong></p>
<p>Priem et al. <span class="citation">[@priem2012altmetrics]</span> provide a thoughtful analysis of the PLOS Article Level Metrics data set. They used principal component analysis to define different “flavors of impact” based on the way different combinations of signals seemed to point to different kinds of interest. Many of the above use cases are variants on this theme—what kind of article is this? Is it a policy piece, of public interest? Is it of interest to a niche research community or does it have wider public implications? Is it being used in education or in health practice? And to what extent are these different kinds of use independent from each other?</p>
<p> It is important to realize that these kinds of data are proxies of things that we do not truly understand. They are signals of the flow of information down paths that we have not mapped. To me this is the most exciting possibility and one we are only just starting to What can these signals tell us about the underlying pathways down which information flows? How do different combinations of signals tell us about who is using that information now, and how they might be applying it in the future? Correlation analysis cannot answer these questions, but more sophisticated approaches might. And with that information in hand we could truly <em>design</em> scholarly communication systems to maximize their reach, value, and efficiency.</p>
</div>
</div>
<div id="sec:4-8" class="section level2">
<h2><span class="header-section-number">2.10</span> Bringing it together: Tracking pathways to impact</h2>
<p>Collecting data on research outputs and their performance clearly has significant promise. However, there are a series of substantial challenges in how best to use these data. First, as we have seen, it is sparse and patchy. Absence of evidence cannot be taken as evidence of absence. But, perhaps more importantly, it is unclear in many cases what these various proxies actually <em>mean</em>. Of course this is also true of more familiar indicators like citations.</p>
<p>Finally, there is a challenge in how to effectively analyze these data. The sparse nature of the data is a substantial problem in itself, but in addition there are a number of significantly confounding effects. The biggest of these is time. The process of moving research outputs and their use online is still proceeding, and the uptake and penetration of online services and social media by researchers and other relevant communities has increased rapidly over the past few years and will continue to do so for some time.</p>
<p>These changes are occurring on a timescale of months, or even weeks, so any analysis must take into account how those changes may contribute to any observed signal. Much attention has focused on how different quantitative proxies correlate with each other. In essence this has continued the mistake that has already been made with citations. Focusing on proxies themselves implicitly makes the assumption that it is the proxy that matters, rather than the underlying process that is actually of interest. Citations are irrelevant; what matters is the influence that a piece of research has had. Citations are merely a proxy for a particular slice of influence, a (limited) indicator of the underlying process in which a research output is used by other researchers.</p>
<p>Of course, these are common challenges for many “big data” situations. The challenge lies in using large, but disparate and messy, data sets to provide insight while avoiding the false positives that will arise from any attempt to mine data blindly for correlations. Using the appropriate models and tools and careful validation of findings against other sources of data are the way forward.</p>
<div id="sec:4-8.1" class="section level3">
<h3><span class="header-section-number">2.10.1</span> Network analysis approaches</h3>
<p>One approach is to use these data to dissect and analyze the (visible) network of relationships between agents and objects. This approach can be useful in defining how networks of collaborators change over time, who is in contact with whom, and how outputs are related to each other. This kind of analysis has been productive with citation graphs (see Eigenfactor for an example) as well as with small-scale analysis of grant programs (see, for instance, the Lattes analysis of the network grant program).</p>
<p>Network analysis techniques and visualization are covered in Chapter <a href="chap-networks.html#chap:networks">Networks: The Basics</a> (on networks) and clustering and categorization in Chapter <a href="chap-ml.html#chap:ml">Machine Learning</a> (on machine learning). Networks may be built up from any combination of outputs, actors/agents, and their relationships to each other. Analyses that may be particularly useful are those searching for highly connected (proxy for influential) actors or outputs, clustering to define categories that emerge from the data itself (as opposed to external categorization) and comparisons between networks, both between those built from specific nodes (people, outputs) and between networks that are built from data relating to different time frames.</p>
<p>Care is needed with such analyses to make sure that comparisons are valid. In particular, when doing analyses of different time frames, it is important to compare any change in the network characteristics that are due to general changes over time as opposed to specific changes. As noted above, this is particularly important with networks based on social media data, as any networks are likely to have increased in size and diversity over the past few years as more users interested in research have joined. It is important to distinguish in these cases between changes relating to a specific intervention or treatment and those that are environmental. As with any retrospective analysis, a good counterfactual sample is required.</p>
</div>
<div id="sec:4-8.3" class="section level3">
<h3><span class="header-section-number">2.10.2</span> Future prospects and new data sources</h3>
<p>As the broader process of research moves online we are likely to have more and more information on what is being created, by whom, and when. As access to these objects increases, both through provision of open access to published work and through increased data sharing, it will become more and more feasible to mine the objects themselves to enrich the metadata. And finally, as the use of unique identifiers increases for both outputs and people, we will be able to cross-reference across data sources much more strongly.</p>
<p>Much of the data currently being collected is of poor quality or is inconsistently processed. Major efforts are underway to develop standards and protocols for initial processing, particularly for page view and usage data. Alongside efforts such as the Crossref DOI Event Tracker Service to provide central clearing houses for data, both consistency and completeness will continue to rise, making new and more comprehensive forms of analysis feasible.</p>
<p>Perhaps the most interesting prospect is new data that arise as more of the outputs and processes of research move online. As the availability of data outputs, software products, and even potentially the raw record of lab notebooks increases, we will have opportunities to query how (and how much) different reagents, techniques, tools, and instruments are being used. As the process of policy development and government becomes more transparent and better connected, it will be possible to watch in real time as research has its impact on the public sphere. And as health data moves online there will be opportunities to see how both chemical and behavioral interventions affect health outcomes in real time.</p>
<p>In the end all of this data will also be grist to the mill for further research. For the first time we will have the opportunity to treat the research enterprise as a system that is subject to optimization and engineering. Once again the challenges of what it is we are seeking to optimize for are questions that the data itself cannot answer, but in turn the data can better help us to have the debate about what matters.</p>
</div>
</div>
<div id="sec:4-9" class="section level2">
<h2><span class="header-section-number">2.11</span> Summary</h2>
<p>The term <em>research impact</em> is difficult and politicized, and it is used differently in different areas. At its root it can be described as the change that a particular part of the research enterprise (e.g., research project, researcher, funding decision, or institute) makes in the world. In this sense, it maps well to standard approaches in the social sciences that seek to identify how an intervention has led to change.</p>
<p>The link between “impact” and the distribution of limited research resources makes its definition highly political. In fact, there are many forms of impact, and the different pathways to different kinds of change, further research, economic growth, improvement in health outcomes, greater engagement of citizenry, or environmental change may have little in common beyond our interest in how they can be traced to research outputs. Most public policy on research investment has avoided the difficult question of which impacts are most important.</p>
<p>In part this is due to the historical challenges of providing evidence for these impacts. We have only had good data on formal research outputs, primarily journal articles, and measures have focused on naïve metrics such as productivity or citations, oron qualitative peer review. Broader impacts have largely been evidenced through case studies, an expensive and nonscalableapproach.</p>
<p>The move of research processes online is providing much richer and more diverse information on how research outputs are used and disseminated. We have the prospect of collecting much more information around the performance and usage of traditional research outputs as well as greater data on the growing diversity of nontraditional research outputs that are now being shared.</p>
<p>It is possible to gain quantitative information on the numbers of people looking at research, different groups talking about research (in different places), those citing research in different places, and recommendations and opinions on the value of work. These data are sparse and incomplete and its use needs to acknowledge these limitations, but it is nonetheless possible to gain new and valuable insights from analysis.</p>
<p>Much of this data is available from web services in the form of application programming interfaces. Well-designed APIs make it easy to search for, gather, and integrate data from multiple sources. A key aspect of successfully integrating data is the effective use and application of unique identifiers across data sets that allow straightforward cross-referencing. Key among the identifiers currently being used are ORCIDs to uniquely identify researchers and DOIs, from both Crossref and increasingly DataCite, to identify research outputs. With good cross-referencing it is possible to obtain rich data sets that can be used as inputs to many of the techniques described elsewhere in the book.</p>
<p>The analysis of this new data is a nascent field and the quality of work done so far has been limited. In my view there is a substantial opportunity to use these rich and diverse data sets to treat the underlying question of how research outputs flow from the academy to their sites of use. What are the underlying processes that lead to various impacts? This means treating these data sets as time domain signals that can be used to map and identify the underlying processes. This approach is appealing because it offers the promise of probing the actual process of knowledge diffusion while making fewer assumptions about what we think is happening.</p>
</div>
<div id="resources" class="section level2">
<h2><span class="header-section-number">2.12</span> Resources</h2>
<p>We talked a great deal here about how to access publications and other resources via their DOIs. Paskin <span class="citation">[@paskin2008digital]</span> provides a nice summary of the problems that DOIs solve and how they work.</p>
<p>ORCIDs are another key piece of this puzzle, as we have seen throughout this chapter. You might find some of the early articles describing the need for unique author IDs useful, such as Bourne et al. <span class="citation">[@bourne2008authorids]</span>, as well as more recent descriptions <span class="citation">[@haak2012orcid]</span>. More recent initiatives on expanding the scope of identifiers to materials and software have also been developed <span class="citation">[@bandrowski2015resource]</span>.</p>
<p>More general discussions of the challenges and opportunities of using metrics in research assessment may be found in recent reports such as the HEFCE Expert Group Report <span class="citation">[@wilsdon2015]</span>, and I have covered some of the broader issues elsewhere <span class="citation">[@neylon2015]</span>.</p>
<p>There are many good introductions to web scraping using BeautifulSoup and other libraries as well as API usage in general. Given the pace at which APIs and Python libraries change, the best and most up to date source of information is likely to be a web search.</p>
<p>In other settings, you may be concerned with assigning DOIs to data that you generate yourself, so that you and others can easily and reliably refer to and access that data in their own work. Here we face an embarrassment of riches, with many systems available that each meet different needs. Big data research communities such as climate science <span class="citation">[@williams2009earth]</span>, high-energy physics <span class="citation">[@pordes2007open]</span>, and astronomy <span class="citation">[@szalay2002sdss]</span> operate their own specialized infrastructures that you are unlikely to require. For small data sets, Figshare <span class="citation">[@figshare]</span> and DataCite <span class="citation">[@datacite]</span> are often used. The Globus publication service <span class="citation">[@chard15publication]</span> permits an institution or community to build their own publicationsystem.</p>
</div>
<div id="acknowledgements-and-copyright" class="section level2">
<h2><span class="header-section-number">2.13</span> Acknowledgements and copyright</h2>
<p>Section <a href="chap-web.html#sec:4-2" reference-type="ref" reference="sec:4-2">2.3</a> is adapted in part from Neylon et al. <span class="citation">[@neylon2014scap]</span>, copyright International Development Research Center, Canada, used here under a Creative Commons Attribution v 4.0 License.</p>
<p>Section <a href="chap-web.html#sec:4-7.1.4" reference-type="ref" reference="sec:4-7.1.4">2.9.4</a> is adapted in part from Neylon <span class="citation">[@neylon2014plosaltmetrics]</span>, copyright PLOS, used here under a Creative Commons Attribution v 4.0License.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="10">
<li id="fn10"><p>Python features many useful libraries; BeautifulSoup is particularly helpful for webscraping.<a href="chap-web.html#fnref10">↩</a></p></li>
<li id="fn11"><p>See Section 3.2.<a href="chap-web.html#fnref11">↩</a></p></li>
<li id="fn12"><p>JSON is an open standard way of storing and exchanging data.<a href="chap-web.html#fnref12">↩</a></p></li>
<li id="fn13"><p>See Chapter 10.<a href="chap-web.html#fnref13">↩</a></p></li>
<li id="fn14"><p>See Chapter 7 and 8.<a href="chap-web.html#fnref14">↩</a></p></li>
<li id="fn15"><p>See Chapter 8.<a href="chap-web.html#fnref15">↩</a></p></li>
</ol>
</div>
<div id="disqus_thread"></div>
<script>
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = '//big-data-and-social-science.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the
<a href="https://disqus.com/?ref_noscript">
  comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<a href="chap-intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chap-link.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/Coleridge-Initiative/big-data-and-social-science/edit/master/02-ChapterWeb.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
