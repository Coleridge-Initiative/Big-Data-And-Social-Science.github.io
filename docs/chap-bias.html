<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 11 Bias and Fairness | Big Data and Social Science</title>
  <meta name="description" content="Chapter 11 Bias and Fairness | Big Data and Social Science" />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 11 Bias and Fairness | Big Data and Social Science" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="Coleridge-Initiative/big-data-and-social-science" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 11 Bias and Fairness | Big Data and Social Science" />
  
  
  

<meta name="author" content="Ian Foster, Rayid Ghani, Ron S. Jarmin, Frauke Kreuter and Julia Lane" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chap-errors.html"/>
<link rel="next" href="chap-privacy.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157005492-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157005492-1');
</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Big Data and Social Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface to the 2nd edition</a></li>
<li class="chapter" data-level="1" data-path="chap-intro.html"><a href="chap-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-1"><i class="fa fa-check"></i><b>1.1</b> Why this book?</a></li>
<li class="chapter" data-level="1.2" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-2"><i class="fa fa-check"></i><b>1.2</b> Defining big data and its value</a></li>
<li class="chapter" data-level="1.3" data-path="chap-intro.html"><a href="chap-intro.html#sec:1.3"><i class="fa fa-check"></i><b>1.3</b> Social science, inference, and big data</a></li>
<li class="chapter" data-level="1.4" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-5"><i class="fa fa-check"></i><b>1.4</b> Social science, data quality, and big data</a></li>
<li class="chapter" data-level="1.5" data-path="chap-intro.html"><a href="chap-intro.html#new-tools-for-new-data"><i class="fa fa-check"></i><b>1.5</b> New tools for new data</a></li>
<li class="chapter" data-level="1.6" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-6"><i class="fa fa-check"></i><b>1.6</b> The book’s “use case”</a></li>
<li class="chapter" data-level="1.7" data-path="chap-intro.html"><a href="chap-intro.html#the-structure-of-the-book"><i class="fa fa-check"></i><b>1.7</b> The structure of the book</a><ul>
<li class="chapter" data-level="1.7.1" data-path="chap-intro.html"><a href="chap-intro.html#part-i-capture-and-curation"><i class="fa fa-check"></i><b>1.7.1</b> Part I: Capture and curation</a></li>
<li class="chapter" data-level="1.7.2" data-path="chap-intro.html"><a href="chap-intro.html#part-ii-modeling-and-analysis"><i class="fa fa-check"></i><b>1.7.2</b> Part II: Modeling and analysis</a></li>
<li class="chapter" data-level="1.7.3" data-path="chap-intro.html"><a href="chap-intro.html#part-iii-inference-and-ethics"><i class="fa fa-check"></i><b>1.7.3</b> Part III: Inference and ethics</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="chap-intro.html"><a href="chap-intro.html#sec:intro:resources"><i class="fa fa-check"></i><b>1.8</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chap-web.html"><a href="chap-web.html"><i class="fa fa-check"></i><b>2</b> Working with Web Data and APIs</a><ul>
<li class="chapter" data-level="2.1" data-path="chap-web.html"><a href="chap-web.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="chap-web.html"><a href="chap-web.html#sec:4-1"><i class="fa fa-check"></i><b>2.2</b> Scraping information from the web</a><ul>
<li class="chapter" data-level="2.2.1" data-path="chap-web.html"><a href="chap-web.html#sec:4-1.1"><i class="fa fa-check"></i><b>2.2.1</b> Obtaining data from websites</a></li>
<li class="chapter" data-level="2.2.2" data-path="chap-web.html"><a href="chap-web.html#sec:4-1.2"><i class="fa fa-check"></i><b>2.2.2</b> Limits of scraping</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="chap-web.html"><a href="chap-web.html#sec:4-3"><i class="fa fa-check"></i><b>2.3</b> Application Programming Interfaces (APIs)</a><ul>
<li class="chapter" data-level="2.3.1" data-path="chap-web.html"><a href="chap-web.html#sec:4-3.1"><i class="fa fa-check"></i><b>2.3.1</b> Relevant APIs and resources</a></li>
<li class="chapter" data-level="2.3.2" data-path="chap-web.html"><a href="chap-web.html#sec:4-3.2"><i class="fa fa-check"></i><b>2.3.2</b> RESTful APIs, returned data, and Python wrappers</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="chap-web.html"><a href="chap-web.html#sec:4-4"><i class="fa fa-check"></i><b>2.4</b> Using an API</a></li>
<li class="chapter" data-level="2.5" data-path="chap-web.html"><a href="chap-web.html#sec:4-4.1"><i class="fa fa-check"></i><b>2.5</b> Another example: Using the ORCID API via a wrapper</a></li>
<li class="chapter" data-level="2.6" data-path="chap-web.html"><a href="chap-web.html#sec:4-6"><i class="fa fa-check"></i><b>2.6</b> Integrating data from multiple sources</a></li>
<li class="chapter" data-level="2.7" data-path="chap-web.html"><a href="chap-web.html#sec:4-9"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chap-link.html"><a href="chap-link.html"><i class="fa fa-check"></i><b>3</b> Record Linkage</a><ul>
<li class="chapter" data-level="3.1" data-path="chap-link.html"><a href="chap-link.html#motivation"><i class="fa fa-check"></i><b>3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2" data-path="chap-link.html"><a href="chap-link.html#sec:recordlinkage"><i class="fa fa-check"></i><b>3.2</b> Introduction to record linkage</a></li>
<li class="chapter" data-level="3.3" data-path="chap-link.html"><a href="chap-link.html#preprocessing-data-for-record-linkage"><i class="fa fa-check"></i><b>3.3</b> Preprocessing data for record linkage</a></li>
<li class="chapter" data-level="3.4" data-path="chap-link.html"><a href="chap-link.html#S:indexing"><i class="fa fa-check"></i><b>3.4</b> Indexing and blocking</a></li>
<li class="chapter" data-level="3.5" data-path="chap-link.html"><a href="chap-link.html#matching"><i class="fa fa-check"></i><b>3.5</b> Matching</a><ul>
<li class="chapter" data-level="3.5.1" data-path="chap-link.html"><a href="chap-link.html#rule-based-approaches"><i class="fa fa-check"></i><b>3.5.1</b> Rule-based approaches</a></li>
<li class="chapter" data-level="3.5.2" data-path="chap-link.html"><a href="chap-link.html#probabilistic-record-linkage"><i class="fa fa-check"></i><b>3.5.2</b> Probabilistic record linkage</a></li>
<li class="chapter" data-level="3.5.3" data-path="chap-link.html"><a href="chap-link.html#machine-learning-approaches-to-record-linkage"><i class="fa fa-check"></i><b>3.5.3</b> Machine learning approaches to record linkage</a></li>
<li class="chapter" data-level="3.5.4" data-path="chap-link.html"><a href="chap-link.html#disambiguating-networks"><i class="fa fa-check"></i><b>3.5.4</b> Disambiguating networks</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="chap-link.html"><a href="chap-link.html#classification"><i class="fa fa-check"></i><b>3.6</b> Classification</a><ul>
<li class="chapter" data-level="3.6.1" data-path="chap-link.html"><a href="chap-link.html#S:thresholds"><i class="fa fa-check"></i><b>3.6.1</b> Thresholds</a></li>
<li class="chapter" data-level="3.6.2" data-path="chap-link.html"><a href="chap-link.html#one-to-one-links"><i class="fa fa-check"></i><b>3.6.2</b> One-to-one links</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="chap-link.html"><a href="chap-link.html#record-linkage-and-data-protection"><i class="fa fa-check"></i><b>3.7</b> Record linkage and data protection</a></li>
<li class="chapter" data-level="3.8" data-path="chap-link.html"><a href="chap-link.html#summary"><i class="fa fa-check"></i><b>3.8</b> Summary</a></li>
<li class="chapter" data-level="3.9" data-path="chap-link.html"><a href="chap-link.html#resources"><i class="fa fa-check"></i><b>3.9</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chap-db.html"><a href="chap-db.html"><i class="fa fa-check"></i><b>4</b> Databases</a><ul>
<li class="chapter" data-level="4.1" data-path="chap-db.html"><a href="chap-db.html#sec:db:intro"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="chap-db.html"><a href="chap-db.html#sec:db:when"><i class="fa fa-check"></i><b>4.2</b> DBMS: When and why</a></li>
<li class="chapter" data-level="4.3" data-path="chap-db.html"><a href="chap-db.html#relational-dbmss"><i class="fa fa-check"></i><b>4.3</b> Relational DBMSs</a><ul>
<li class="chapter" data-level="4.3.1" data-path="chap-db.html"><a href="chap-db.html#structured-query-language-sql"><i class="fa fa-check"></i><b>4.3.1</b> Structured Query Language (SQL)</a></li>
<li class="chapter" data-level="4.3.2" data-path="chap-db.html"><a href="chap-db.html#sec:db:sql"><i class="fa fa-check"></i><b>4.3.2</b> Manipulating and querying data</a></li>
<li class="chapter" data-level="4.3.3" data-path="chap-db.html"><a href="chap-db.html#sec:db:schema"><i class="fa fa-check"></i><b>4.3.3</b> Schema design and definition</a></li>
<li class="chapter" data-level="4.3.4" data-path="chap-db.html"><a href="chap-db.html#loading-data"><i class="fa fa-check"></i><b>4.3.4</b> Loading data</a></li>
<li class="chapter" data-level="4.3.5" data-path="chap-db.html"><a href="chap-db.html#transactions-and-crash-recovery"><i class="fa fa-check"></i><b>4.3.5</b> Transactions and crash recovery</a></li>
<li class="chapter" data-level="4.3.6" data-path="chap-db.html"><a href="chap-db.html#sec:db:index"><i class="fa fa-check"></i><b>4.3.6</b> Database optimizations</a></li>
<li class="chapter" data-level="4.3.7" data-path="chap-db.html"><a href="chap-db.html#caveats-and-challenges"><i class="fa fa-check"></i><b>4.3.7</b> Caveats and challenges</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="chap-db.html"><a href="chap-db.html#linking-dbmss-and-other-tools"><i class="fa fa-check"></i><b>4.4</b> Linking DBMSs and other tools</a></li>
<li class="chapter" data-level="4.5" data-path="chap-db.html"><a href="chap-db.html#sec:db:nosql"><i class="fa fa-check"></i><b>4.5</b> NoSQL databases</a><ul>
<li class="chapter" data-level="4.5.1" data-path="chap-db.html"><a href="chap-db.html#challenges-of-scale-the-cap-theorem"><i class="fa fa-check"></i><b>4.5.1</b> Challenges of scale: The CAP theorem</a></li>
<li class="chapter" data-level="4.5.2" data-path="chap-db.html"><a href="chap-db.html#nosql-and-keyvalue-stores"><i class="fa fa-check"></i><b>4.5.2</b> NoSQL and key–value stores</a></li>
<li class="chapter" data-level="4.5.3" data-path="chap-db.html"><a href="chap-db.html#other-nosql-databases"><i class="fa fa-check"></i><b>4.5.3</b> Other NoSQL databases</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="chap-db.html"><a href="chap-db.html#sec:db:spatial"><i class="fa fa-check"></i><b>4.6</b> Spatial databases</a></li>
<li class="chapter" data-level="4.7" data-path="chap-db.html"><a href="chap-db.html#which-database-to-use"><i class="fa fa-check"></i><b>4.7</b> Which database to use?</a><ul>
<li class="chapter" data-level="4.7.1" data-path="chap-db.html"><a href="chap-db.html#relational-dbmss-1"><i class="fa fa-check"></i><b>4.7.1</b> Relational DBMSs</a></li>
<li class="chapter" data-level="4.7.2" data-path="chap-db.html"><a href="chap-db.html#nosql-dbmss"><i class="fa fa-check"></i><b>4.7.2</b> NoSQL DBMSs</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="chap-db.html"><a href="chap-db.html#summary-1"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
<li class="chapter" data-level="4.9" data-path="chap-db.html"><a href="chap-db.html#resources-1"><i class="fa fa-check"></i><b>4.9</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chap-parallel.html"><a href="chap-parallel.html"><i class="fa fa-check"></i><b>5</b> Scaling up through Parallel and Distributed Computing</a><ul>
<li class="chapter" data-level="5.1" data-path="chap-parallel.html"><a href="chap-parallel.html#introduction-1"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="chap-parallel.html"><a href="chap-parallel.html#sec:intro"><i class="fa fa-check"></i><b>5.2</b> MapReduce</a></li>
<li class="chapter" data-level="5.3" data-path="chap-parallel.html"><a href="chap-parallel.html#apache-hadoop-mapreduce"><i class="fa fa-check"></i><b>5.3</b> Apache Hadoop MapReduce</a><ul>
<li class="chapter" data-level="5.3.1" data-path="chap-parallel.html"><a href="chap-parallel.html#the-hadoop-distributed-file-system"><i class="fa fa-check"></i><b>5.3.1</b> The Hadoop Distributed File System</a></li>
<li class="chapter" data-level="5.3.2" data-path="chap-parallel.html"><a href="chap-parallel.html#hadoop-setup-bringing-compute-to-the-data"><i class="fa fa-check"></i><b>5.3.2</b> Hadoop Setup: Bringing compute to the data</a></li>
<li class="chapter" data-level="5.3.3" data-path="chap-parallel.html"><a href="chap-parallel.html#hardware-provisioning"><i class="fa fa-check"></i><b>5.3.3</b> Hardware provisioning</a></li>
<li class="chapter" data-level="5.3.4" data-path="chap-parallel.html"><a href="chap-parallel.html#programming-in-hadoop"><i class="fa fa-check"></i><b>5.3.4</b> Programming in Hadoop</a></li>
<li class="chapter" data-level="5.3.5" data-path="chap-parallel.html"><a href="chap-parallel.html#programming-language-support"><i class="fa fa-check"></i><b>5.3.5</b> Programming language support</a></li>
<li class="chapter" data-level="5.3.6" data-path="chap-parallel.html"><a href="chap-parallel.html#benefits-and-limitations-of-hadoop"><i class="fa fa-check"></i><b>5.3.6</b> Benefits and Limitations of Hadoop</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="chap-parallel.html"><a href="chap-parallel.html#other-mapreduce-implementations"><i class="fa fa-check"></i><b>5.4</b> Other MapReduce Implementations</a></li>
<li class="chapter" data-level="5.5" data-path="chap-parallel.html"><a href="chap-parallel.html#apache-spark"><i class="fa fa-check"></i><b>5.5</b> Apache Spark</a></li>
<li class="chapter" data-level="5.6" data-path="chap-parallel.html"><a href="chap-parallel.html#summary-2"><i class="fa fa-check"></i><b>5.6</b> Summary</a></li>
<li class="chapter" data-level="5.7" data-path="chap-parallel.html"><a href="chap-parallel.html#resources-2"><i class="fa fa-check"></i><b>5.7</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chap-viz.html"><a href="chap-viz.html"><i class="fa fa-check"></i><b>6</b> Information Visualization</a><ul>
<li class="chapter" data-level="6.1" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-1"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2"><i class="fa fa-check"></i><b>6.2</b> Developing effective visualizations</a></li>
<li class="chapter" data-level="6.3" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-3"><i class="fa fa-check"></i><b>6.3</b> A data-by-tasks taxonomy</a><ul>
<li class="chapter" data-level="6.3.1" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.1"><i class="fa fa-check"></i><b>6.3.1</b> Multivariate data</a></li>
<li class="chapter" data-level="6.3.2" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.2"><i class="fa fa-check"></i><b>6.3.2</b> Spatial data</a></li>
<li class="chapter" data-level="6.3.3" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.4"><i class="fa fa-check"></i><b>6.3.3</b> Temporal data</a></li>
<li class="chapter" data-level="6.3.4" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.5"><i class="fa fa-check"></i><b>6.3.4</b> Hierarchical data</a></li>
<li class="chapter" data-level="6.3.5" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.6"><i class="fa fa-check"></i><b>6.3.5</b> Network data</a></li>
<li class="chapter" data-level="6.3.6" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.7"><i class="fa fa-check"></i><b>6.3.6</b> Text data</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4"><i class="fa fa-check"></i><b>6.4</b> Challenges</a><ul>
<li class="chapter" data-level="6.4.1" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4.1"><i class="fa fa-check"></i><b>6.4.1</b> Scalability</a></li>
<li class="chapter" data-level="6.4.2" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4.2"><i class="fa fa-check"></i><b>6.4.2</b> Evaluation</a></li>
<li class="chapter" data-level="6.4.3" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4.3"><i class="fa fa-check"></i><b>6.4.3</b> Visual impairment</a></li>
<li class="chapter" data-level="6.4.4" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4.4"><i class="fa fa-check"></i><b>6.4.4</b> Visual literacy</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-5"><i class="fa fa-check"></i><b>6.5</b> Summary</a></li>
<li class="chapter" data-level="6.6" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-6"><i class="fa fa-check"></i><b>6.6</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chap-ml.html"><a href="chap-ml.html"><i class="fa fa-check"></i><b>7</b> Machine Learning</a><ul>
<li class="chapter" data-level="7.1" data-path="chap-ml.html"><a href="chap-ml.html#introduction-2"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="chap-ml.html"><a href="chap-ml.html#what-is-machine-learning"><i class="fa fa-check"></i><b>7.2</b> What is machine learning?</a></li>
<li class="chapter" data-level="7.3" data-path="chap-ml.html"><a href="chap-ml.html#types-of-analysis"><i class="fa fa-check"></i><b>7.3</b> Types of analysis</a></li>
<li class="chapter" data-level="7.4" data-path="chap-ml.html"><a href="chap-ml.html#the-machine-learning-process"><i class="fa fa-check"></i><b>7.4</b> The Machine Learning process</a></li>
<li class="chapter" data-level="7.5" data-path="chap-ml.html"><a href="chap-ml.html#problem-formulation-mapping-a-problem-to-machine-learning-methods"><i class="fa fa-check"></i><b>7.5</b> Problem formulation: Mapping a problem to machine learning methods</a><ul>
<li class="chapter" data-level="7.5.1" data-path="chap-ml.html"><a href="chap-ml.html#features"><i class="fa fa-check"></i><b>7.5.1</b> Features</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="chap-ml.html"><a href="chap-ml.html#methods"><i class="fa fa-check"></i><b>7.6</b> Methods</a><ul>
<li class="chapter" data-level="7.6.1" data-path="chap-ml.html"><a href="chap-ml.html#unsupervised-learning-methods"><i class="fa fa-check"></i><b>7.6.1</b> Unsupervised learning methods</a></li>
<li class="chapter" data-level="7.6.2" data-path="chap-ml.html"><a href="chap-ml.html#sec:MLchapter:super"><i class="fa fa-check"></i><b>7.6.2</b> Supervised learning</a></li>
<li class="chapter" data-level="7.6.3" data-path="chap-ml.html"><a href="chap-ml.html#binary-vs-multiclass-classification-problems"><i class="fa fa-check"></i><b>7.6.3</b> Binary vs Multiclass classification problems</a></li>
<li class="chapter" data-level="7.6.4" data-path="chap-ml.html"><a href="chap-ml.html#skewed-or-imbalanced-classification-problems"><i class="fa fa-check"></i><b>7.6.4</b> Skewed or imbalanced classification problems</a></li>
<li class="chapter" data-level="7.6.5" data-path="chap-ml.html"><a href="chap-ml.html#model-interpretability"><i class="fa fa-check"></i><b>7.6.5</b> Model interpretability</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="chap-ml.html"><a href="chap-ml.html#evaluation"><i class="fa fa-check"></i><b>7.7</b> Evaluation</a><ul>
<li class="chapter" data-level="7.7.1" data-path="chap-ml.html"><a href="chap-ml.html#methodology"><i class="fa fa-check"></i><b>7.7.1</b> Methodology</a></li>
<li class="chapter" data-level="7.7.2" data-path="chap-ml.html"><a href="chap-ml.html#metrics"><i class="fa fa-check"></i><b>7.7.2</b> Metrics</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="chap-ml.html"><a href="chap-ml.html#practical-tips"><i class="fa fa-check"></i><b>7.8</b> Practical tips</a><ul>
<li class="chapter" data-level="7.8.1" data-path="chap-ml.html"><a href="chap-ml.html#avoiding-leakage"><i class="fa fa-check"></i><b>7.8.1</b> Avoiding Leakage</a></li>
<li class="chapter" data-level="7.8.2" data-path="chap-ml.html"><a href="chap-ml.html#machine-learning-pipeline"><i class="fa fa-check"></i><b>7.8.2</b> Machine learning pipeline</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="chap-ml.html"><a href="chap-ml.html#how-can-social-scientists-benefit-from-machine-learning"><i class="fa fa-check"></i><b>7.9</b> How can social scientists benefit from machine learning?</a></li>
<li class="chapter" data-level="7.10" data-path="chap-ml.html"><a href="chap-ml.html#advanced-topics"><i class="fa fa-check"></i><b>7.10</b> Advanced topics</a></li>
<li class="chapter" data-level="7.11" data-path="chap-ml.html"><a href="chap-ml.html#summary-3"><i class="fa fa-check"></i><b>7.11</b> Summary</a></li>
<li class="chapter" data-level="7.12" data-path="chap-ml.html"><a href="chap-ml.html#ml:res"><i class="fa fa-check"></i><b>7.12</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chap-text.html"><a href="chap-text.html"><i class="fa fa-check"></i><b>8</b> Text Analysis</a><ul>
<li class="chapter" data-level="8.1" data-path="chap-text.html"><a href="chap-text.html#understanding-human-generated-text"><i class="fa fa-check"></i><b>8.1</b> Understanding human generated text</a></li>
<li class="chapter" data-level="8.2" data-path="chap-text.html"><a href="chap-text.html#how-is-text-data-different-than-structured-data"><i class="fa fa-check"></i><b>8.2</b> How is text data different than “structured” data?</a></li>
<li class="chapter" data-level="8.3" data-path="chap-text.html"><a href="chap-text.html#what-can-we-do-with-text-data"><i class="fa fa-check"></i><b>8.3</b> What can we do with text data?</a></li>
<li class="chapter" data-level="8.4" data-path="chap-text.html"><a href="chap-text.html#how-to-analyze-text"><i class="fa fa-check"></i><b>8.4</b> How to analyze text</a><ul>
<li class="chapter" data-level="8.4.1" data-path="chap-text.html"><a href="chap-text.html#initial-processing"><i class="fa fa-check"></i><b>8.4.1</b> Initial Processing</a></li>
<li class="chapter" data-level="8.4.2" data-path="chap-text.html"><a href="chap-text.html#linguistic-analysis"><i class="fa fa-check"></i><b>8.4.2</b> Linguistic Analysis</a></li>
<li class="chapter" data-level="8.4.3" data-path="chap-text.html"><a href="chap-text.html#turning-text-data-into-a-matrix-how-much-is-a-word-worth"><i class="fa fa-check"></i><b>8.4.3</b> Turning text data into a matrix: How much is a word worth?</a></li>
<li class="chapter" data-level="8.4.4" data-path="chap-text.html"><a href="chap-text.html#analysis"><i class="fa fa-check"></i><b>8.4.4</b> Analysis</a></li>
<li class="chapter" data-level="8.4.5" data-path="chap-text.html"><a href="chap-text.html#sec:lda"><i class="fa fa-check"></i><b>8.4.5</b> Topic modeling</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="chap-text.html"><a href="chap-text.html#word-embeddings-and-deep-learning"><i class="fa fa-check"></i><b>8.5</b> Word Embeddings and Deep Learning</a></li>
<li class="chapter" data-level="8.6" data-path="chap-text.html"><a href="chap-text.html#text-analysis-tools"><i class="fa fa-check"></i><b>8.6</b> Text analysis tools</a></li>
<li class="chapter" data-level="8.7" data-path="chap-text.html"><a href="chap-text.html#summary-4"><i class="fa fa-check"></i><b>8.7</b> Summary</a></li>
<li class="chapter" data-level="8.8" data-path="chap-text.html"><a href="chap-text.html#resources-3"><i class="fa fa-check"></i><b>8.8</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chap-networks.html"><a href="chap-networks.html"><i class="fa fa-check"></i><b>9</b> Networks: The Basics</a><ul>
<li class="chapter" data-level="9.1" data-path="chap-networks.html"><a href="chap-networks.html#introduction-3"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="chap-networks.html"><a href="chap-networks.html#what-are-networks"><i class="fa fa-check"></i><b>9.2</b> What are networks?</a></li>
<li class="chapter" data-level="9.3" data-path="chap-networks.html"><a href="chap-networks.html#structure-for-this-chapter"><i class="fa fa-check"></i><b>9.3</b> Structure for this chapter</a></li>
<li class="chapter" data-level="9.4" data-path="chap-networks.html"><a href="chap-networks.html#turning-data-into-a-network"><i class="fa fa-check"></i><b>9.4</b> Turning Data into a Network</a><ul>
<li class="chapter" data-level="9.4.1" data-path="chap-networks.html"><a href="chap-networks.html#types-of-networks"><i class="fa fa-check"></i><b>9.4.1</b> Types of Networks</a></li>
<li class="chapter" data-level="9.4.2" data-path="chap-networks.html"><a href="chap-networks.html#inducing-one-mode-networks-from-two-mode-data"><i class="fa fa-check"></i><b>9.4.2</b> Inducing one-mode networks from two-mode data</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="chap-networks.html"><a href="chap-networks.html#network-measures"><i class="fa fa-check"></i><b>9.5</b> Network measures</a><ul>
<li class="chapter" data-level="9.5.1" data-path="chap-networks.html"><a href="chap-networks.html#reachability"><i class="fa fa-check"></i><b>9.5.1</b> Reachability</a></li>
<li class="chapter" data-level="9.5.2" data-path="chap-networks.html"><a href="chap-networks.html#whole-network-measures"><i class="fa fa-check"></i><b>9.5.2</b> Whole-network measures</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="chap-networks.html"><a href="chap-networks.html#case-study-comparing-collaboration-networks"><i class="fa fa-check"></i><b>9.6</b> Case Study: Comparing collaboration networks</a></li>
<li class="chapter" data-level="9.7" data-path="chap-networks.html"><a href="chap-networks.html#summary-5"><i class="fa fa-check"></i><b>9.7</b> Summary</a></li>
<li class="chapter" data-level="9.8" data-path="chap-networks.html"><a href="chap-networks.html#resources-4"><i class="fa fa-check"></i><b>9.8</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chap-errors.html"><a href="chap-errors.html"><i class="fa fa-check"></i><b>10</b> Data Quality and Inference Errors</a><ul>
<li class="chapter" data-level="10.1" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-1"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-2"><i class="fa fa-check"></i><b>10.2</b> The total error paradigm</a><ul>
<li class="chapter" data-level="10.2.1" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-2.1"><i class="fa fa-check"></i><b>10.2.1</b> The traditional model</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-3"><i class="fa fa-check"></i><b>10.3</b> Example: Google Flu Trends</a></li>
<li class="chapter" data-level="10.4" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-4"><i class="fa fa-check"></i><b>10.4</b> Errors in data analysis</a><ul>
<li class="chapter" data-level="10.4.1" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-4.2"><i class="fa fa-check"></i><b>10.4.1</b> Analysis errors resulting from inaccurate data</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-5"><i class="fa fa-check"></i><b>10.5</b> Detecting and Compensating for Data Errors</a><ul>
<li class="chapter" data-level="10.5.1" data-path="chap-errors.html"><a href="chap-errors.html#tableplots"><i class="fa fa-check"></i><b>10.5.1</b> TablePlots</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-6"><i class="fa fa-check"></i><b>10.6</b> Summary</a></li>
<li class="chapter" data-level="10.7" data-path="chap-errors.html"><a href="chap-errors.html#resources-5"><i class="fa fa-check"></i><b>10.7</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chap-bias.html"><a href="chap-bias.html"><i class="fa fa-check"></i><b>11</b> Bias and Fairness</a><ul>
<li class="chapter" data-level="11.1" data-path="chap-bias.html"><a href="chap-bias.html#introduction-4"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="chap-bias.html"><a href="chap-bias.html#sec:biassources"><i class="fa fa-check"></i><b>11.2</b> Sources of Bias</a><ul>
<li class="chapter" data-level="11.2.1" data-path="chap-bias.html"><a href="chap-bias.html#sample-bias"><i class="fa fa-check"></i><b>11.2.1</b> Sample Bias</a></li>
<li class="chapter" data-level="11.2.2" data-path="chap-bias.html"><a href="chap-bias.html#labeloutcome-bias"><i class="fa fa-check"></i><b>11.2.2</b> Label(Outcome) Bias</a></li>
<li class="chapter" data-level="11.2.3" data-path="chap-bias.html"><a href="chap-bias.html#sec:mlbiasexamples"><i class="fa fa-check"></i><b>11.2.3</b> Machine Learning Pipeline Bias</a></li>
<li class="chapter" data-level="11.2.4" data-path="chap-bias.html"><a href="chap-bias.html#application-bias"><i class="fa fa-check"></i><b>11.2.4</b> Application Bias</a></li>
<li class="chapter" data-level="11.2.5" data-path="chap-bias.html"><a href="chap-bias.html#considering-bias-when-deploying-your-model"><i class="fa fa-check"></i><b>11.2.5</b> Considering Bias When Deploying Your Model</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="chap-bias.html"><a href="chap-bias.html#dealing-with-bias"><i class="fa fa-check"></i><b>11.3</b> Dealing with Bias</a><ul>
<li class="chapter" data-level="11.3.1" data-path="chap-bias.html"><a href="chap-bias.html#sec:metrics"><i class="fa fa-check"></i><b>11.3.1</b> Define Bias</a></li>
<li class="chapter" data-level="11.3.2" data-path="chap-bias.html"><a href="chap-bias.html#definitions"><i class="fa fa-check"></i><b>11.3.2</b> Definitions</a></li>
<li class="chapter" data-level="11.3.3" data-path="chap-bias.html"><a href="chap-bias.html#choosing-bias-metrics"><i class="fa fa-check"></i><b>11.3.3</b> Choosing Bias Metrics</a></li>
<li class="chapter" data-level="11.3.4" data-path="chap-bias.html"><a href="chap-bias.html#sec:punitiveexample"><i class="fa fa-check"></i><b>11.3.4</b> Punitive Example</a></li>
<li class="chapter" data-level="11.3.5" data-path="chap-bias.html"><a href="chap-bias.html#sec:assistiveexample"><i class="fa fa-check"></i><b>11.3.5</b> Assistive Example</a></li>
<li class="chapter" data-level="11.3.6" data-path="chap-bias.html"><a href="chap-bias.html#sec:constrainedassistive"><i class="fa fa-check"></i><b>11.3.6</b> Special Case: Resource-Constrained Programs</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="chap-bias.html"><a href="chap-bias.html#sec:applications"><i class="fa fa-check"></i><b>11.4</b> Mitigating Bias</a><ul>
<li class="chapter" data-level="11.4.1" data-path="chap-bias.html"><a href="chap-bias.html#auditing-model-results"><i class="fa fa-check"></i><b>11.4.1</b> Auditing Model Results</a></li>
<li class="chapter" data-level="11.4.2" data-path="chap-bias.html"><a href="chap-bias.html#model-selection"><i class="fa fa-check"></i><b>11.4.2</b> Model Selection</a></li>
<li class="chapter" data-level="11.4.3" data-path="chap-bias.html"><a href="chap-bias.html#other-options-for-mitigating-bias"><i class="fa fa-check"></i><b>11.4.3</b> Other Options for Mitigating Bias</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="chap-bias.html"><a href="chap-bias.html#further-considerations"><i class="fa fa-check"></i><b>11.5</b> Further Considerations</a><ul>
<li class="chapter" data-level="11.5.1" data-path="chap-bias.html"><a href="chap-bias.html#compared-to-what"><i class="fa fa-check"></i><b>11.5.1</b> Compared to What?</a></li>
<li class="chapter" data-level="11.5.2" data-path="chap-bias.html"><a href="chap-bias.html#costs-to-both-errors"><i class="fa fa-check"></i><b>11.5.2</b> Costs to Both Errors</a></li>
<li class="chapter" data-level="11.5.3" data-path="chap-bias.html"><a href="chap-bias.html#what-is-the-relevant-population"><i class="fa fa-check"></i><b>11.5.3</b> What is the Relevant Population?</a></li>
<li class="chapter" data-level="11.5.4" data-path="chap-bias.html"><a href="chap-bias.html#continuous-outcomes"><i class="fa fa-check"></i><b>11.5.4</b> Continuous Outcomes</a></li>
<li class="chapter" data-level="11.5.5" data-path="chap-bias.html"><a href="chap-bias.html#considerations-for-ongoing-measurement"><i class="fa fa-check"></i><b>11.5.5</b> Considerations for Ongoing Measurement</a></li>
<li class="chapter" data-level="11.5.6" data-path="chap-bias.html"><a href="chap-bias.html#equity-in-practice"><i class="fa fa-check"></i><b>11.5.6</b> Equity in Practice</a></li>
<li class="chapter" data-level="11.5.7" data-path="chap-bias.html"><a href="chap-bias.html#other-names-you-might-see"><i class="fa fa-check"></i><b>11.5.7</b> Other Names You Might See</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="chap-bias.html"><a href="chap-bias.html#case-studies"><i class="fa fa-check"></i><b>11.6</b> Case Studies</a><ul>
<li class="chapter" data-level="11.6.1" data-path="chap-bias.html"><a href="chap-bias.html#sec:compascase"><i class="fa fa-check"></i><b>11.6.1</b> Recidivism Predictions with COMPAS</a></li>
<li class="chapter" data-level="11.6.2" data-path="chap-bias.html"><a href="chap-bias.html#facial-recognition"><i class="fa fa-check"></i><b>11.6.2</b> Facial Recognition</a></li>
<li class="chapter" data-level="11.6.3" data-path="chap-bias.html"><a href="chap-bias.html#facebook-advertisement-targeting"><i class="fa fa-check"></i><b>11.6.3</b> Facebook Advertisement Targeting</a></li>
<li class="chapter" data-level="11.6.4" data-path="chap-bias.html"><a href="chap-bias.html#tay-microsofts-ai-twitter-bot"><i class="fa fa-check"></i><b>11.6.4</b> Tay: Microsoft’s AI Twitter Bot</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="chap-bias.html"><a href="chap-bias.html#aequitas---a-toolkit-for-auditing-bias-and-fairness-in-machine-learning-models"><i class="fa fa-check"></i><b>11.7</b> Aequitas - A Toolkit for Auditing Bias and Fairness in Machine Learning Models</a><ul>
<li class="chapter" data-level="11.7.1" data-path="chap-bias.html"><a href="chap-bias.html#getting-started-with-aequitas"><i class="fa fa-check"></i><b>11.7.1</b> Getting Started with Aequitas</a></li>
<li class="chapter" data-level="11.7.2" data-path="chap-bias.html"><a href="chap-bias.html#requirements"><i class="fa fa-check"></i><b>11.7.2</b> Requirements</a></li>
<li class="chapter" data-level="11.7.3" data-path="chap-bias.html"><a href="chap-bias.html#data-preparation"><i class="fa fa-check"></i><b>11.7.3</b> Data Preparation</a></li>
<li class="chapter" data-level="11.7.4" data-path="chap-bias.html"><a href="chap-bias.html#working-with-bias-metrics"><i class="fa fa-check"></i><b>11.7.4</b> Working with Bias Metrics</a></li>
<li class="chapter" data-level="11.7.5" data-path="chap-bias.html"><a href="chap-bias.html#measuring-disparities"><i class="fa fa-check"></i><b>11.7.5</b> Measuring Disparities</a></li>
<li class="chapter" data-level="11.7.6" data-path="chap-bias.html"><a href="chap-bias.html#assessing-model-fairness"><i class="fa fa-check"></i><b>11.7.6</b> Assessing Model Fairness</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chap-privacy.html"><a href="chap-privacy.html"><i class="fa fa-check"></i><b>12</b> Privacy and Confidentiality</a><ul>
<li class="chapter" data-level="12.1" data-path="chap-privacy.html"><a href="chap-privacy.html#introduction-5"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="chap-privacy.html"><a href="chap-privacy.html#why-is-access-important"><i class="fa fa-check"></i><b>12.2</b> Why is access important?</a></li>
<li class="chapter" data-level="12.3" data-path="chap-privacy.html"><a href="chap-privacy.html#providing-access"><i class="fa fa-check"></i><b>12.3</b> Providing access</a></li>
<li class="chapter" data-level="12.4" data-path="chap-privacy.html"><a href="chap-privacy.html#non-tabular-data"><i class="fa fa-check"></i><b>12.4</b> Non-Tabular data</a></li>
<li class="chapter" data-level="12.5" data-path="chap-privacy.html"><a href="chap-privacy.html#the-new-challenges"><i class="fa fa-check"></i><b>12.5</b> The new challenges</a></li>
<li class="chapter" data-level="12.6" data-path="chap-privacy.html"><a href="chap-privacy.html#legal-and-ethical-framework"><i class="fa fa-check"></i><b>12.6</b> Legal and ethical framework</a></li>
<li class="chapter" data-level="12.7" data-path="chap-privacy.html"><a href="chap-privacy.html#summary-6"><i class="fa fa-check"></i><b>12.7</b> Summary</a></li>
<li class="chapter" data-level="12.8" data-path="chap-privacy.html"><a href="chap-privacy.html#resources-6"><i class="fa fa-check"></i><b>12.8</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="chap-workbooks.html"><a href="chap-workbooks.html"><i class="fa fa-check"></i><b>13</b> Workbooks</a><ul>
<li class="chapter" data-level="13.1" data-path="chap-workbooks.html"><a href="chap-workbooks.html#introduction-6"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="chap-workbooks.html"><a href="chap-workbooks.html#notebooks"><i class="fa fa-check"></i><b>13.2</b> Notebooks</a><ul>
<li class="chapter" data-level="13.2.1" data-path="chap-workbooks.html"><a href="chap-workbooks.html#databases"><i class="fa fa-check"></i><b>13.2.1</b> Databases</a></li>
<li class="chapter" data-level="13.2.2" data-path="chap-workbooks.html"><a href="chap-workbooks.html#dataset-exploration-and-visualization"><i class="fa fa-check"></i><b>13.2.2</b> Dataset Exploration and Visualization</a></li>
<li class="chapter" data-level="13.2.3" data-path="chap-workbooks.html"><a href="chap-workbooks.html#apis"><i class="fa fa-check"></i><b>13.2.3</b> APIs</a></li>
<li class="chapter" data-level="13.2.4" data-path="chap-workbooks.html"><a href="chap-workbooks.html#record-linkage"><i class="fa fa-check"></i><b>13.2.4</b> Record Linkage</a></li>
<li class="chapter" data-level="13.2.5" data-path="chap-workbooks.html"><a href="chap-workbooks.html#text-analysis"><i class="fa fa-check"></i><b>13.2.5</b> Text Analysis</a></li>
<li class="chapter" data-level="13.2.6" data-path="chap-workbooks.html"><a href="chap-workbooks.html#networks"><i class="fa fa-check"></i><b>13.2.6</b> Networks</a></li>
<li class="chapter" data-level="13.2.7" data-path="chap-workbooks.html"><a href="chap-workbooks.html#machine-learning-creating-labels"><i class="fa fa-check"></i><b>13.2.7</b> Machine Learning – Creating Labels</a></li>
<li class="chapter" data-level="13.2.8" data-path="chap-workbooks.html"><a href="chap-workbooks.html#machine-learning-creating-features"><i class="fa fa-check"></i><b>13.2.8</b> Machine Learning – Creating Features</a></li>
<li class="chapter" data-level="13.2.9" data-path="chap-workbooks.html"><a href="chap-workbooks.html#machine-learning-model-training-and-evaluation"><i class="fa fa-check"></i><b>13.2.9</b> Machine Learning – Model Training and Evaluation</a></li>
<li class="chapter" data-level="13.2.10" data-path="chap-workbooks.html"><a href="chap-workbooks.html#bias-and-fairness"><i class="fa fa-check"></i><b>13.2.10</b> Bias and Fairness</a></li>
<li class="chapter" data-level="13.2.11" data-path="chap-workbooks.html"><a href="chap-workbooks.html#errors-and-inference"><i class="fa fa-check"></i><b>13.2.11</b> Errors and Inference</a></li>
<li class="chapter" data-level="13.2.12" data-path="chap-workbooks.html"><a href="chap-workbooks.html#additional-workbooks"><i class="fa fa-check"></i><b>13.2.12</b> Additional Workbooks</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="chap-workbooks.html"><a href="chap-workbooks.html#resources-7"><i class="fa fa-check"></i><b>13.3</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Big Data and Social Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chap:bias" class="section level1">
<h1><span class="header-section-number">Chapter 11</span> Bias and Fairness</h1>
<p><strong>Kit T. Rodolfa, Pedro Saleiro, and Rayid Ghani</strong></p>
<p>Interest in algorithmic fairness and bias has been growing recently (for good reason), but it’s easy to get lost in the large number of definitions and metrics. There are many different, often competing, ways to measure whether a given model is statistically “fair” but it’s important to remember to start from the social and policy goals for equity and fairness and map those to the statistical properties we want in our models to help achieve those goals. In this chapter, we provide an overview of these statistical metrics along with some concrete examples to help navigate these concepts and understand the trade-offs involved in choosing to optimize to one metric over others, focusing on the metrics relevant to binary classification methods used frequently in risk-based models for policy settings.</p>
<div id="introduction-4" class="section level2">
<h2><span class="header-section-number">11.1</span> Introduction</h2>
<p>In Chapter <a href="chap-ml.html#chap:ml">Machine Learning</a>, you learned about several of the concepts, tools, and approaches used in the field of machine learning and how they can be used in the social sciences. In chapter <a href="chap-ml.html#chap:ml">Machine Learning</a>, we focused on evaluation metrics such as precision (positive predictive value), recall (sensitivity), area-under-curve (AUC), and accuracy, that are often used to measure the performance of machine learning methods. In most (if not every) public policy problems, a key goal for the analytical systems being developed is to help achieve equitable outcomes.</p>
<p>When machine learning models are being used to make decisions, they cannot be separated from the social and ethical context in which they are applied, and those developing and deploying these models must take care to do so in a manner that accounts for both accuracy and fairness. In this chapter, we will discuss sources of potential bias in your modeling pipeline, as well as some of the ways that bias introduced by a model can be measured, with a particular focus on classification problems. Unfortunately, just as there is no single machine learning algorithm that is best suited to every application, no one fairness metric will fit every situation. However, we hope this chapter will provide you with a grounding in the available ways of measuring algorithmic fairness that will help you navigate the trade-offs involved putting these into practice in your own applications.</p>
</div>
<div id="sec:biassources" class="section level2">
<h2><span class="header-section-number">11.2</span> Sources of Bias</h2>
<p>Bias may be introduced into a machine learning project at any step along the way and it is important to carefully think through each potential source and how it may affect your results. In many cases, some sources may be difficult to measure precisely (or even at all), but this doesn’t mean these potential biases can be readily ignored when developing interventions or performing analyses.</p>
<div id="sample-bias" class="section level3">
<h3><span class="header-section-number">11.2.1</span> Sample Bias</h3>
<p>You’re likely familiar with sampling issues as a potential source of bias in the contexts of causal inference and external validity in the social science literature. A biased sample can be just as problematic for machine learning as it can be for inference, and predictions made on individuals or groups not represented in the training set are likely to be unreliable. As such, any application of machine learning should start with a careful understanding of data generating process for the training and test sets. What is the relevant population for the project and how might some individuals be incorrectly excluded or included from the data available for modeling or analysis?</p>
<p>If there is a mismatch between the available training data and the population to whom the model will be applied, you may want to consider whether it is possible to collect more representative data. A model to evaluate the risk of health violations at restaurants may be of limited applicability if the only training data available is based on inspections that resulted from reported complaints. In such a case, an initial trial of randomized inspections might provide a more representative dataset. However, this may not always be possible. For instance, in the case of bail determinations, labeled data will only be available for individuals who are released under the existing system.</p>
<p>How does the available training data relate to the population that the model will be applied to? If there is a mismatch here, is it possible to collect more appropriate data? Example of bail determination – only have subsequent outcome data for individuals who were released in the past</p>
<p>Even if the training data matches the population, are their underlying systemic biases involved in defining that population in general? For instance, over-policing of black neighborhoods</p>
<p>For data with a time component or models that will be deployed to aid future decisions, are there relevant policy changes in the past that may make data from certain periods of time less relevant? Pending policy changes going forward that may affect the modeling population?</p>
<p>Measurement here might be difficult, but helpful to think through each of these questions in detail. Often, other sources of data (even in aggregate form) can provide some insight on how representative your data may be, including census data, surveys, and academic studies in the relevant area.</p>
</div>
<div id="labeloutcome-bias" class="section level3">
<h3><span class="header-section-number">11.2.2</span> Label(Outcome) Bias</h3>
<p>Regardless of whether your dataset reflects a representative sample of the relevant population for your intervention or analysis, there may also be bias inherent in the labels (that is, the measured outcomes) associated with individuals in that data.</p>
<p>One mechanism by which bias may be introduced is in how the label/outcome itself is defined. For instance, a study of recidivism might use a new arrest as an outcome variable when it really cares about committing a new crime. However, if some groups are policed more heavily than others, using arrests to define the outcome variable may introduce bias into the system’s decisions. Similarly, a label that relies on the number of days an individual has been incarcerated would reflect known biases in sentence lengths between black and white defendants.</p>
<p>A related mechanism is measurement error. Even when the outcome of interest is well-defined and can be measured directly, bias may be introduced through differential measurement accuracy across groups. For instance, data collected through survey research might suffer from language barriers or cultural differences in social desirability that introduce measurement errors across groups.</p>
</div>
<div id="sec:mlbiasexamples" class="section level3">
<h3><span class="header-section-number">11.2.3</span> Machine Learning Pipeline Bias</h3>
<p>Biases can be introduced by the handling and transformation of data throughout the machine learning pipeline as well, requiring careful consideration as you ingest data, create features, and model outcomes of interest. Below are a few examples at each stage of the process, but these are far from exhaustive and intended to help motivate thinking about how bias might be introduced in your own projects.</p>
<p>**Ingesting <a href="Data:**" class="uri">Data:**</a> The process of loading, cleaning, and reconciling data from a variety of data sources (often referred to as ETL) can introduce a number of errors that might have differential downstream impacts on different populations:</p>
<ul>
<li><p>Are your processes for matching individuals across data sources equally accurate across different populations? For instance, married vs maiden names may bias match rates against women, while inconsistencies in handling of multi-part last names may make matching less reliable for hispanic individuals.</p></li>
<li><p>Nickname dictionaries used in record reconciliation might be derived from different populations than your population of interest.</p></li>
<li><p>A data loading process that drops records with “special characters” might inadvertently exclude names with accents or tildes.</p></li>
</ul>
<p><strong>Feature Engineering:</strong> Biases are easy to introduce during the process of constructing features, both in the handling of features that relate directly to protected classes as well as information that correlates with these populations (such as geolocation). A few examples include:</p>
<ul>
<li><p>Dictionaries to infer age or gender from name might be derived from a population that is not relevant to your problem.</p></li>
<li><p>Handling of missing values and combining “other” categories can become problematic, especially for multi-racial individuals or people with non-binary gender.</p></li>
<li><p>Thought should be given to how race and ethnicity indicators are collected – are these self-reported, recorded by a third party, or inferred from other data? The data collection process may inform the accuracy of the data and how errors differ across populations.</p></li>
<li><p>Features that rely on geocoding to incorporate information based on distances or geographic aggregates may miss homeless individuals or provide less predictive power for more mobile populations.</p></li>
</ul>
<p><strong>Modeling:</strong> The model itself may introduce bias into decisions made from its scores by performing worse on some groups relative to others (many examples have been highlighted in popular press recently, such as racial biases in facial recognition algorithms and gender biases in targeting algorithms for job advertisement on social media). Because of the complex correlation structure of the data, it generally isn’t sufficient to simply leave out the protected attributes and assume this will result in fair outcomes. Rather model performance across groups needs to be measured directly in order to understand and address any biases. However, there are many (often incompatible) ways to define fairness and Section <a href="chap-bias.html#sec:metrics">metrics</a> will take a closer look at these options in much more detail.</p>
<p>Much of the remainder of this chapter focuses on how we might define and measure fairness at the level of the machine learning pipeline itself. In Section <a href="chap-bias.html#sec:metrics">metrics</a>, we will introduce several of the metrics used to measure algorithmic fairness and in Section <a href="chap-bias.html#sec:applications">applications</a> we discuss how these can be used in the process of evaluating and selecting machine learning models.</p>
</div>
<div id="application-bias" class="section level3">
<h3><span class="header-section-number">11.2.4</span> Application Bias</h3>
<p>A final potential source of bias worth considering is how the model or analysis might be put into use in practice. One way this might happen is through heterogeneity in the effectiveness of an intervention across groups. For instance, imagine a machine learning model to identify individuals most at risk for developing diabetes in the next 3 years for a particular preventive treatment. If the treatment is much more effective for individuals with a certain genetic background relative to others, the overall outcome of the effort might be to exacerbate disparities in diabetes rates even if the model itself is modeling risk in an unbiased way.</p>
<p>Likewise, it is important to be aware of the risk of discriminatory applications of a machine learning model. Perhaps a model developed to screen out unqualified job candidates is only “trusted” by a hiring manager for female candidates but often ignored or overridden for men. In a perverse way, applying an unbiased model in such a context might serve to increase inequities by giving bad actors more information with which to (wrongly) justify their discriminatory practices.</p>
<p>While there may be relatively little you can do to detect or mitigate these types of bias at the modeling stage, performing a trial to compare current practice with a deployed model can be instructive where doing so is feasible. Keep in mind, of course, that the potential for machine learning systems to be applied in biased ways shouldn’t be construed as an argument against developing these systems at all any more than it would be reasonable to suggest that current practices are likely to be free of bias. Rather, it is an argument for thinking carefully about both the status quo and how it may change in the presence of such a system, putting in place legal and technical safeguards to help ensure that these methods are applied in socially responsible ways.</p>
</div>
<div id="considering-bias-when-deploying-your-model" class="section level3">
<h3><span class="header-section-number">11.2.5</span> Considering Bias When Deploying Your Model</h3>
<p>Ultimately, what we care about is some global idea of how putting a model into practice will affect some overall concept of social welfare and fairness influenced by all of these possible sources of bias. While this is generally impossible to measure in a quantitative way, it can provide a valuable framework for qualitatively evaluating the potential impact of your model. For most of the remainder of this chapter, we consider a set of more quantitative metrics that can be applied to the predictions of a machine learning pipeline specifically, but it is important to keep in mind that these metrics only apply to the sample and labels you have and ignoring other sources of bias that may be at play in the underlying data generating process could result in unfair outcomes even when applying a model that appears to be “fair” by your chosen metric.</p>
</div>
</div>
<div id="dealing-with-bias" class="section level2">
<h2><span class="header-section-number">11.3</span> Dealing with Bias</h2>
<div id="sec:metrics" class="section level3">
<h3><span class="header-section-number">11.3.1</span> Define Bias</h3>
<p>Section <a href="chap-bias.html#sec:mlbiasexamples">bias examples</a> provided some examples for how bias might be introduced in the process of using machine learning to work with a dataset. While far from exhaustive as a source of potential bias in an overall application, these biases can be more readily measured and addressed through choices made during data preparation, modeling, and model selection. This section focuses on detecting and understanding biases introduced at this stage of the process.</p>
<p>One key challenge, however, is that there is no universally-accepted definition of what it means for a model to be fair. Take the example of a model being used to make bail determinations. Different people might consider it “fair” if:</p>
<ul>
<li><p>It makes mistakes about denying bail to an equal number of white and black individuals</p></li>
<li><p>The chances that a given black or white person will be wrongly denied bail is equal, regardless of race</p></li>
<li><p>Among the jailed population, the probability of having been wrongly denied bail is independent of race</p></li>
<li><p>For people who should be released, the chances that a given black or white person will be denied bail is equal</p></li>
</ul>
<p>In different contexts, reasonable arguments can be made for each of these potential definitions, but unfortunately, not all of them can hold at the same time. The remainder of this section explores these competing options and how to approach them in more detail.</p>
</div>
<div id="definitions" class="section level3">
<h3><span class="header-section-number">11.3.2</span> Definitions</h3>
<p>Most of the metrics used to assess model fairness relate either to the types of errors a model might make or how predictive the model is across different groups. For binary classification models (which we focus on here), these are generally derived from values in the <em>confusion matrix</em> (see Figure 6.9 and Section 6.6.2 for more details):</p>
<ul>
<li><p><strong>True Positives (<span class="math inline">\(TP\)</span>)</strong> are individuals for whom both the model prediction and actual outcome are positive labels.</p></li>
<li><p><strong>False Positives (<span class="math inline">\(FP\)</span>)</strong> are individuals for whom both the model predicts a positive label, but the actual outcome is a negative label.</p></li>
<li><p><strong>True Negatives (<span class="math inline">\(TN\)</span>)</strong> are individuals for whom both the model prediction and actual outcome are negative labels.</p></li>
<li><p><strong>False Negatives (<span class="math inline">\(FN\)</span>)</strong> are individuals for whom both the model predicts a negative label, but the actual outcome is a positive label.</p></li>
</ul>
<p>Based on these four categories, we can calculate several ratios that are instructive for thinking about the equity of a model’s predictions in different situations (Sections <a href="chap-bias.html#sec:punitiveexample">punitive example</a> and <a href="chap-bias.html#sec:assistiveexample">assistive example</a> provide some detailed examples here):</p>
<p>%can we move the following definitions or the equations to the side margin?</p>
<ul>
<li><p><strong>False Positive Rate (<span class="math inline">\(FPR\)</span>)</strong> is the fraction of individuals with negative actual labels who the model misclassifies with a positive predicted label: <span class="math inline">\(FPR = FP / (FP+TN)\)</span></p></li>
<li><p><strong>False Negative Rate (<span class="math inline">\(FNR\)</span>)</strong> is the fraction of individuals with positive actual labels who the model misclassifies with a negative predicted label: <span class="math inline">\(FNR = FN / (FN+TP)\)</span></p></li>
<li><p><strong>False Discovery Rate (<span class="math inline">\(FDR\)</span>)</strong> is the fraction of individuals who the model predicts to have a positive label but for whom the actual label is negative: <span class="math inline">\(FDR = FP / (FP+TP)\)</span></p></li>
<li><p><strong>False Omission Rate (<span class="math inline">\(FOR\)</span>)</strong> is the fraction of individuals who the model predicts to have a negative label but for whom the actual label is positive: <span class="math inline">\(FOR = FN / (FN+TN)\)</span></p></li>
<li><p><strong>Precision</strong> is the fraction of individuals who the model predicts to have a positive label about whom this prediction is correct: <span class="math inline">\(\textrm{precision} = TP / (FP+TP)\)</span></p></li>
<li><p><strong>Recall</strong> is the fraction of individuals with positive actual labels who the model has correctly classified as such: <span class="math inline">\(\textrm{recall} = TP / (FN+TP)\)</span></p></li>
</ul>
<p>For the first two metrics (<span class="math inline">\(FPR\)</span> and <span class="math inline">\(FNR\)</span>), notice that the denominator is based on actual outcomes (rather than model predictions), while in the next two (<span class="math inline">\(FDR\)</span> and <span class="math inline">\(FOR\)</span>) the denominator is based on model predictions (whether an individual falls above or below the threshold used to turn model scores into 0/1 predicted classes). The final two metrics relate to correct predictions rather than errors, but are directly related to error measurements (that is, <span class="math inline">\(\textrm{recall} = 1-FNR\)</span> and <span class="math inline">\(\textrm{precision} = 1-FDR\)</span>) and may sometimes have better properties for calculating model bias.</p>
<p>Notice that the metrics defined here require the use of a threshold to turn modeled scores into 0/1 predicted classes and are therefore most useful when either a threshold is well-defined for the problem (e.g., when available resources mean a program can only serve a given number of individuals) or where calculating these metrics at different threshold levels might be used (along with model performance metrics) to choose a threshold for application. In some cases, it may also be of interest to think about equity across the full distribution of the modeled score. Common practices in these situations are to look at how model performance metrics such as the area under the receiver-operator curve (<span class="math inline">\(AUC-ROC\)</span>) or model calibration compare across subgroups (such as by race, gender, age). Or, in cases where the underlying causal relationships are well-known, counterfactual methods may also be used to assess a model’s bias (these methods may also be useful when you suspect label bias to be an issue in your data). We don’t explore these topics deeply here, but refer you out to the relevant references if you would like to learn more ****.</p>
</div>
<div id="choosing-bias-metrics" class="section level3">
<h3><span class="header-section-number">11.3.3</span> Choosing Bias Metrics</h3>
<p>Any of the metrics defined above can be used to calculate disparities across groups in your data and (unless you have a perfect model) many of them cannot be balanced across subgroups at the same time. As a result, one of the most important — and frequently most challenging — aspects of measuring bias in your machine learning pipeline is simply understanding how “fairness” should be defined for your particular case.</p>
<p>In general, this requires consideration of the project’s goals and a detailed discussion between the data scientists, decision makers, and those who will be affected by the application of the model. Each perspective may have a different concept of fairness and a different understanding of harm involved in making different types of errors, both at individual and societal levels. Importantly, data scientists have an critical role in this conversation, both as the experts in understanding how different concepts of fairness might translate into metrics and measurement and as individuals with experience deploying similar models. While there is no universally correct definition of fairness, nor one that can be learned from the data, this doesn’t excuse the data scientists from responsibility for taking part in the conversation around fairness and equity in their models and helping decision makers understand the options and trade-offs involved.</p>
<p>Practically speaking, coming to an agreement on how fairness should be measured in a purely abstract manner is likely to be difficult. Often it can be instructive instead to explore different options and metrics based on preliminary results, providing tangible context for potential trade-offs between overall performance and different definitions of equity and helping guide stakeholders through the process of deciding what to optimize. The remainder of this section looks at some of the metrics that may be of particular interest in different types of applications:</p>
<ul>
<li><p>If your intervention is punitive in nature (e.g., determining whom to deny bail), individuals may be harmed by intervening on them in error so you may care more about metrics that focus on false positives. Section <a href="chap-bias.html#sec:punitiveexample">punitive example</a> provides an example to guide you through what some of these metrics mean in this case.</p></li>
<li><p>If your intervention is assistive in nature (e.g., determining who should receive a food subsidy), individuals may be harmed by failing to intervene on them when they have need, so you may care more about metrics that focus on false positives. Section <a href="chap-bias.html#sec:assistiveexample">assistive example</a> provides an example to guide you through metrics that may be applicable in this case.</p></li>
<li><p>If your resources are significantly constrained such that you can only intervene on a small fraction of the population at need, some of the metrics described here may be of limited use and Section <a href="chap-bias.html#sec:constrainedassistive">constrained assistive</a> describes this case in more detail.</p></li>
</ul>
</div>
<div id="sec:punitiveexample" class="section level3">
<h3><span class="header-section-number">11.3.4</span> Punitive Example</h3>
<p>When the application of a risk model is punitive in nature, individuals may be harmed by being incorrectly included in the “high risk” population that receives an intervention. In an extreme case, we can think of this as incorrectly detaining an innocent person in jail. Hence, with punitive interventions, we focus on bias and fairness metrics based on false positives.</p>
<p>We might naturally think about the number of people wrongly jailed from each group as reasonable place to start for assessing whether our model is biased. Here, we are concerned with statements like “twice as many people from Group A were wrongly convicted as from Group B.”</p>
<p>^(In probabilistic terms, we could express this as: <span class="math display">\[P(\textrm{wrongly jailed, group $i$}) = C~~~\forall~i\]</span> Where <span class="math inline">\(C\)</span> is a constant value. Or, alternatively, <span class="math display">\[\frac{FP_i}{FP_j} = 1~~~\forall~i,j\]</span> Where <span class="math inline">\(FP_i\)</span> is the number of false positives in group <span class="math inline">\(i\)</span>.)</p>
<p>However, it is unclear whether differences in the number of false positives across groups reflect unfairness in the model. For instance, if there are twice as many people in Group A as there are in Group B, some might deem the the situation described above as fair from the standpoint that the composition of the false positives reflects the composition of the groups. This brings us to our second metric:</p>
<p>By accounting for differently sized groups, we ask the question, “just by virtue of the fact that an individual is a member of a given group, what are the chances they’ll be wrongly convicted?”</p>
<p>^(in terms of probability, <span class="math display">\[P(\textrm{wrongly jailed $\mid$ group $i$}) = C~~~\forall~i\]</span> Where <span class="math inline">\(C\)</span> is a constant value. Or, alternatively, <span class="math display">\[\frac{FP_i}{FP_j} = \frac{n_i}{n_j}~~~\forall~i,j\]</span> Where <span class="math inline">\(FP_i\)</span> is the number of false positives and <span class="math inline">\(n_i\)</span> the total number of individuals in group <span class="math inline">\(i\)</span>.)</p>
<p>While this metric might feel like it meets a reasonable criteria of avoiding treating groups differently in terms of classification errors, there are other sources of disparities we might care about as well. For instance, suppose there are 10,000 individuals in Group A and 30,000 in Group B. Suppose further that 100 individuals from each group are jail, with 10 Group A people wrongly convicted and 30 Group B people wrongly convicted. We’ve balanced the number of false positives by group size (0.1% for both groups) so there are no disparities as far as this metric is concerned, but note that 10% of the jailed Group A individuals are innocent compared to 30% of the jailed Group B individuals. The next metric is concerned with unfairness in this way:</p>
<p>The False Discovery Rate (<span class="math inline">\(FDR\)</span>) focuses specifically on the people who are affected by the intervention — in the example above, among the 200 people in jail, what are the group-level disparities in rates of wrong convictions. The jail example is particularly instructive here as we could imagine the social cost of disparities manifesting directly through inmates observing how frequently different groups are wrongly convicted.</p>
<p>^(In probabilistic terms, <span class="math display">\[P(\textrm{wrongly jailed $\mid$ jailed, group $i$}) = C~~~\forall~i\]</span> Where <span class="math inline">\(C\)</span> is a constant value. Or, alternatively, <span class="math display">\[\frac{FP_i}{FP_j} = \frac{k_i}{k_j}~~~\forall~i,j\]</span> Where <span class="math inline">\(FP_i\)</span> is the number of false positives and <span class="math inline">\(k_i\)</span> the total number of <em>jailed</em> individuals in group <span class="math inline">\(i\)</span>.)</p>
<p>The False Positive Rate (<span class="math inline">\(FPR\)</span>) focuses on a different subset, specifically, the individuals who should <strong>not</strong> be subject to the intervention. Here, this would ask, “for an <em>innocent</em> person, what are the chances they will be wrongly convicted by virtue of the fact that they’re a member of a given group?”</p>
<p>^(probabilistic terms, <span class="math display">\[P(\textrm{wrongly jailed $\mid$ innocent, group $i$}) = C~~~\forall~i\]</span> Where <span class="math inline">\(C\)</span> is a constant value. Or, alternatively, <span class="math display">\[\frac{FP_i}{FP_j} = \frac{n_i \times (1-p_i)}{n_j \times (1-p_j)}~~~\forall~i,j\]</span> Where <span class="math inline">\(FP_i\)</span> is the number of false positives, <span class="math inline">\(n_i\)</span> the total number of individuals, and <span class="math inline">\(p_i\)</span> is the prevalence (here, rate of being truly guilty) in group <span class="math inline">\(i\)</span>.)</p>
<p>The difference between the choosing to focus on the <span class="math inline">\(FPR\)</span> and group size-adjusted false positives is somewhat nuanced and warrants highlighting:</p>
<ul>
<li><p>Having no disparities in group size-adjusted false positives implies that, if I were to choose a random person from a given group (regardless of group-level crime rates or their individual guilt or innocence), I would have the same chance of picking out a wrongly convicted person across groups.</p></li>
<li><p>Having no disparities in <span class="math inline">\(FPR\)</span> implies that, if I were to choose a random <em>innocent</em> person from a given group, I would have the same chance of picking out a wrongly convicted person across groups.</p></li>
</ul>
<p>By way of example, imagine you have a society with two groups (A and B) and a criminal process with equal <span class="math inline">\(FDR\)</span> and group-size adjusted false positives with:</p>
<ul>
<li><p>Group A has 1000 total individuals, of whom 100 have been jailed with 10 wrongfully convicted. Suppose the other 900 are all guilty.</p></li>
<li><p>Group B has 3000 total individuals, of whom 300 have been jailed with 30 wrongfully convicted. Suppose the other 2700 are all innocent.</p></li>
</ul>
<p>^(In this case, <span class="math display">\[\begin{aligned}
&amp;\frac{FP_A}{n_A} = \frac{10}{1000} = 1.0\% \\
&amp;FDR_A = \frac{10}{100} = 10.0\% \\
&amp;FPR_A = \frac{10}{10} = 100.0\%\end{aligned}\]</span></p>
<p>while, <span class="math display">\[\begin{aligned}
&amp;\frac{FP_B}{n_B} = \frac{30}{3000} = 1.0\% \\
&amp;FDR_B = \frac{30}{300} = 10.0\% \\
&amp;FPR_B = \frac{30}{2730} = 1.1\%\end{aligned}\]</span>)</p>
<p>That is,</p>
<ul>
<li><p>A randomly chosen individual has the same chance (1.0%) of being wrongly convicted regardless of which group they belong to</p></li>
<li><p>In both groups, a randomly chosen person who is convicted has the same chance (10.0%) of actually being innocent</p></li>
<li><p>HOWEVER, an innocent person in Group A is certain to be wrongly convicted, nearly 100 times the rate of an innocent person in Group B</p></li>
</ul>
<p>While this is an exaggerated case for illustrative purposes, there is a more general principle at play here, namely: when prevalences differ across groups, disparities cannot be eliminated from both the <span class="math inline">\(FPR\)</span> and group-size adjusted false positives at the same time (in the absence of perfect prediction).</p>
<p>While there is no universal rule for choosing a bias metric (or set of metrics) to prioritize, it is important to keep in mind that there are both theoretical and practical limits on the degree to which these metrics can be jointly optimized.</p>
<p>Balancing these trade-offs will generally require some degree of subjective judgment on the part of policy makers. For instance, if there is uncertainty in the quality of the labels (e.g., how well can we truly measure the size of the innocent population?), it may make more sense in practical terms to focus on the group-size adjusted false positives than <span class="math inline">\(FPR\)</span>.</p>
</div>
<div id="sec:assistiveexample" class="section level3">
<h3><span class="header-section-number">11.3.5</span> Assistive Example</h3>
<p>By contrast to the punitive case, when the application of a risk model is assistive in nature, individuals may be harmed by being incorrectly excluded from the “high risk” population that receives an intervention. Here, we use identifying families to receive a food assistance benefit as a motivating example. Where the punitive case focused on errors of inclusion through false positives, most of the metrics of interest in the assistive case focus on analogues that measure errors of omission through false negatives.</p>
<div id="count-of-false-negatives" class="section level4">
<h4><span class="header-section-number">11.3.5.1</span> Count of False Negatives</h4>
<p>A natural starting point for understanding whether a program is being applied fairly is to count how many people it is missing from each group, focusing on statements like “twice as many families with need for food assistance from Group A were missed by the benefit as from Group B.”</p>
<p>^(In probabilistic terms, we could express this as: <span class="math display">\[P(\textrm{missed by benefit, group $i$}) = C~~~\forall~i\]</span> Where <span class="math inline">\(C\)</span> is a constant value. Or, alternatively, <span class="math display">\[\frac{FN_i}{FN_j} = 1~~~\forall~i,j\]</span> Where <span class="math inline">\(FN_i\)</span> is the number of false negatives in group <span class="math inline">\(i\)</span>.)</p>
<p>Differences in the number of false negatives by group, however, may be relatively limited in measuring equity when the groups are very different in size. If there are twice as many families in Group A as in Group B in the example above, the larger number of false negatives might not be seen as inequitable, which motivates our next metric:</p>
</div>
<div id="group-size-adjusted-false-negatives" class="section level4">
<h4><span class="header-section-number">11.3.5.2</span> Group Size-Adjusted False Negatives</h4>
<p>To account for differently sized groups, one way of phrasing the question of fairness is to ask, “just by virtue of the fact that an individual is a member of a given group, what are the chances they will be missed by the food subsidy?”</p>
<p>^(That is, in terms of probability, <span class="math display">\[P(\textrm{missed by benefit $\mid$ group $i$}) = C~~~\forall~i\]</span> Where <span class="math inline">\(C\)</span> is a constant value. Or, alternatively, <span class="math display">\[\frac{FN_i}{FN_j} = \frac{n_i}{n_j}~~~\forall~i,j\]</span> Where <span class="math inline">\(FN_i\)</span> is the number of false negatives and <span class="math inline">\(n_i\)</span> the total number of families in group <span class="math inline">\(i\)</span>.)</p>
<p>While avoiding disparities on this metric focuses on the reasonable goal of treating different groups similarly in terms of classification errors, we may also want to directly consider two subsets within each group: (1) the set of families not receiving the subsidy, and (2) the set of families who would benefit from receiving the subsidy. We take a closer look at each of these cases below.</p>
</div>
<div id="false-omission-rate" class="section level4">
<h4><span class="header-section-number">11.3.5.3</span> False Omission Rate</h4>
<p>The False Omission Rate (<span class="math inline">\(FOR\)</span>) focuses specifically on people on whom the program doesn’t intervene – in our example, the set of families not receiving the food subsidy. Such families will either be true negatives (that is, those not in need of the assistance) or false negatives (that is, those who did need assistance but were missed by the program), and the <span class="math inline">\(FOR\)</span> asks what fraction of this set fall into the latter category.</p>
<p>^(In probabilistic terms, <span class="math display">\[P(\textrm{missed by program $\mid$ no subsidy, group $i$}) = C~~~\forall~i\]</span> Where <span class="math inline">\(C\)</span> is a constant value. Or, alternatively, <span class="math display">\[\frac{FN_i}{FN_j} = \frac{n_i-k_i}{n_j-k_j}~~~\forall~i,j\]</span> Where <span class="math inline">\(FN_i\)</span> is the number of false negatives, <span class="math inline">\(k_i\)</span> the number of families receiving the subsidy, and <span class="math inline">\(n_i\)</span> is the total number of families in group <span class="math inline">\(i\)</span>.)</p>
<p>In practice, the <span class="math inline">\(FOR\)</span> can be a useful metric in many situations, particularly because need can often be more easily measured among individuals not receiving a benefit than among those who do (for instance, when the benefit affects the outcome on which need is measured). However, when resources are constrained such that a program can only reach a relatively small fraction of the population, its utility is more limited. See <a href="chap-bias.html#sec:constrainedassistive">constrained assistive</a> for more details on this case.</p>
</div>
<div id="false-negative-rate" class="section level4">
<h4><span class="header-section-number">11.3.5.4</span> False Negative Rate</h4>
<p>The False Negative Rate (<span class="math inline">\(FNR\)</span>) focuses instead on the set of people with need for the intervention. In our example, this asks the question, “for a family that needs food assistance, what are the chances they will be missed by the subsidy by virtue of the fact they’re a member of a given group?”</p>
<p>^(In probabilistic terms, <span class="math display">\[P(\textrm{missed by subsidy $\mid$ need assistance, group $i$}) = C~~~\forall~i\]</span> Where <span class="math inline">\(C\)</span> is a constant value. Or, alternatively, <span class="math display">\[\frac{FN_i}{FN_j} = \frac{n_i \times p_i}{n_j \times p_j}~~~\forall~i,j\]</span> Where <span class="math inline">\(FN_i\)</span> is the number of false negatives, <span class="math inline">\(n_i\)</span> the total number of individuals, and <span class="math inline">\(p_i\)</span> is the prevalence (here, rate of need for food assistance) in group <span class="math inline">\(i\)</span>.)</p>
<p>As with the punitive case, there is some nuance in the difference between choosing to focus on group-size adjusted false negatives and the <span class="math inline">\(FNR\)</span> that are worth pointing out:</p>
<ul>
<li><p>Having no disparities in group size-adjusted false negatives implies that, if I were to choose a random family from a given group (regardless of group-level nutritional outcomes or their individual need), I would have the same chance of picking out a family missed by the program person across groups.</p></li>
<li><p>Having no disparities in <span class="math inline">\(FNR\)</span> implies that, if I were to choose a random family <em>with need for assistance</em> from a given group, I would have the same chance of picking out one missed by the subsidy across groups.</p></li>
<li><p>Unfortunately, disparities in both of these metrics cannot be eliminated at the same time, except where the level of need is identical across groups or in the generally unrealist case of perfect prediction.</p></li>
</ul>
</div>
</div>
<div id="sec:constrainedassistive" class="section level3">
<h3><span class="header-section-number">11.3.6</span> Special Case: Resource-Constrained Programs</h3>
<p>In many real-world applications, programs may only have sufficient resources to serve a small fraction of individuals who might benefit. In these cases, some of the metrics described here may prove less useful. For instance, where the number of individuals served is much smaller than the number of individuals with need, the false omission rate will converge on the overall prevalence, and it will prove impossible to balance <span class="math inline">\(FOR\)</span> across groups.</p>
<p>In such cases, group-level recall may provide a useful metric for thinking about equity, asking the question, “given that the program cannot serve everyone with need, is it at least serving different populations in a manner that reflects their level of need?”</p>
<p>^(In probabilistic terms, <span class="math display">\[P(\textrm{received subsidy $\mid$ need assistance, group $i$}) = C~~~\forall~i\]</span> Where <span class="math inline">\(C\)</span> is a constant value. Or, alternatively, <span class="math display">\[\frac{TP_i}{TP_j} = \frac{n_i \times p_i}{n_j \times p_j}~~~\forall~i,j\]</span> Where <span class="math inline">\(TP_i\)</span> is the number of true positives, <span class="math inline">\(n_i\)</span> the total number of individuals, and <span class="math inline">\(p_i\)</span> is the prevalence (here, rate of need for food assistance) in group <span class="math inline">\(i\)</span>.)</p>
<p>Note that, unlike the metrics described above, using recall as an equity metric doesn’t explicitly focus on the mistakes being made by the program, but rather on how it is addressing need within each group. Nevertheless, balancing recall is equivalent to balancing the false negative rate across groups (note that <span class="math inline">\(recall = 1-FNR\)</span>), but may be a more well-behaved metric for resource-constrained programs in practical terms. When the number of individuals served is small relative to need, <span class="math inline">\(FNR\)</span> will approach 1 and ratios between group-level <span class="math inline">\(FNR\)</span> values will not be particularly instructive, while ratios between group-level recall values will be meaningful.</p>
<p>As an aside, a focus on recall can also provide a lever that a program can use to consider options for achieving programmatic or social goals. For instance, if underlying differences in prevalence across groups is believed to be a result of social or historical inequities, a program may want to go further than balancing recall across groups, focusing even more heavily on historically under-served groups. One rule of thumb we have used in these cases is to balance recall relative to prevalence (however, there’s no theoretically “right” choice here and it’s generally best to consider a range of options):</p>
<p>^(<span class="math display">\[\frac{recall_i}{recall_j} = \frac{p_i}{p_j}~~~\forall~i,j\]</span>)</p>
<p>The idea here is that (assuming the program is equally effective across groups), balancing recall will seek to improve outcomes at an equal rate across groups without impacting underlying disparities while a heavier focus on previously under-served groups might seek to both improve outcomes across groups while attempting to close these gaps as well.</p>
</div>
</div>
<div id="sec:applications" class="section level2">
<h2><span class="header-section-number">11.4</span> Mitigating Bias</h2>
<p>The metrics described in this chapter can be put to use in a variety of ways: auditing existing models and processes for equitable results, in the process of choosing a model to deploy, or in making choices about how a chosen model is put into use. This section provides some details about how you might approach each of these tasks.</p>
<div id="auditing-model-results" class="section level3">
<h3><span class="header-section-number">11.4.1</span> Auditing Model Results</h3>
<p>Because the metrics describe here rely only on the predicted and actual labels, no specific knowledge of the process by which the predicted labels are determined is needed to make use of them to assess bias and fairness in the results. Given this sort of labeled outcome data for any existing or proposed process, these tools can be applied to help understand whether that process is yielding equitable results (for the various possible definitions of “equitable” described above).</p>
<p>Note that the existing process need not be a machine learning model: these equity metrics can be calculated for any set of decisions and outcomes, regardless of whether the decisions are derived from a model, judge, case worker, heuristic rule, or other process. And, in fact, it will generally be useful to make measures of equity in any existing processes which a model might augment or replace to help understand whether application of the model might improve, degrade, or leave unchanged the fairness of the existing system.</p>
</div>
<div id="model-selection" class="section level3">
<h3><span class="header-section-number">11.4.2</span> Model Selection</h3>
<p>As described in Chapter 6, many different types of models (each in turn with many tune-able hyperparameters) can be brought to bear on a given machine learning problem, making the task of selecting a specific model to put into use an important step in the process of model development. Chapter 6 described how this might be done by considering a model’s performance on various evaluation metrics, as well as how consistent that performance is across time or random splits of the data. This framework for model selection can naturally be extended to incorporate equity metrics, however doing so introduces a layer of complexity in determining how to evaluate trade-offs between overall performance and predictive equity.</p>
<p>Just as there is no one-size-fits-all metric for measuring equity that works in all contexts, you might choose to incorporate fairness in the model selection process in a variety of different ways. Here are a couple of options we have considered (though certainly not an exhaustive list):</p>
<ul>
<li><p>If many models perform similarly on overall evaluation metrics of interest (say, above some reasonably threshold), how do they vary in terms of equitability?</p></li>
<li><p>How much “cost” in terms of performance do you have to pay to reach various levels of fairness? Think of this as creating a menu of options to explicitly show the trade-offs involved. For instance, imagine your best-performing model has a precision of 0.75 but FDR ratio of 1.3, but you can reach an FDR ratio of 1.2 by selecting a model with precision of 0.73, or a ratio of 1.1 at a precision of 0.70, or FDR parity at a precision of 0.64.</p></li>
<li><p>You may want to consider several of the equity metrics described above and might look at the model that performs best on each metric of interest (perhaps above some overall performance threshold) and consider choosing between these options.</p></li>
<li><p>If you are concerned about fairness across several subgroups (e.g., multiple categories of race/ethnicity, different age groups, etc), you might consider exploring the models that perform best for each subgroup in addition to those that perform similarly across groups</p></li>
<li><p>Another option might be to develop a single model selection parameter that penalizes performance by how far a model is from equity and explore how model choice changes based on how heavily you weight the equity parameter. Note, however, that when you are comparing equity across more than two groups, you will need to find a means of aggregating these to a single value (e.g., you might look at the average disparity, largest disparity, or use some weighting scheme to reflect different costs of disparities favoring different groups)</p></li>
</ul>
<p>In most cases, this process will yield a number of options for a final model to deploys: some with better overall performance, some with better overall equity measures, and some with better performance for specific subgroups. Unlike model selection based on performance metrics alone, the final choice between these will generally involve a judgment call that reflects the project’s dual goals of balancing accuracy and equity. As such, the final choice of model from this narrowed menu of options is best treated as a discussion between the data scientists and stakeholders in the same manner as choosing how to define fairness in the first place.</p>
</div>
<div id="other-options-for-mitigating-bias" class="section level3">
<h3><span class="header-section-number">11.4.3</span> Other Options for Mitigating Bias</h3>
<p>Beyond incorporating measurements of equity into your model selection process, they can also inform how you put the model you choose into action. In general, disparities will vary as you vary the threshold used for turning continuous scores into 0/1 predicted classes. While many applications will dictate the total number of individuals who can be selected for intervention, it may still be useful to consider lower thresholds. For instance, in one project we saw large <span class="math inline">\(FDR\)</span> disparities across age and race in our models when selecting the top 150 individuals for an intervention (a number dictated by programmatic capacity), but these disparities were mitigated by considering the top 1000 with relatively little cost in precision. This result suggested a strategy for deployment: use the model to select the 1000 highest risk individuals and randomly select 150 individuals from this set to stay within the program’s capacity while balancing equity and performance.</p>
<p>Another approach that can work well in some situations is to consider using different thresholds across groups to achieve more equitable results. This is perhaps most robust where the metric of interest in monotonically increasing or decreasing with the number of individuals chosen for intervention (such as recall). This can be formulated in two ways:</p>
<ul>
<li><p>For programs that have a target scale but may have some flexibility in budgeting, you can look at to what extent the overall size of the program would need to increase to achieve equitable results (or other fairness goals such as those described in <a href="chap-bias.html#sec:constrainedassistive">constrained assistive</a>. In this case, interventions don’t need to be denied to any individuals in the interest of fairness, but the program would incur some additional cost in order to achieve a more equitable result.</p></li>
<li><p>If the program’s scale is a hard constraint, you may still be able to use subgroup-specific thresholds to achieve more equitable results by selecting fewer of some groups and more of others relative to the single threshold. In this case, the program would not need additional costs of expansion, but some individuals who might have received the intervention based just on their score would need to be substituted for individuals with somewhat lower scores of under-represented subgroups.</p></li>
</ul>
<p>As you’re thinking about equity in the application of your machine learning models, it’s also particularly important to keep in mind that measuring fairness in a model’s predictions is only a proxy for what you fundamentally care about: fairness in outcomes in the presence of the model. As a model is put into practice, you may find that the program itself is more effective for some groups than others, motivating either additional changes in your model selection process or customizing interventions to the specific needs of different populations (or individuals). Incorporating fairness into decisions about who is chosen to receive an intervention is an important first step, but shouldn’t be mistaken for a comprehensive solution to disparities in a program’s application and outcomes.</p>
<p>%cite LA ACM FAT* paepr</p>
<p>Some work is also being done investigating means for incorporating bias and fairness more directly in the process of model development itself. For instance, in may cases different numbers of examples across groups or unmeasured variables may contribute to a model having higher error rates on some populations than others and additional data collection (either more examples or new features) may help mitigate these biases where doing so is feasible ****. Other work is being done to explore the results of incorporating equity metrics directly into the loss functions used to train some classes of machine learning models, making balancing accuracy and equity an aspect of model training itself ****. While we don’t explore these more advanced topics in depth here, we refer you to the cited articles for more detail.</p>
</div>
</div>
<div id="further-considerations" class="section level2">
<h2><span class="header-section-number">11.5</span> Further Considerations</h2>
<div id="compared-to-what" class="section level3">
<h3><span class="header-section-number">11.5.1</span> Compared to What?</h3>
<p>While building machine learning models that are completely free of bias is an admirable goal, it may not always be an achievable one. Nevertheless, even an imperfect model may provide an improvement over current practices depending on the degree of bias involved in existing processes. It’s important to be cognizant of the existing context and make measurements of equity for current practices as well as new algorithms that might replace or augment them. The status quo shouldn’t be assumed to be free of bias because it is familiar any more than algorithms should be assumed capable of achieving perfection simply because they are complex. In practice, a more nuanced view is likely to yield better results: new models should be rigorously compared with current results and implemented when they are found to yield improvements but continually refined to improve on their outcomes over time as well.</p>
</div>
<div id="costs-to-both-errors" class="section level3">
<h3><span class="header-section-number">11.5.2</span> Costs to Both Errors</h3>
<p>In the examples in Section <a href="chap-bias.html#sec:metrics">metrics</a>, we focused on programs that could be considered purely assistive or purely punitive to illustrate some of the relevant metrics for such programs. While this classification may work for some real-world applications, in many others there will be costs associated with both errors of inclusion and errors of exclusion that need to be considered together in deciding both on how to think about fairness and how to put those definitions into practice through model selection and deployment. For the bail example, there are of course real costs to society both of jailing innocent people and releasing someone who does, in fact, commit a subsequent crime. In many programs where individuals may be harmed by being left out, errors of inclusion may mean wasted resources or even political backlash about excessive budgets.</p>
<p>In theory, you might imagine being able to assign some cost to each type of error — as well as to disparities in these errors across groups — and make a single, unified cost-benefit calculation of the net result of putting a given model into application in a given way. In practice, of course, making an even reasonable quantitative estimate of the individual and social costs of these different types of errors is likely infeasible in most cases. Instead, a more practical approach generally involves exploring a number of different options through different choices of models and parameters and using these options to motivate a conversation about the program’s goals, philosophy, and constraints.</p>
</div>
<div id="what-is-the-relevant-population" class="section level3">
<h3><span class="header-section-number">11.5.3</span> What is the Relevant Population?</h3>
<p>Related to the sample bias discussed in <a href="chap-bias.html#sec:biassources">bias sources</a>, understanding the relevant population for your machine learning problem is important both to the modeling itself and to your measures of equity. Calculation of metrics like the group-size adjusted false positive rate or false negative rate will vary depending on who is included in the denominator.</p>
<p>For instance, imagine modeling who should be selected to receive a given benefit using data from previous applicants and looking at racial equity based on these metrics. What population is actually relevant to thinking about equity in this case? It might be the pool of applicants available in your data, but it just as well might be the set of people who might apply if they had knowledge of the program (regardless of whether or not they actually do), or perhaps even the population at large (for instance, as measured by the census). Each of those choices could potentially lead to different conclusions about the fairness of the program’s decisions (either in the presence or absence of a machine learning model), highlighting the importance of understanding the relevant population and who might potentially be left out of your data as an element of how fairness is defined in your context. Keep in mind that determining (or at least making a reasonable estimate of) the correct population may at times require collecting additional data.</p>
</div>
<div id="continuous-outcomes" class="section level3">
<h3><span class="header-section-number">11.5.4</span> Continuous Outcomes</h3>
<p>For the sake of simplicity, we focused here on binary classification problems to help illustrate the sorts of considerations you might encounter when thinking about fairness in the application of machine learning techniques. However, these considerations do of course extend to other types of problems, such as regression models of continuous outcomes.</p>
<p>In these cases, bias metrics can be formulated around aggregate functions of the errors a model makes on different types of individuals (such as the mean squared error and mean absolute error metrics you are likely familiar with from regression) or tests for similarity of the distributions of these errors across subgroups. Working with continuous outcomes adds an additional layer of complexity in terms of defining fairness to account for the magnitude of the errors being made (e.g., how do you choose between a model that makes very large errors on a small number of individuals vs one that makes relatively small errors on a large number of individuals?).</p>
<p>If you would like to learn more about understanding bias and fairness in machine learning problems with continuous outcomes, we suggest consulting **** for a useful overview.</p>
</div>
<div id="considerations-for-ongoing-measurement" class="section level3">
<h3><span class="header-section-number">11.5.5</span> Considerations for Ongoing Measurement</h3>
<p>The role of a data scientist is far from over when their machine learning model gets put into production. Making use of these models requires ongoing curation, both to guard against degradation in terms of performance or fairness as well as to constantly improve outcomes. The vast majority of models you put into production will make mistakes, and a responsible data scientist will seek to look closely at these mistakes and understand — on both individual and population levels — how to learn from them to improve the model. Ensuring errors are balanced across groups is a good starting point, but seeking to reduce these errors over time is an important aspect of fairness as well.</p>
<p>One challenge you may face in making these ongoing improvements to your model is with measuring outcomes in the presence of a program that seeks to change them. In particular, the measurement of true positives and false positives in the absence of knowledge of a counterfactual (that is, what would have happened in the absence of intervention) may be difficult or impossible. For instance, among families who have improved nutritional outcomes after receiving a food subsidy, you may not be able to ascertain which families’ outcomes were actually helped by the program versus which would have improved on their own, obfuscating any measure of recall you might use to judge performance or equity. Likewise, for individuals denied bail, you cannot know if they actually would have fled or committed a crime had they been released, making metrics like false discovery rate impossible to calculate.</p>
<p>During a model’s pilot phase, initially making predictions without taking action or using the model in parallel with existing processes can help mitigate some of these measurement problems over the short term. Likewise, when resources are limited such that only a fraction of individuals can receive an intervention, using some degree of randomness in the decision-making process can help establish the necessary counterfactual. However, in many contexts, this may not be practical or ethical, and you will need to consider other means for ongoing monitoring of the model’s performance. Even in these contexts, however, it is important to continually review the performance of the models and seek to both improve its performance in terms of both equity and efficiency. In practice, this may include some combination of considering proxies for the counterfactual, quasi-experimental inference methods, and expert/stakeholder review of the model’s results (both in aggregate and of selected individual cases).</p>
</div>
<div id="equity-in-practice" class="section level3">
<h3><span class="header-section-number">11.5.6</span> Equity in Practice</h3>
<p>Much of the discussion here has been about fairness in the machine learning pipeline, focused on the ways in which a model may be correct or incorrect in its predictions and how these might vary across groups. As a responsible practitioner of data science, these issues are no doubt important to understand and seek to get correct in your models, but fundamentally they can only serve as a proxy for a bigger concept of fairness – ultimately, we care about equity in terms of differences in outcomes across groups. Ensuring fairness in decisions (whether made by machines, humans, or some combination) is an element of achieving this goal, but neither is it the only element nor, in many cases, is it likely to be the largest one. In the face of other potential sources of bias — sample, label, application, historical, and societal — even fair decisions at the machine learning level may not lead to equitable results in society and the decision-making process may need to compensate for these other inequities. Some of these other sources of bias may be more challenging to quantify or incorporate into models directly, but data scientists have a shared responsibility to understand the broader context in which their models will be applied and seek equitable outcomes in these applications.</p>
</div>
<div id="other-names-you-might-see" class="section level3">
<h3><span class="header-section-number">11.5.7</span> Other Names You Might See</h3>
<p>The metrics described here have been given a variety of names in the literature. While we have tried to use language focused on the underlying statistics in this chapter, here are some other names you might see for several of these metrics in the literature:</p>
<ul>
<li><p>Equalizing <strong>false discovery rates (<span class="math inline">\(FDR\)</span>)</strong> is sometimes referred to as <strong>predictive parity</strong> or the <strong>“outcome test”</strong>. Note that the is mathematically equivalent to having equal <strong>precision</strong> (also called <strong>positive predictive value</strong>) across groups.</p></li>
<li><p>Equalizing <strong>false omission rates (<span class="math inline">\(FOR\)</span>)</strong> is mathematically equivalent to equalizing the <strong>negative predictive value (<span class="math inline">\(NPV\)</span>)</strong>.</p></li>
<li><p>When both <span class="math inline">\(FOR\)</span> and <span class="math inline">\(FDR\)</span> are equal across groups at the same time, this is sometimes referred to as <strong>sufficiency</strong> or <strong>conditional use accuracy equality</strong>.</p></li>
<li><p>Equalizing the <strong>false negative rate (<span class="math inline">\(FNR\)</span>)</strong>, which is equivalent to equalizing <strong>recall</strong> (also called the <strong>true positive rate</strong> or <strong>sensitivity</strong>), is also sometimes called <strong>equal opportunity</strong>.</p></li>
<li><p>Equalizing the <strong>false positive rates (<span class="math inline">\(FPR\)</span>)</strong>, which is equivalent to equilizing the <strong>true negative rate</strong> (also known as <strong>specificity</strong>), is sometimes called <strong>predictive equality</strong>.</p></li>
<li><p>When both <span class="math inline">\(FNR\)</span> and <span class="math inline">\(FPR\)</span> is equal across groups (that is, when both <strong>equal opportunity</strong> and <strong>predictive equality</strong> are satisfied), various authors have referred to this as <strong>error rate balance</strong>, <strong>separation</strong>, <strong>equalized odds</strong>, <strong>conditional procedure accuracy equality</strong>, or <strong>disparate mistreatment</strong>.</p></li>
<li><p>When members of every group have an equal probability of being assigned to the positive predictive class, this condition is referred to as <strong>group fairness</strong>, <strong>statistical parity</strong>, <strong>equal acceptance rate</strong>, <strong>demographic parity</strong>, or <strong>benchmarking</strong>. When this true up to the contributions of only a set of “legitimate” attributes allowed to affect the prediction, this is called <strong>conditional statistical parity</strong> or <strong>conditional demographic parity</strong>.</p></li>
<li><p>One definition, termed <strong>treatment equality</strong>, suggests considering disparities in the ratio of false negatives to false positives across groups.</p></li>
<li><p>Metrics that look at the entirety of the score distribution across groups include <strong>AUC parity</strong> and <strong>calibration</strong> (also called <strong>test fairness</strong>, <strong>matching conditional frequencies</strong>, or under certain conditions, <strong>well-calibration</strong>). Similarly, <strong>balance for the positive class</strong> and <strong>balance for the negative class</strong> look at average scores across groups among individuals with positive or negative labels, respectively.</p></li>
<li><p>Additional work is being done looking at the fairness through the lens of similarity between individuals (…) and causal reasoning (…).</p></li>
</ul>
<p>As a field, we have yet to settle on a single widely-accepted terminology for thinking about bias and fairness, but rather than get lost in competing naming conventions, we would encourage you to focus on what disparities in the different underlying metrics actually mean for how models you build might actually affect different populations in your particular context.</p>
<hr />
</div>
</div>
<div id="case-studies" class="section level2">
<h2><span class="header-section-number">11.6</span> Case Studies</h2>
<p>The active conversation about algorithmic bias and fairness in both the academic and popular press has contributed to a more well-rounded evaluation of many of the models and technologies that are already in everyday use. This section highlights several recent cases, discussing them through the context of the metrics described above as well as providing some resources for you to read further about each one.</p>
<div id="sec:compascase" class="section level3">
<h3><span class="header-section-number">11.6.1</span> Recidivism Predictions with COMPAS</h3>
<p>Over the course of the last two decades, models of recidivism risk have been put into use in many jurisdictions around the country. These models show a wide variation in methods (from heuristic rule-based scores to machine learning models) and have been developed by a variety of academic researchers, government employees, and private companies, many built with little or no evaluation of potential biases (Desmarais and Singh). Different jurisdictions put these models to use in a variety of ways, including identifying individuals for diversion &amp; treatment programs, making bail determinations, and even in the course of sentencing decisions.</p>
<p>In May 2016, journalists at ProPublica undertook an exploration of accuracy and racial bias in these algorithms, focusing on one the commercial solutions called Correctional Offender Management Profiling for Alternative Sanctions (COMPAS), built by a company called Northpointe. Their analysis focused on some of the errors made by the model, finding dramatic disparities between black and white defendants: among black defendants who would did not end up with another arrest in the subsequent two years, 45% were labeled by the system as high risk, almost twice the rate for whites (23%). Likewise, among individuals who did recidivate within two years, 48% of white defendants were labeled low risk, compared with 28% of black defendants.</p>
<p>Here, ProPublica was focusing on <span class="math inline">\(FPR\)</span> and <span class="math inline">\(FNR\)</span> metrics for their definition of fairness: e.g., if you’re a person who, in fact, will not recidivate, do your chances of being mislabeled as high risk by the model differ depending on your race? In their response, Northpointe argued that this was the wrong fairness metric in this context — instead, they claimed, <span class="math inline">\(FDR\)</span> should be the focus: If the model labels you as high risk, do the chances that it was wrong in doing so depend on your race? By this definition, COMPAS is fair: 37% of black defendants labeled as high risk did not recidivate, compared to 41% of white defendants. Table <a href="chap-bias.html#tab:compastable">11.1</a> summarizes these metrics for both racial groups.</p>
<table>
<caption><span id="tab:compastable">Table 11.1: </span> COMPAS Fairness Metrics</caption>
<thead>
<tr class="header">
<th align="left"><strong>Metric</strong></th>
<th align="center"><strong>Caucasian</strong></th>
<th align="center"><strong>African American</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">False Positive Rate (<span class="math inline">\(FPR\)</span>)</td>
<td align="center">23%</td>
<td align="center">45%</td>
</tr>
<tr class="even">
<td align="left">False Negative Rate (<span class="math inline">\(FNR\)</span>)</td>
<td align="center">48%</td>
<td align="center">28%</td>
</tr>
<tr class="odd">
<td align="left">False Discovery Rate (<span class="math inline">\(FDR\)</span>)</td>
<td align="center">41%</td>
<td align="center">37%</td>
</tr>
</tbody>
</table>
<p>In a follow-up article in December 2016, the ProPublica authors remarked on the surprising result that a model could be “simultaneously fair and unfair.” The public debate around COMPAS also inspired a number of academic researchers to look closer at these definitions of fairness, showing that in the presence of different base rates, it would be impossible for a model to satisfy both definitions of fairness at the same time. The case of COMPAS demonstrates the potentially dramatic impact of decisions about how equity is defined and measured in real applications with considerable implications for individual’s lives. It’s incumbent upon the researchers developing such a model, the policymakers deciding to put it into practice, and the users making decisions based upon its scores to understand and explore the options for measuring fairness as well as the trade-offs involved in making that choice.</p>
</div>
<div id="facial-recognition" class="section level3">
<h3><span class="header-section-number">11.6.2</span> Facial Recognition</h3>
<p>A growing number of applications for facial recognition software are being found, from tagging friends in photos on social media to recognizing suspects by police departments, and off-the-shelf software is available from several large technology firms, including Microsoft, IBM, and Amazon. However, growth in the use of technologies has seen a number of embarrassing stumbles related to how well they work across race along the way: In 2015, Google released an automated image annotation app that mistakenly tagged several African American users as gorillas; and a number of early applications deployed on digital cameras would erroneously tell Asian users to open their eyes or fail to detect users with darker skin tones entirely.</p>
<p>Despite the broad uses of these technologies, even in policing, relatively little work had been done to quantify their racial bias prior to 2018 when a researcher at MIT’s Media Lab undertook a study to assess racial bias in the ability to correctly detect gender of three commercial facial recognition applications (developed by Microsoft, Face++, and IBM). She developed a benchmark dataset reasonably well-balanced across race and gender by collecting 1,270 photos of members of parliament in several African and European nations, scoring each photo for skin tone using the Fitzpatrick Skin Type classification system commonly used in dermatology.</p>
<p>The results of this analysis showed stark differences across gender and skin tone, focusing on false discovery rates for predictions of each gender. Overall, <span class="math inline">\(FDR\)</span> was very low for individuals predicted to be male in the dataset, ranging from 0.7% to 5.6% between systems, while it was much higher among individuals predicted to be female (ranging from 10.7% to 21.3%). Notice that the models here are making a binary classification of gender, so individuals with a score on one side of a threshold are predicted as male and on the other side are predicted as female. The overall gender disparities seen here, then, indicate that at least relative to this dataset, all three thresholds were chosen in such a way that the models are more certain when making a prediction that an individual is male than making a prediction that they are female. In theory, these thresholds could be readily tuned to produce a better balance in errors, but Buolamwini notes that all three APIs provide only predicted classes rather than the underlying scores, precluding users from choosing a different balance of error rates by predicted gender.</p>
<p>The disparities were even more stark when considering skin tone and gender jointly. In general model performance was much worse for individuals with darker skin tones than those with lighter skin. Most dramatically, the <span class="math inline">\(FDR\)</span> for individuals with darker skin who were predicted to be female ranged from 20.8% to 34.7%. At the other extreme, the largest <span class="math inline">\(FDR\)</span> for lighter-skinned individuals predicted to be male was under 1%. Table <a href="chap-bias.html#tab:facialrectable">11.2</a> shows these results in more detail.</p>
<table>
<caption><span id="tab:facialrectable">Table 11.2: </span> <span class="math inline">\(FDR\)</span> Values By Skin Tone and Predicted Gender (F = Female, M = Male, D = Dark Skin, L = Light Skin)</caption>
<thead>
<tr class="header">
<th align="left"><strong>System</strong></th>
<th align="center"><strong>All F</strong></th>
<th align="center"><strong>All M</strong></th>
<th align="center"><strong>DF</strong></th>
<th align="center"><strong>DM</strong></th>
<th align="center"><strong>LF</strong></th>
<th align="center"><strong>LM</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Microsoft</td>
<td align="center">10.7%</td>
<td align="center">2.6%</td>
<td align="center">20.8%</td>
<td align="center">6.0%</td>
<td align="center">1.7%</td>
<td align="center">0.0%</td>
</tr>
<tr class="even">
<td align="left">Face++</td>
<td align="center">21.3%</td>
<td align="center">0.7%</td>
<td align="center">34.5%</td>
<td align="center">0.7%</td>
<td align="center">6.0%</td>
<td align="center">0.8%</td>
</tr>
<tr class="odd">
<td align="left">IBM</td>
<td align="center">20.3%</td>
<td align="center">5.6%</td>
<td align="center">34.7%</td>
<td align="center">12.0%</td>
<td align="center">7.1%</td>
<td align="center">0.3%</td>
</tr>
</tbody>
</table>
<p>One factor contributing to these disparities is likely sample bias. While the training data used for these particularly commercial models is not available, many of the widely available public data sets for developing similar facial recognition algorithms have been heavily skewed, with as many as 80% of training images being of white individuals and 75% being of men. Improving the representativeness of these data sets may be helpful, but wouldn’t eliminate the need for ongoing studies of disparate performance of facial recognition across populations that might arise from other characteristics of the underlying models as well.</p>
<p>These technologies also provide a case study for when policy makers might decide against putting a given model to use for certain applications entirely. In 2019, the city of San Francisco, CA, announced that it would become the first city in the country to ban the use of facial recognition technologies entirely from city services, including its police department. There, city officials reached the conclusion that any potential benefits of these technologies were outweighed by the combination of potential biases and overall privacy concerns, with the city’s Board of Supervisors voting 8-1 to ban the technology. While the debate around appropriate uses for facial recognition is likely to continue for some time across jurisdictions, San Francisco’s decision highlights the role of legal and policy constraints around how models are used in addition to ensuring that the models are fair when and where they are applied.</p>
</div>
<div id="facebook-advertisement-targeting" class="section level3">
<h3><span class="header-section-number">11.6.3</span> Facebook Advertisement Targeting</h3>
<p>Social media has created new opportunities for advertisers to quickly and easily target their advertisements to particular subsets of the population. Additionally, regardless of this user-specified targeting, these advertising platforms will make automated decisions about who is shown a given advertisement, generally optimizing to some metric of cost efficiency. Recently, however, these tools have begun to come under scrutiny surrounding the possibility that they might violate US Civil Rights laws that make it illegal for individuals to be excluded from job or housing opportunities on the basis of protected characteristics such as age, race, or sex.</p>
<p>Public awareness that Facebook’s tools allowed advertisers to target content based on these protected characteristics began to form in 2016 with news reports highlighting the feature. While the company initially responded that their policies forbid advertisers from targeting ads in discriminatory ways, there were legitimate use cases for these technologies as well, suggesting that the responsibility fell to the people placing the ads. By 2018, however, it was clear that the platform was allowing some advertisers to do just that and the American Civil Liberties Union filed a complain of gender discrimination with the US Equal Employment Opportunity Commission. The complaint pointed to 10 employers that had posted job ads targeted exclusively to men, including positions such as truck drivers, tire salesmen, mechanics, and security engineers. Similar concerns were cited by the US Department of Housing and Urban Development in 2019 when it filed charges against the social media company alleging it had served ads that violate the Fair Housing Act. Responding to the growing criticism, Facebook began to the limit the attributes advertisers could use to target their content.</p>
<p>However, these limitations might not be sufficient in light of the platform’s machine learning algorithms that are determining who is shown a given ad regardless of the specific targeting parameters. Research performed by Ali, et al. (2019), confirmed that the content of an advertisement could have a dramatic impact on who it was served to despite broad targeting parameters. Users who were shown a job posting for a position as a lumberjack were more than 90% men and more than 70% white, while those seeing a posting for a janitorial position were over 75% women and 65% black. Similarly wide variety was seen for housing advertisements, ranging from an audience nearly 65% black in some conditions to 85% white in others. A separate study of placement of STEM career ads with broad targeting found similar gender biases in actual impressions, with content shown to more men than women.</p>
<p>Unlike the other case studies described above, the concept of fairness for housing and job advertisements is provided by existing legislation, focusing not on errors of inclusion or exclusion, but rather on representativeness itself. As such, the metric of interest here is disparity in the probability of being assigned to the predicted positive class (e.g., being shown the ad) across groups, unrelated to potentially differential propensities of each group to respond. To address these disparities, Facebook (as well as other ad servers) may need to modify their targeting algorithms to directly ensure job and housing ads are shown to members of protected groups at similar rates. This, in turn, would require a reliable mechanism for determining whether a given ad is subject to these requirements, which poses technical challenges in its own right. As of this writing, understanding how best to combat discrimination in ad targeting is an ongoing area of research as well as an active public conversation.</p>
<p>%remove next example?</p>
</div>
<div id="tay-microsofts-ai-twitter-bot" class="section level3">
<h3><span class="header-section-number">11.6.4</span> Tay: Microsoft’s AI Twitter Bot</h3>
<p>In March, 2016, Microsoft released a Twitter chatbot named Tay with the intention of showcasing their AI technology for understanding conversational language. The bot was targeted at younger Twitter users, hoping to carry on fun, colloquial conversations with them while learning to improve its conversational skills through these interactions. Resulting at least in part from an attack coordinated on 4Chan, within 24 hours, Tay’s tweets had taken a decidedly dark turn, generating tweets with racist, misogynistic, and anti-Semitic messages (Figure <a href="chap-bias.html#fig:taytweets">11.1</a>. In the face of this hateful, offensive content, Microsoft had shut Tay down by the end of its first day. Issuing a public apology, the company indicated that although the had “stress-tested” the bot in several contexts, they had failed to anticipate the type of abuse of it encountered in the wild.</p>
<div class="figure" style="text-align: center"><span id="fig:taytweets"></span>
<img src="ChapterBias/figures/tay_tweets2.png" alt="Example of Tays offensive output" width="80%" />
<p class="caption">
Figure 11.1: Example of Tays offensive output
</p>
</div>
<p>Although the stakes are arguably relatively low for a chatbot built for entertainment, the story of Tay is a cautionary one for more consequential applications of these technologies. In practice, this experience can be seen through the lens of sample and application biases: not only was the corpus of data on which Microsoft initially trained Tay vastly different from the data on which in ultimately operated, but the way in which people ultimately used the system was out of step with its anticipated use. While these issues may be particularly dramatic for models like Tay that learn on the fly, they can certainly have an adverse effect on more static models as well. Tay’s nearly immediate transformation from a fun, playful chatbot to one spewing cruelty and hate showcases not only the need for data scientists to anticipate how their models and technologies might be abused as well as the critical need for continual monitoring for both performance metrics and biases in any deployed application.</p>
</div>
</div>
<div id="aequitas---a-toolkit-for-auditing-bias-and-fairness-in-machine-learning-models" class="section level2">
<h2><span class="header-section-number">11.7</span> Aequitas - A Toolkit for Auditing Bias and Fairness in Machine Learning Models</h2>
<p>To help data scientists and policymakers make informed decisions about bias and fairness in their applications, we developed Aequitas, an open source<a href="#fn71" class="footnoteRef" id="fnref71"><sup>71</sup></a> bias and fairness audit toolkit that was released in May 2018.<a href="#fn72" class="footnoteRef" id="fnref72"><sup>72</sup></a> It is an intuitive and easy to use addition to the machine learning workflow, enabling users to seamlessly audit models for several bias and fairness metrics in relation to multiple population sub-groups. Aequitas can be used directly as a Python library, via command line interface or a web application, making it accessible and friendly to a wide range of users (from data scientists to policymakers).</p>
<p>Because the concept of fairness is highly dependent on the particular context and application, Aequitas provides comprehensive information on how its results should be used in a public policy context, taking the resulting interventions and its implications into consideration. It is intended to be used not just by data scientists but also policymakers, through both seamless integration in the machine learning workflow as well as a web app tailored for non-technical users auditing these models’ outputs.</p>
<p>In Aequitas, bias assessments can be made prior to model selection, evaluating the disparities of the various models developed based on whatever training data was used to tune it for its task. The audits can be performed prior to a model being operationalized, based on operational data of how biased the model proved to be in holdout data. Or they can involve a bit of both, auditing bias in an A/B testing environment in which limited trials of revised algorithms are evaluated whatever biases were observed in those same systems in prior production deployments.</p>
<p>Aequitas was designed to be used by two types of users:</p>
<ol style="list-style-type: decimal">
<li><p>Data Scientists and AI Researchers who are building systems for use in risk assessment tools. They will use Aequitas to compare bias measures and check for disparities in different models they are building during the process of model building and selection.</p></li>
<li><p>Policymakers who, before “accepting” an AI system to use in a policy decision, will run Aequitas to understand what biases exist in the system and what (if anything) they need to do in order to mitigate those biases. This process must be repeated periodically to assess the fairness degradation through time of a model in production.</p></li>
</ol>
<p>![Aequitas in the larger context of the machine learning pipeline. Audits must be carried out internally by data scientists before evaluation and model selection. Policymakers (or clients) must audit externally before accepting a model in production as well as perform periodic audits to detect any fairness degradation over time.</p>
<div class="figure" style="text-align: center"><span id="fig:ml-pipeline"></span>
<img src="ChapterBias/figures/audit_pipeline.png" alt="ML pipeline" width="80%" />
<p class="caption">
Figure 11.2: ML pipeline
</p>
</div>
<p>Figure <a href="chap-bias.html#fig:ml-pipeline">11.2</a> puts Aequitas in the context of the machine learning workflow and shows which type of user and when the audits must be made. The main goal of Aequitas is to standardize the process of understanding model biases. By providing a toolkit for auditing by both data scientists and decision makers, it makes it possible for these different actors to take bias and fairness into consideration at all stages of decision-making in the modeling process: model selection, whether or not to deploy a model, when to retrain, the need to collect more and better data, and so on.</p>
<!-- 
% move following to workbooks chapter
-->
<div id="getting-started-with-aequitas" class="section level3">
<h3><span class="header-section-number">11.7.1</span> Getting Started with Aequitas</h3>
<p>Aequitas is a python package that can be used in several ways: as an imported module in your code or jupyter notebooks, as a command-line utility, or served as a web interface. To help orient you to its interface and explore some of the concepts covered in this chapter in more detail, we have developed a tutorial notebook around the COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) case study described in Section <a href="chap-bias.html#sec:compascase">COMPAS case</a>.<a href="#fn73" class="footnoteRef" id="fnref73"><sup>73</sup></a> Each of the tutorial’s sections is described briefly below.</p>
</div>
<div id="requirements" class="section level3">
<h3><span class="header-section-number">11.7.2</span> Requirements</h3>
<p>To work with the Aequitas tutorial, you will need a jupyter notebook server running a python 3.6 (or higher) kernel with the <code>pandas</code> and <code>seaborn</code> packages installed. In addition, you will need to install the <code>aequitas</code> module:</p>
<pre><code>    pip install aequitas</code></pre>
<p>Alternatively, you can install from source with:</p>
<pre><code>    git clone https://github.com/dssg/aequitas.git
    cd aequitas
    python setup.py install</code></pre>
</div>
<div id="data-preparation" class="section level3">
<h3><span class="header-section-number">11.7.3</span> Data Preparation</h3>
<p>Included with the tutorial notebook is a sample dataset, <code>compas_for_aequitas.csv</code>, representing two years of COMPAS recidivism predictions and subsequent arrest outcomes from Broward County, Florida. The data provided here represent the same data used in the ProPublica analysis described above, cleaned somewhat and structured for use with Aequitas.</p>
<p>The example dataset has be preprocessed into the format required in order to use Aequitas. To do so, your input data should be provided at the individual entity level, and contain the following columns:</p>
<ul>
<li><p>A <code>score</code> column, which may be either a binary (0 or 1) or continuous (between 0.0 and 1.0) value representing the output of a predictive model (for a continuous case, you will need to provide a classification threshold for the analysis). For the example data included here, the COMPAS scores have been mapped to a binary score based on ProPublica’s interpretation of Northpointe’s practioner guide, with 0 representing a <code>low</code> COMPAS score and 1 representing a <code>medium</code> or <code>high</code> score.</p></li>
<li><p>A <code>label_value</code> column, provided as a binary (0 or 1) value, representing the actual outcome for each entity. Again following ProPublica, the example dataset defines a recidivism outcome (<code>label_value</code> = 1) as a new arrest within two years.</p></li>
<li><p>One or more attribute columns for which you would like to evaluate predictive fairness and disparities. Here, you can consider any attributes of interest for your particular application. For instance, the example dataset includes the columns <code>race</code>, <code>sex</code>, and <code>age_cat</code> that will be used for our tutorial.</p></li>
</ul>
<p>As you follow along in the tutorial notebook, we start with a little bit of descriptive data exploration just to get a feel for the data. Figure <a href="chap-bias.html#fig:tutorial-explore">11.3</a> shows the large difference in the distribution of COMPAS scores across race. The notebook guides you through several additional initial analyses and you should feel free to further explore the data as well.</p>
<div class="figure" style="text-align: center"><span id="fig:tutorial-explore"></span>
<img src="ChapterBias/figures/tutorial_explore.png" alt="Data exploration screenshot from the Aequitas tutorial" width="100%" />
<p class="caption">
Figure 11.3: Data exploration screenshot from the Aequitas tutorial
</p>
</div>
<p>Applying Aequitas progammatically is a three step process represented by three python classes that will be described in the following sections:</p>
<ul>
<li><p><code>Group()</code>: Define groups</p></li>
<li><p><code>Bias()</code>: Calculate disparities</p></li>
<li><p><code>Fairness()</code>: Assert fairness</p></li>
</ul>
</div>
<div id="working-with-bias-metrics" class="section level3">
<h3><span class="header-section-number">11.7.4</span> Working with Bias Metrics</h3>
<p>In the second section of the notebook, you will learn how to use Aequitas to understand the distribution of your data and outcomes, as well as measure, visualize, and interpret bias metrics of interest. To perform these analyses, you’ll make use of the <code>Group()</code> and <code>Plot()</code> classes, which can be imported with:</p>
<pre><code>    from aequitas.group import Group
    from aequitas.plotting import Plot</code></pre>
<p>Aequitas’s <code>Group()</code> class enables researchers to evaluate biases across all subgroups in their dataset by assembling a confusion matrix of each subgroup, calculating commonly used metrics such as false positive rate and false omission rate, as well as counts by group and group prevalance among the sample population.</p>
<p>Following the notebook, after constructing a <code>Group</code> object, you can use the <code>Group.get_crosstabs()</code> method to calculate a confusion matrix for each subgroup in your data. This method expects as input a <code>pandas.DataFrame</code> object formatted with the columns described above, and will infer the names and distinct values of your attribute columns defining the subgroups in your data. Calling <code>get_crosstabs</code> with your input data will return a <code>pandas.DataFrame</code> at the subgroup level with confusion matrix counts and ratios. The tutorial notebook walks through some exploration of this dataframe and interpreting biases across groups.</p>
<p>Absolute group bias metrics metrics from the crosstabs dataframe created by the <code>Group.get_crosstabs()</code> method can be visualized with two methods in the Aequitas <code>Plot()</code> class. After instantiating a <code>Plot</code> object, you can plot the results of a single metric by passing the crosstabs dataframe and metric name to <code>Plot.plot_group_metric()</code>, as well as optionally specifying a threshold below which to exclude small groups which may be particularly noisy. The tutorial notebook walks through an example of visualizing the false negative rates of groups using:</p>
<pre><code>    aqp = Plot()
    fnr = aqp.plot_group_metric(xtab, &#39;fnr&#39;, min_group_size=0.05)</code></pre>
<p>Additionally, you can visualize several metrics at the same time in small multiples using the <code>Plot.plot_group_metric_all()</code> method. Figure @ref(fig:tutorial_plot_crosstabs) shows an example of this method from notebook. In it, you can see that the largest <code>race</code> group, <code>African American</code>, is predicted positive more often than any other race group (predicted positive rate <span class="math inline">\(PPR\)</span> of 0.66), and are more likely to be incorrectly classified as ’high’ risk (false positive rate <span class="math inline">\(FPR\)</span> of 0.45) than incorrectly classified as ’low’ or ’medium’ risk (false negative rate <span class="math inline">\(FNR\)</span> of 0.28).</p>
<p><img src="ChapterBias/figures/tutorial_plot_crosstabs.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Data exploration screenshot from the Aequitas tutorial workbook, showing the predicted positive rate <span class="math inline">\(PPR\)</span>, predicted prevalence <span class="math inline">\(PPrev\)</span>, false negative rate <span class="math inline">\(FNR\)</span>, and false positive rate <span class="math inline">\(FPR\)</span> across subgroups in the COMPAS data.</p>
<p>To graph specific metrics of interest, you can pass a list to <code>plot_group_metric_all()</code> using the <code>metrics</code> keyword. Alternatively, you can pass <code>'all'</code> to visualize all calculated metrics or omit the keyword to plot the default metrics:</p>
<ul>
<li><p>Predicted Prevalence (<code>'pprev'</code>)</p></li>
<li><p>Predicted Positive Rate (<code>'ppr'</code>)</p></li>
<li><p>False Discovery Rate (<code>'fdr'</code>)</p></li>
<li><p>False Omission Rate (<code>'for'</code>)</p></li>
<li><p>False Positive Rate (<code>'fpr'</code>)</p></li>
<li><p>False Negative Rate (<code>'fnr'</code>)</p></li>
</ul>
<p>You can explore these options in more detail in the tutorial notebook.</p>
</div>
<div id="measuring-disparities" class="section level3">
<h3><span class="header-section-number">11.7.5</span> Measuring Disparities</h3>
<p>We use the Aequitas <code>Bias()</code> class to calculate disparities between groups based on the crosstabs returned by the <code>Group.get_crosstabs()</code> method described above. Disparities are calculated as a ratio of a metric for a group of interest compared to a base group. For example, the False Negative Rate Disparity for black defendants vis-a-vis whites is:</p>
<p><span class="math display">\[Disparity_{FNR} = \frac{FNR_{black}}{FNR_{white}}\]</span></p>
<p>Aequitas provides a couple of options for determining the reference group for each attribute’s disparity calculations: using <code>Bias.get_disparity_predefined_groups()</code> allows you to specify the reference groups directly, while <code>Bias.get_disparity_major_group()</code> will choose the largest group as the reference and <code>Bias.get_disparity_min_metric()</code> will use the group with the smallest value of the metric being calculated. Note in this last case that different reference groups may be used for different metrics.</p>
<p>In the tutorial notebook, you can walk through a couple examples of disparity calculations and how your choice of reference groups affects the results. Each of the <code>get_disparity_</code> methods will return a <code>pandas.DataFrame</code> containing the results of disparity calculations as well as (optionally) tests of statistical significance. For instance, the call below will calculate disparities relative to the specified groups and determine statistical significance at a level of <span class="math inline">\(\alpha = 0.05\)</span> (<code>mask_significance</code> means values of <code>True</code> or <code>False</code> will be returned rather than the p-value itself).</p>
<pre><code>    b = Bias()
    bdf = b.get_disparity_predefined_groups(
            xtab, 
            original_df=df, 
            ref_groups_dict={
                &#39;race&#39;:&#39;Caucasian&#39;, &#39;sex&#39;:&#39;Male&#39;, &#39;age_cat&#39;:&#39;25 - 45&#39;
            }, 
            alpha=0.05, 
            check_significance=True, 
            mask_significance=True
        )</code></pre>
<p>Notice that because disparities are calculated as ratios, the reference group will always have a disparity of 1.0. They can be interpreted as how much more prone the model is to making a certain type of mistake for one group relative to the reference group. For instance, calculating the <code>fpr_disparity</code> values by race with the COMPAS data indicates that black people are falsely identified as being high or medium risks 1.9 times the rate for white people, while the <code>fdr_disparity</code> values are much closer to 1.</p>
<p>The Aequitas <code>Plot()</code> class provides methods to visualize the results of your disparity calculations, with a similar interface to the methods described for plotting the absolute metrics described above. <code>Plot.plot_disparity()</code> allows for plotting a single specified disparity metric for a single attribute while <code>Plot.plot_disparity_all()</code> allows you to plot multiple disparities and attributes in small multiples at once.</p>
<p>Figure @ref(fig:tutorial_plot_disparity) shows an example of using the <code>plot_disparity()</code> method from the tutorial notebook. Each disparity is plotted as a treemap, with the size of the rectangle representing the size of the group and color indicating the level of disparity (with values over 1.0 in orange and those under 1.0 in blue). Notice in the figure that the reference group is colored gray and labeled <code>(Ref)</code>. Statistically significant disparities (at the level specified with <code>significance_alpha</code>) will be labeled with two asterisks (**), as seen for the <code>African-American</code> group in Figure @ref(fig:tutorial_plot_disparity).</p>
<p><img src="ChapterBias/figures/tutorial_plot_disparity.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Data exploration screenshot from the Aequitas tutorial workbook, showing racial disparities on the false positive rate <span class="math inline">\(FPR\)</span>. Note that the reference group, Hispanic, is indicated in gray and a statistically significant disparity for African-Americans is labeled with two asterisks (**).</p>
<p>The tutorial notebook walks through several additional examples of using <code>plot_disparity()</code> and <code>plot_disparity_all()</code>. When visualizing more than one disparity using <code>plot_disparity_all()</code>, you can specify a list of disparity metrics, <code>'all'</code> disaprity metrics, or use the Aequitas default disparity metrics by not supplying an argument:</p>
<ul>
<li><p>Predicted Positive Group Rate Disparity (<code>pprev_disparity</code>)</p></li>
<li><p>Predicted Positive Rate Disparity (<code>ppr_disparity</code>)</p></li>
<li><p>False Discovery Rate Disparity (<code>fdr_disparity</code>)</p></li>
<li><p>False Omission Rate Disparity (<code>for_disparity</code>)</p></li>
<li><p>False Positive Rate Disparity (<code>fpr_disparity</code>)</p></li>
<li><p>False Negative Rate Disparity (<code>fnr_disparity</code>)</p></li>
</ul>
</div>
<div id="assessing-model-fairness" class="section level3">
<h3><span class="header-section-number">11.7.6</span> Assessing Model Fairness</h3>
<p>Finally, the Aequitas <code>Fairness()</code> class provides three functions that provide a high level summary of fairness. This class builds on the dataframe returned from one of the three <code>Bias.get_dispariy_</code> methods. By specifying a threshold within which you would consider disparities to meet a reasonable level of fairness, this class allows you to evaluate at the group, attribute, and overall levels. For example, evaluating group-level FPR fairness with the default thresholds evaluates:</p>
<p><span class="math display">\[0.8 &lt; Disparity_{FPR} = \frac{FPR_{group}}{FPR_{base\_group}} &lt; 1.25\]</span></p>
<p>Calling <code>Fairness.get_group_value_fairness()</code> with your bias dataframe as an argument will return a <code>pandas.DataFrame</code> with boolean results indicating where your fairness criteria is met for each of the disparity metrics, as well as:</p>
<ul>
<li><p><strong>Type I Parity:</strong> Fairness in both FDR Parity and FPR Parity</p></li>
<li><p><strong>Type II Parity:</strong> Fairness in both FOR Parity and FNR Parity</p></li>
<li><p><strong>Equalized Odds:</strong> Fairness in both FPR Parity and TPR Parity</p></li>
<li><p><strong>Unsupervised Fairness:</strong> Fairness in both Statistical Parity and Impact Parity</p></li>
<li><p><strong>Supervised Fairness:</strong> Fairness in both Type I and Type II Parity</p></li>
<li><p><strong>Overall Fairness:</strong> Fairness across all parities for all attributes</p></li>
</ul>
<p>You can also assess whether each of these metrics meets your fairness threshold for all groups across each attribute at the same time using <code>Fairness.get_group_attribute_fairness()</code>. That is, this method will return a boolean at the level of each attribute (e.g., <code>race</code>, <code>sex</code>, or <code>age</code>) if the criteria is met for every subgroup defined by that attribute. For a further roll-up across all attributes as well, you can use <code>Fairness.get_overall_fairness()</code> to see a high-level assessment of unsupervised fairness, supervised fairness, and overall fairness. Below is a quick example of the usage for each of these methods, which you can explore further in the tutorial notebook:</p>
<pre><code>    f = Fairness()

    # group-level fairness:
    fdf = f.get_group_value_fairness(bdf)   # input is the result of a Bias.get_disparity_ method
    
    # attribute-level fairness:
    gaf = f.get_group_attribute_fairness(fdf)   # input is group-level fairness result from above
    
    # overall fairness:
    gof = f.get_overall_fairness(fdf)           # input is group-level fairness result from above</code></pre>
<p>In the tutorial notebook, you can calculate these metrics using the COMPAS data. There, the African-American false omission and false discovery are within the bounds of fairness (when assessed using Aequitas’s default thresholds), which should be expected because COMPAS is calibrated. On the other hand, African-Americans are roughly twice as likely to have false positives and 40 percent less likely to false negatives. In real terms, 44.8% of African-Americans who did not recidivate were marked high or medium risk (with potential for associated penalties), compared with 23.4% of Caucasian non-reoffenders. This result doesn’t meet the fairness threshold and thus returns <code>False</code> in the resulting dataframe. These findings mark an inherent trade-off between FPR Fairness, FNR Fairness and calibration, which is present in any decision system where base rates are not equal as discussed in Section <a href="chap-bias.html#sec:compascase">COMPAS case</a>. Aequitas helps bring this trade-off to the forefront with clear metrics and asks system designers to make a reasoned decision based on their use case.</p>
<p>The Aequitas <code>Plot()</code> class provides four methods to allow you to visualize the results of these various fairness calculations:</p>
<ul>
<li><p>Using <code>Plot.plot_fairness_group()</code>, you can plot a graph of a single fairness metric across different groups showing the absolute metric values. Example usage is shown in Figure @ref(fig:tutorial_plot_fairness1).</p></li>
<li><p><code>Plot.plot_fairness_group_all()</code> allows you to plot small multiples of several fairness metrics’ absolute values at the group level</p></li>
<li><p>With <code>Plot.plot_fairness_disparity()</code>, you can plot a treemap of the fairness results (similar to the disparity plot in Figure @ref(fig:tutorial_plot_disparity) showing the values of disparities relative to a base group and fairness results. Figure @ref(fig:tutorial_plot_fairness2) shows example usage of this method.</p></li>
<li><p><code>Plot.plot_fairness_disparity_all()</code> allows you to plot small multiples of several disparity treemaps across different groups and metrics.</p></li>
</ul>
<p><img src="ChapterBias/figures/tutorial_plot_fairness1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Data exploration screenshot from the Aequitas tutorial workbook, showing the fairness results for predicted positive rate <span class="math inline">\(PPR\)</span> across subgroups in the COMPAS data. Absolute values of <span class="math inline">\(PPR\)</span> are plotted for each group with bars colored by the results of applying fairness criteria to these disparities with groups meeting the criteria shown in green and those not meeting the criteria shown in red.</p>
<p><img src="ChapterBias/figures/tutorial_plot_fairness2.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Data exploration screenshot from the Aequitas tutorial workbook, showing a treemap representing <span class="math inline">\(FPR\)</span> disparities across racial groups in the COMPAS data. The size of each square represents the size of the group, with labels indicating the disparity values, and color indicating whether these values meet specified fairness thresholds (green for those meeting the criteria, red for those not meeting the criteria, and reference groups shown in gray.</p>
<p>The graphs generated by these methods will generally show similar information to the plots of absolute metric values and disparities described above, however the application of fairness criteria is overlayed on these plots to indicate whether a group meets the specified fairness criteria for a given metric (those meeting the threshold are shown in green and those failing to meet it are shown in red). The tutorial notebook will walk you through several examples of using each of these methods, and you should feel free to explore their usage further, including how fairness results change with the application of different thresholds.</p>

<!--
% - Rayid: "check Initially, assume the organization is on the outer frontier." ... what does this mean?
% - some references appear with n.d. in the compiled markdown
% - Julia: on Research Data Centers, add a line how the new envisioned ADRF would allow more replicability/reproducibility and access toothers but researchers? (industry, non-profit, government) for whom current RDCs are too hard to jump through hoops?
-->
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="71">
<li id="fn71"><p><a href="https://github.com/dssg/aequitas" class="uri">https://github.com/dssg/aequitas</a><a href="chap-bias.html#fnref71">↩</a></p></li>
<li id="fn72"><p><a href="https://twitter.com/datascifellows/status/994204100542783488" class="uri">https://twitter.com/datascifellows/status/994204100542783488</a><a href="chap-bias.html#fnref72">↩</a></p></li>
<li id="fn73"><p>See <a href="https://dssg.github.io/aequitas/" class="uri">https://dssg.github.io/aequitas/</a>.<a href="chap-bias.html#fnref73">↩</a></p></li>
</ol>
</div>
<div id="disqus_thread"></div>
<script>
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = '//big-data-and-social-science.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the
<a href="https://disqus.com/?ref_noscript">
  comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<a href="chap-errors.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chap-privacy.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/Coleridge-Initiative/big-data-and-social-science/edit/master/11-ChapterBiasFairness.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["big-data-and-social-science.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
