<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Big Data and Social Science</title>
  <meta name="description" content="Big Data and Social Science">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Big Data and Social Science" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="Coleridge-Initiative/big-data-and-social-science" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Big Data and Social Science" />
  
  
  

<meta name="author" content="Ian Foster">
<meta name="author" content="Rayid Ghani">
<meta name="author" content="Ron S. Jarmin">
<meta name="author" content="Frauke Kreuter">
<meta name="author" content="Julia Lane">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="chap-web.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Big Data and Social Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="chap-intro.html"><a href="chap-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-1"><i class="fa fa-check"></i><b>1.1</b> Why this book?</a></li>
<li class="chapter" data-level="1.2" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-2"><i class="fa fa-check"></i><b>1.2</b> Defining big data and its value</a></li>
<li class="chapter" data-level="1.3" data-path="chap-intro.html"><a href="chap-intro.html#sec:1.3"><i class="fa fa-check"></i><b>1.3</b> Social science, inference, and big data</a></li>
<li class="chapter" data-level="1.4" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-5"><i class="fa fa-check"></i><b>1.4</b> Social science, data quality, and big data</a></li>
<li class="chapter" data-level="1.5" data-path="chap-intro.html"><a href="chap-intro.html#new-tools-for-new-data"><i class="fa fa-check"></i><b>1.5</b> New tools for new data</a></li>
<li class="chapter" data-level="1.6" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-6"><i class="fa fa-check"></i><b>1.6</b> The book’s “use case”</a></li>
<li class="chapter" data-level="1.7" data-path="chap-intro.html"><a href="chap-intro.html#the-structure-of-the-book"><i class="fa fa-check"></i><b>1.7</b> The structure of the book</a><ul>
<li class="chapter" data-level="1.7.1" data-path="chap-intro.html"><a href="chap-intro.html#part-i-capture-and-curation"><i class="fa fa-check"></i><b>1.7.1</b> Part I: Capture and curation</a></li>
<li class="chapter" data-level="1.7.2" data-path="chap-intro.html"><a href="chap-intro.html#part-ii-modeling-and-analysis"><i class="fa fa-check"></i><b>1.7.2</b> Part II: Modeling and analysis</a></li>
<li class="chapter" data-level="1.7.3" data-path="chap-intro.html"><a href="chap-intro.html#part-iii-inference-and-ethics"><i class="fa fa-check"></i><b>1.7.3</b> Part III: Inference and ethics</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="chap-intro.html"><a href="chap-intro.html#sec:intro:resources"><i class="fa fa-check"></i><b>1.8</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chap-web.html"><a href="chap-web.html"><i class="fa fa-check"></i><b>2</b> Working with Web Data and APIs</a><ul>
<li class="chapter" data-level="2.1" data-path="chap-web.html"><a href="chap-web.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="chap-web.html"><a href="chap-web.html#sec:4-1"><i class="fa fa-check"></i><b>2.2</b> Scraping information from the web</a><ul>
<li class="chapter" data-level="2.2.1" data-path="chap-web.html"><a href="chap-web.html#sec:4-1.1"><i class="fa fa-check"></i><b>2.2.1</b> Obtaining data from websites</a></li>
<li class="chapter" data-level="2.2.2" data-path="chap-web.html"><a href="chap-web.html#sec:4-1.2"><i class="fa fa-check"></i><b>2.2.2</b> Limits of scraping</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="chap-web.html"><a href="chap-web.html#sec:4-3"><i class="fa fa-check"></i><b>2.3</b> Application Programming Interfaces (APIs)</a><ul>
<li class="chapter" data-level="2.3.1" data-path="chap-web.html"><a href="chap-web.html#sec:4-3.1"><i class="fa fa-check"></i><b>2.3.1</b> Relevant APIs and resources</a></li>
<li class="chapter" data-level="2.3.2" data-path="chap-web.html"><a href="chap-web.html#sec:4-3.2"><i class="fa fa-check"></i><b>2.3.2</b> RESTful APIs, returned data, and Python wrappers</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="chap-web.html"><a href="chap-web.html#sec:4-4"><i class="fa fa-check"></i><b>2.4</b> Using an API</a></li>
<li class="chapter" data-level="2.5" data-path="chap-web.html"><a href="chap-web.html#sec:4-4.1"><i class="fa fa-check"></i><b>2.5</b> Another example: Using the ORCID API via a wrapper</a></li>
<li class="chapter" data-level="2.6" data-path="chap-web.html"><a href="chap-web.html#sec:4-6"><i class="fa fa-check"></i><b>2.6</b> Integrating data from multiple sources</a></li>
<li class="chapter" data-level="2.7" data-path="chap-web.html"><a href="chap-web.html#sec:4-9"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="chap-web.html"><a href="chap-web.html#acknowledgements-and-copyright"><i class="fa fa-check"></i><b>2.8</b> Acknowledgements and copyright</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chap-link.html"><a href="chap-link.html"><i class="fa fa-check"></i><b>3</b> Record Linkage</a><ul>
<li class="chapter" data-level="3.1" data-path="chap-link.html"><a href="chap-link.html#motivation"><i class="fa fa-check"></i><b>3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2" data-path="chap-link.html"><a href="chap-link.html#sec:recordlinkage"><i class="fa fa-check"></i><b>3.2</b> Introduction to record linkage</a></li>
<li class="chapter" data-level="3.3" data-path="chap-link.html"><a href="chap-link.html#preprocessing-data-for-record-linkage"><i class="fa fa-check"></i><b>3.3</b> Preprocessing data for record linkage</a></li>
<li class="chapter" data-level="3.4" data-path="chap-link.html"><a href="chap-link.html#S:indexing"><i class="fa fa-check"></i><b>3.4</b> Indexing and blocking</a></li>
<li class="chapter" data-level="3.5" data-path="chap-link.html"><a href="chap-link.html#matching"><i class="fa fa-check"></i><b>3.5</b> Matching</a><ul>
<li class="chapter" data-level="3.5.1" data-path="chap-link.html"><a href="chap-link.html#rule-based-approaches"><i class="fa fa-check"></i><b>3.5.1</b> Rule-based approaches</a></li>
<li class="chapter" data-level="3.5.2" data-path="chap-link.html"><a href="chap-link.html#probabilistic-record-linkage"><i class="fa fa-check"></i><b>3.5.2</b> Probabilistic record linkage</a></li>
<li class="chapter" data-level="3.5.3" data-path="chap-link.html"><a href="chap-link.html#machine-learning-approaches-to-linking"><i class="fa fa-check"></i><b>3.5.3</b> Machine learning approaches to linking</a></li>
<li class="chapter" data-level="3.5.4" data-path="chap-link.html"><a href="chap-link.html#disambiguating-networks"><i class="fa fa-check"></i><b>3.5.4</b> Disambiguating networks</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="chap-link.html"><a href="chap-link.html#classification"><i class="fa fa-check"></i><b>3.6</b> Classification</a><ul>
<li class="chapter" data-level="3.6.1" data-path="chap-link.html"><a href="chap-link.html#S:thresholds"><i class="fa fa-check"></i><b>3.6.1</b> Thresholds</a></li>
<li class="chapter" data-level="3.6.2" data-path="chap-link.html"><a href="chap-link.html#one-to-one-links"><i class="fa fa-check"></i><b>3.6.2</b> One-to-one links</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="chap-link.html"><a href="chap-link.html#record-linkage-and-data-protection"><i class="fa fa-check"></i><b>3.7</b> Record linkage and data protection</a></li>
<li class="chapter" data-level="3.8" data-path="chap-link.html"><a href="chap-link.html#summary"><i class="fa fa-check"></i><b>3.8</b> Summary</a></li>
<li class="chapter" data-level="3.9" data-path="chap-link.html"><a href="chap-link.html#resources"><i class="fa fa-check"></i><b>3.9</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chap-db.html"><a href="chap-db.html"><i class="fa fa-check"></i><b>4</b> Databases</a><ul>
<li class="chapter" data-level="4.1" data-path="chap-db.html"><a href="chap-db.html#sec:db:intro"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="chap-db.html"><a href="chap-db.html#sec:db:when"><i class="fa fa-check"></i><b>4.2</b> DBMS: When and why</a></li>
<li class="chapter" data-level="4.3" data-path="chap-db.html"><a href="chap-db.html#relational-dbmss"><i class="fa fa-check"></i><b>4.3</b> Relational DBMSs</a><ul>
<li class="chapter" data-level="4.3.1" data-path="chap-db.html"><a href="chap-db.html#structured-query-language-sql"><i class="fa fa-check"></i><b>4.3.1</b> Structured Query Language (SQL)</a></li>
<li class="chapter" data-level="4.3.2" data-path="chap-db.html"><a href="chap-db.html#sec:db:sql"><i class="fa fa-check"></i><b>4.3.2</b> Manipulating and querying data</a></li>
<li class="chapter" data-level="4.3.3" data-path="chap-db.html"><a href="chap-db.html#sec:db:schema"><i class="fa fa-check"></i><b>4.3.3</b> Schema design and definition</a></li>
<li class="chapter" data-level="4.3.4" data-path="chap-db.html"><a href="chap-db.html#loading-data"><i class="fa fa-check"></i><b>4.3.4</b> Loading data</a></li>
<li class="chapter" data-level="4.3.5" data-path="chap-db.html"><a href="chap-db.html#transactions-and-crash-recovery"><i class="fa fa-check"></i><b>4.3.5</b> Transactions and crash recovery</a></li>
<li class="chapter" data-level="4.3.6" data-path="chap-db.html"><a href="chap-db.html#sec:db:index"><i class="fa fa-check"></i><b>4.3.6</b> Database optimizations</a></li>
<li class="chapter" data-level="4.3.7" data-path="chap-db.html"><a href="chap-db.html#caveats-and-challenges"><i class="fa fa-check"></i><b>4.3.7</b> Caveats and challenges</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="chap-db.html"><a href="chap-db.html#linking-dbmss-and-other-tools"><i class="fa fa-check"></i><b>4.4</b> Linking DBMSs and other tools</a></li>
<li class="chapter" data-level="4.5" data-path="chap-db.html"><a href="chap-db.html#sec:db:nosql"><i class="fa fa-check"></i><b>4.5</b> NoSQL databases</a><ul>
<li class="chapter" data-level="4.5.1" data-path="chap-db.html"><a href="chap-db.html#challenges-of-scale-the-cap-theorem"><i class="fa fa-check"></i><b>4.5.1</b> Challenges of scale: The CAP theorem</a></li>
<li class="chapter" data-level="4.5.2" data-path="chap-db.html"><a href="chap-db.html#nosql-and-keyvalue-stores"><i class="fa fa-check"></i><b>4.5.2</b> NoSQL and key–value stores</a></li>
<li class="chapter" data-level="4.5.3" data-path="chap-db.html"><a href="chap-db.html#other-nosql-databases"><i class="fa fa-check"></i><b>4.5.3</b> Other NoSQL databases</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="chap-db.html"><a href="chap-db.html#sec:db:spatial"><i class="fa fa-check"></i><b>4.6</b> Spatial databases</a></li>
<li class="chapter" data-level="4.7" data-path="chap-db.html"><a href="chap-db.html#which-database-to-use"><i class="fa fa-check"></i><b>4.7</b> Which database to use?</a><ul>
<li class="chapter" data-level="4.7.1" data-path="chap-db.html"><a href="chap-db.html#relational-dbmss-1"><i class="fa fa-check"></i><b>4.7.1</b> Relational DBMSs</a></li>
<li class="chapter" data-level="4.7.2" data-path="chap-db.html"><a href="chap-db.html#nosql-dbmss"><i class="fa fa-check"></i><b>4.7.2</b> NoSQL DBMSs</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="chap-db.html"><a href="chap-db.html#summary-1"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
<li class="chapter" data-level="4.9" data-path="chap-db.html"><a href="chap-db.html#resources-1"><i class="fa fa-check"></i><b>4.9</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chap-parallel.html"><a href="chap-parallel.html"><i class="fa fa-check"></i><b>5</b> Programming with Big Data</a><ul>
<li class="chapter" data-level="5.1" data-path="chap-parallel.html"><a href="chap-parallel.html#introduction-1"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="chap-parallel.html"><a href="chap-parallel.html#sec:intro"><i class="fa fa-check"></i><b>5.2</b> The MapReduce programming model</a></li>
<li class="chapter" data-level="5.3" data-path="chap-parallel.html"><a href="chap-parallel.html#apache-hadoop-mapreduce"><i class="fa fa-check"></i><b>5.3</b> Apache Hadoop MapReduce</a><ul>
<li class="chapter" data-level="5.3.1" data-path="chap-parallel.html"><a href="chap-parallel.html#the-hadoop-distributed-file-system"><i class="fa fa-check"></i><b>5.3.1</b> The Hadoop Distributed File System</a></li>
<li class="chapter" data-level="5.3.2" data-path="chap-parallel.html"><a href="chap-parallel.html#hadoop-bringing-compute-to-the-data"><i class="fa fa-check"></i><b>5.3.2</b> Hadoop: Bringing compute to the data</a></li>
<li class="chapter" data-level="5.3.3" data-path="chap-parallel.html"><a href="chap-parallel.html#hardware-provisioning"><i class="fa fa-check"></i><b>5.3.3</b> Hardware provisioning</a></li>
<li class="chapter" data-level="5.3.4" data-path="chap-parallel.html"><a href="chap-parallel.html#programming-language-support"><i class="fa fa-check"></i><b>5.3.4</b> Programming language support</a></li>
<li class="chapter" data-level="5.3.5" data-path="chap-parallel.html"><a href="chap-parallel.html#fault-tolerance"><i class="fa fa-check"></i><b>5.3.5</b> Fault tolerance</a></li>
<li class="chapter" data-level="5.3.6" data-path="chap-parallel.html"><a href="chap-parallel.html#limitations-of-hadoop"><i class="fa fa-check"></i><b>5.3.6</b> Limitations of Hadoop</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="chap-parallel.html"><a href="chap-parallel.html#apache-spark"><i class="fa fa-check"></i><b>5.4</b> Apache Spark</a></li>
<li class="chapter" data-level="5.5" data-path="chap-parallel.html"><a href="chap-parallel.html#summary-2"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
<li class="chapter" data-level="5.6" data-path="chap-parallel.html"><a href="chap-parallel.html#resources-2"><i class="fa fa-check"></i><b>5.6</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chap-ml.html"><a href="chap-ml.html"><i class="fa fa-check"></i><b>6</b> Machine Learning</a><ul>
<li class="chapter" data-level="6.1" data-path="chap-ml.html"><a href="chap-ml.html#introduction-2"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="chap-ml.html"><a href="chap-ml.html#what-is-machine-learning"><i class="fa fa-check"></i><b>6.2</b> What is machine learning?</a></li>
<li class="chapter" data-level="6.3" data-path="chap-ml.html"><a href="chap-ml.html#the-machine-learning-process"><i class="fa fa-check"></i><b>6.3</b> The machine learning process</a></li>
<li class="chapter" data-level="6.4" data-path="chap-ml.html"><a href="chap-ml.html#problem-formulation-mapping-a-problem-to-machine-learning-methods"><i class="fa fa-check"></i><b>6.4</b> Problem formulation: Mapping a problem to machine learning methods</a></li>
<li class="chapter" data-level="6.5" data-path="chap-ml.html"><a href="chap-ml.html#methods"><i class="fa fa-check"></i><b>6.5</b> Methods</a><ul>
<li class="chapter" data-level="6.5.1" data-path="chap-ml.html"><a href="chap-ml.html#unsupervised-learning-methods"><i class="fa fa-check"></i><b>6.5.1</b> Unsupervised learning methods</a></li>
<li class="chapter" data-level="6.5.2" data-path="chap-ml.html"><a href="chap-ml.html#sec:MLchapter:super"><i class="fa fa-check"></i><b>6.5.2</b> Supervised learning</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="chap-ml.html"><a href="chap-ml.html#evaluation"><i class="fa fa-check"></i><b>6.6</b> Evaluation</a><ul>
<li class="chapter" data-level="6.6.1" data-path="chap-ml.html"><a href="chap-ml.html#methodology"><i class="fa fa-check"></i><b>6.6.1</b> Methodology</a></li>
<li class="chapter" data-level="6.6.2" data-path="chap-ml.html"><a href="chap-ml.html#metrics"><i class="fa fa-check"></i><b>6.6.2</b> Metrics</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="chap-ml.html"><a href="chap-ml.html#practical-tips"><i class="fa fa-check"></i><b>6.7</b> Practical tips</a><ul>
<li class="chapter" data-level="6.7.1" data-path="chap-ml.html"><a href="chap-ml.html#features"><i class="fa fa-check"></i><b>6.7.1</b> Features</a></li>
<li class="chapter" data-level="6.7.2" data-path="chap-ml.html"><a href="chap-ml.html#machine-learning-pipeline"><i class="fa fa-check"></i><b>6.7.2</b> Machine learning pipeline</a></li>
<li class="chapter" data-level="6.7.3" data-path="chap-ml.html"><a href="chap-ml.html#multiclass-problems"><i class="fa fa-check"></i><b>6.7.3</b> Multiclass problems</a></li>
<li class="chapter" data-level="6.7.4" data-path="chap-ml.html"><a href="chap-ml.html#skewed-or-imbalanced-classification-problems"><i class="fa fa-check"></i><b>6.7.4</b> Skewed or imbalanced classification problems</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="chap-ml.html"><a href="chap-ml.html#how-can-social-scientists-benefit-from-machine-learning"><i class="fa fa-check"></i><b>6.8</b> How can social scientists benefit from machine learning?</a></li>
<li class="chapter" data-level="6.9" data-path="chap-ml.html"><a href="chap-ml.html#advanced-topics"><i class="fa fa-check"></i><b>6.9</b> Advanced topics</a></li>
<li class="chapter" data-level="6.10" data-path="chap-ml.html"><a href="chap-ml.html#summary-3"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
<li class="chapter" data-level="6.11" data-path="chap-ml.html"><a href="chap-ml.html#ml:res"><i class="fa fa-check"></i><b>6.11</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chap-text.html"><a href="chap-text.html"><i class="fa fa-check"></i><b>7</b> Text Analysis</a><ul>
<li class="chapter" data-level="7.1" data-path="chap-text.html"><a href="chap-text.html#understanding-what-people-write"><i class="fa fa-check"></i><b>7.1</b> Understanding what people write</a></li>
<li class="chapter" data-level="7.2" data-path="chap-text.html"><a href="chap-text.html#how-to-analyze-text"><i class="fa fa-check"></i><b>7.2</b> How to analyze text</a><ul>
<li class="chapter" data-level="7.2.1" data-path="chap-text.html"><a href="chap-text.html#processing-text-data"><i class="fa fa-check"></i><b>7.2.1</b> Processing text data</a></li>
<li class="chapter" data-level="7.2.2" data-path="chap-text.html"><a href="chap-text.html#how-much-is-a-word-worth"><i class="fa fa-check"></i><b>7.2.2</b> How much is a word worth?</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="chap-text.html"><a href="chap-text.html#sec:appapp"><i class="fa fa-check"></i><b>7.3</b> Approaches and applications</a><ul>
<li class="chapter" data-level="7.3.1" data-path="chap-text.html"><a href="chap-text.html#sec:lda"><i class="fa fa-check"></i><b>7.3.1</b> Topic modeling</a></li>
<li class="chapter" data-level="7.3.2" data-path="chap-text.html"><a href="chap-text.html#sec:ir"><i class="fa fa-check"></i><b>7.3.2</b> Information retrieval and clustering</a></li>
<li class="chapter" data-level="7.3.3" data-path="chap-text.html"><a href="chap-text.html#sec:other"><i class="fa fa-check"></i><b>7.3.3</b> Other approaches</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="chap-text.html"><a href="chap-text.html#sec:eval"><i class="fa fa-check"></i><b>7.4</b> Evaluation</a></li>
<li class="chapter" data-level="7.5" data-path="chap-text.html"><a href="chap-text.html#text-analysis-tools"><i class="fa fa-check"></i><b>7.5</b> Text analysis tools</a></li>
<li class="chapter" data-level="7.6" data-path="chap-text.html"><a href="chap-text.html#summary-4"><i class="fa fa-check"></i><b>7.6</b> Summary</a></li>
<li class="chapter" data-level="7.7" data-path="chap-text.html"><a href="chap-text.html#resources-3"><i class="fa fa-check"></i><b>7.7</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chap-networks.html"><a href="chap-networks.html"><i class="fa fa-check"></i><b>8</b> Networks: The Basics</a><ul>
<li class="chapter" data-level="8.1" data-path="chap-networks.html"><a href="chap-networks.html#introduction-3"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="chap-networks.html"><a href="chap-networks.html#network-data"><i class="fa fa-check"></i><b>8.2</b> Network data</a><ul>
<li class="chapter" data-level="8.2.1" data-path="chap-networks.html"><a href="chap-networks.html#forms-of-network-data"><i class="fa fa-check"></i><b>8.2.1</b> Forms of network data</a></li>
<li class="chapter" data-level="8.2.2" data-path="chap-networks.html"><a href="chap-networks.html#inducing-one-mode-networks-from-two-mode-data"><i class="fa fa-check"></i><b>8.2.2</b> Inducing one-mode networks from two-mode data</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="chap-networks.html"><a href="chap-networks.html#network-measures"><i class="fa fa-check"></i><b>8.3</b> Network measures</a><ul>
<li class="chapter" data-level="8.3.1" data-path="chap-networks.html"><a href="chap-networks.html#reachability"><i class="fa fa-check"></i><b>8.3.1</b> Reachability</a></li>
<li class="chapter" data-level="8.3.2" data-path="chap-networks.html"><a href="chap-networks.html#whole-network-measures"><i class="fa fa-check"></i><b>8.3.2</b> Whole-network measures</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="chap-networks.html"><a href="chap-networks.html#comparing-collaboration-networks"><i class="fa fa-check"></i><b>8.4</b> Comparing collaboration networks</a></li>
<li class="chapter" data-level="8.5" data-path="chap-networks.html"><a href="chap-networks.html#summary-5"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="chap-networks.html"><a href="chap-networks.html#resources-4"><i class="fa fa-check"></i><b>8.6</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chap-viz.html"><a href="chap-viz.html"><i class="fa fa-check"></i><b>9</b> Information Visualization</a><ul>
<li class="chapter" data-level="9.1" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-1"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2"><i class="fa fa-check"></i><b>9.2</b> Developing effective visualizations</a></li>
<li class="chapter" data-level="9.3" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-3"><i class="fa fa-check"></i><b>9.3</b> A data-by-tasks taxonomy</a><ul>
<li class="chapter" data-level="9.3.1" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.1"><i class="fa fa-check"></i><b>9.3.1</b> Multivariate data</a></li>
<li class="chapter" data-level="9.3.2" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.2"><i class="fa fa-check"></i><b>9.3.2</b> Spatial data</a></li>
<li class="chapter" data-level="9.3.3" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.4"><i class="fa fa-check"></i><b>9.3.3</b> Temporal data</a></li>
<li class="chapter" data-level="9.3.4" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.5"><i class="fa fa-check"></i><b>9.3.4</b> Hierarchical data</a></li>
<li class="chapter" data-level="9.3.5" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.6"><i class="fa fa-check"></i><b>9.3.5</b> Network data</a></li>
<li class="chapter" data-level="9.3.6" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.7"><i class="fa fa-check"></i><b>9.3.6</b> Text data</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4"><i class="fa fa-check"></i><b>9.4</b> Challenges</a><ul>
<li class="chapter" data-level="9.4.1" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4.1"><i class="fa fa-check"></i><b>9.4.1</b> Scalability</a></li>
<li class="chapter" data-level="9.4.2" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4.2"><i class="fa fa-check"></i><b>9.4.2</b> Evaluation</a></li>
<li class="chapter" data-level="9.4.3" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4.3"><i class="fa fa-check"></i><b>9.4.3</b> Visual impairment</a></li>
<li class="chapter" data-level="9.4.4" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4.4"><i class="fa fa-check"></i><b>9.4.4</b> Visual literacy</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-5"><i class="fa fa-check"></i><b>9.5</b> Summary</a></li>
<li class="chapter" data-level="9.6" data-path="chap-viz.html"><a href="chap-viz.html#sec:mylabel4"><i class="fa fa-check"></i><b>9.6</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chap-errors.html"><a href="chap-errors.html"><i class="fa fa-check"></i><b>10</b> Errors and Inference</a><ul>
<li class="chapter" data-level="10.1" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-1"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-2"><i class="fa fa-check"></i><b>10.2</b> The total error paradigm</a><ul>
<li class="chapter" data-level="10.2.1" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-2.1"><i class="fa fa-check"></i><b>10.2.1</b> The traditional model</a></li>
<li class="chapter" data-level="10.2.2" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-2.2"><i class="fa fa-check"></i><b>10.2.2</b> Extending the framework to big data</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-3"><i class="fa fa-check"></i><b>10.3</b> Illustrations of errors in big data</a></li>
<li class="chapter" data-level="10.4" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-4"><i class="fa fa-check"></i><b>10.4</b> Errors in big data analytics</a><ul>
<li class="chapter" data-level="10.4.1" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-4.1"><i class="fa fa-check"></i><b>10.4.1</b> Errors resulting from volume, velocity, and variety, assuming perfect veracity</a></li>
<li class="chapter" data-level="10.4.2" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-4.2"><i class="fa fa-check"></i><b>10.4.2</b> Errors resulting from lack of veracity</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-5"><i class="fa fa-check"></i><b>10.5</b> Some methods for mitigating, detecting, and compensating for errors</a></li>
<li class="chapter" data-level="10.6" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-6"><i class="fa fa-check"></i><b>10.6</b> Summary</a></li>
<li class="chapter" data-level="10.7" data-path="chap-errors.html"><a href="chap-errors.html#resources-5"><i class="fa fa-check"></i><b>10.7</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chap-privacy.html"><a href="chap-privacy.html"><i class="fa fa-check"></i><b>11</b> Privacy and Confidentiality</a><ul>
<li class="chapter" data-level="11.1" data-path="chap-privacy.html"><a href="chap-privacy.html#introduction-4"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="chap-privacy.html"><a href="chap-privacy.html#why-is-access-important"><i class="fa fa-check"></i><b>11.2</b> Why is access important?</a></li>
<li class="chapter" data-level="11.3" data-path="chap-privacy.html"><a href="chap-privacy.html#providing-access"><i class="fa fa-check"></i><b>11.3</b> Providing access</a></li>
<li class="chapter" data-level="11.4" data-path="chap-privacy.html"><a href="chap-privacy.html#the-new-challenges"><i class="fa fa-check"></i><b>11.4</b> The new challenges</a></li>
<li class="chapter" data-level="11.5" data-path="chap-privacy.html"><a href="chap-privacy.html#legal-and-ethical-framework"><i class="fa fa-check"></i><b>11.5</b> Legal and ethical framework</a></li>
<li class="chapter" data-level="11.6" data-path="chap-privacy.html"><a href="chap-privacy.html#summary-6"><i class="fa fa-check"></i><b>11.6</b> Summary</a></li>
<li class="chapter" data-level="11.7" data-path="chap-privacy.html"><a href="chap-privacy.html#resources-6"><i class="fa fa-check"></i><b>11.7</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chap-workbooks.html"><a href="chap-workbooks.html"><i class="fa fa-check"></i><b>12</b> Workbooks</a><ul>
<li class="chapter" data-level="12.1" data-path="chap-workbooks.html"><a href="chap-workbooks.html#introduction-5"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="chap-workbooks.html"><a href="chap-workbooks.html#environment"><i class="fa fa-check"></i><b>12.2</b> Environment</a><ul>
<li class="chapter" data-level="12.2.1" data-path="chap-workbooks.html"><a href="chap-workbooks.html#running-workbooks-locally"><i class="fa fa-check"></i><b>12.2.1</b> Running workbooks locally</a></li>
<li class="chapter" data-level="12.2.2" data-path="chap-workbooks.html"><a href="chap-workbooks.html#central-workbook-server"><i class="fa fa-check"></i><b>12.2.2</b> Central workbook server</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="chap-workbooks.html"><a href="chap-workbooks.html#workbook-details"><i class="fa fa-check"></i><b>12.3</b> Workbook details</a><ul>
<li class="chapter" data-level="12.3.1" data-path="chap-workbooks.html"><a href="chap-workbooks.html#social-media-and-apis"><i class="fa fa-check"></i><b>12.3.1</b> Social Media and APIs</a></li>
<li class="chapter" data-level="12.3.2" data-path="chap-workbooks.html"><a href="chap-workbooks.html#database-basics"><i class="fa fa-check"></i><b>12.3.2</b> Database basics</a></li>
<li class="chapter" data-level="12.3.3" data-path="chap-workbooks.html"><a href="chap-workbooks.html#data-linkage"><i class="fa fa-check"></i><b>12.3.3</b> Data Linkage</a></li>
<li class="chapter" data-level="12.3.4" data-path="chap-workbooks.html"><a href="chap-workbooks.html#machine-learning"><i class="fa fa-check"></i><b>12.3.4</b> Machine Learning</a></li>
<li class="chapter" data-level="12.3.5" data-path="chap-workbooks.html"><a href="chap-workbooks.html#text-analysis"><i class="fa fa-check"></i><b>12.3.5</b> Text Analysis</a></li>
<li class="chapter" data-level="12.3.6" data-path="chap-workbooks.html"><a href="chap-workbooks.html#networks"><i class="fa fa-check"></i><b>12.3.6</b> Networks</a></li>
<li class="chapter" data-level="12.3.7" data-path="chap-workbooks.html"><a href="chap-workbooks.html#visualization"><i class="fa fa-check"></i><b>12.3.7</b> Visualization</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="chap-workbooks.html"><a href="chap-workbooks.html#resources-7"><i class="fa fa-check"></i><b>12.4</b> Resources</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Big Data and Social Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chap:intro" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Introduction</h1>
<p>This section provides a brief overview of the goals and structure of the
book.</p>
<div id="sec:1-1" class="section level2">
<h2><span class="header-section-number">1.1</span> Why this book?</h2>
<p>The world has changed for empirical social scientists. The new types of
“big data” have generated an entire new research field—that of data
science. That world is dominated by computer scientists who have
generated new ways of creating and collecting data, developed new
analytical and statistical techniques, and provided new ways of
visualizing and presenting information. These new sources of data and
techniques have the potential to transform the way applied social
science is done.</p>
<p>Research has certainly changed. Researchers draw on data that are
“found” rather than “made” by federal agencies; those publishing in
leading academic journals are much less likely today to draw on
preprocessed survey data (Figure <a href="chap-intro.html#fig:fig1">1.1</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:fig1"></span>
<img src="ChapterIntro/figures/Figure1.png" alt="Use of pre-existing survey data in publications in leading journals, 1980--2010 (@Chetty2012)" width="70%" />
<p class="caption">
Figure 1.1: Use of pre-existing survey data in publications in leading journals, 1980–2010 (<span class="citation">Chetty (<a href="#ref-Chetty2012">2012</a>)</span>)
</p>
</div>
<p>The way in which data are used has also changed for both government
agencies and businesses. Chief data officers are becoming as common in
federal and state governments as chief economists were decades ago, and
in cities like New York and Chicago, mayoral offices of data analytics
have the ability to provide rapid answers to important policy questions
<span class="citation">(Lee et al. <a href="#ref-lee2012rise">2012</a>)</span>. But since federal, state, and local agencies lack the
capacity to do such analysis themselves <span class="citation">(Alawadhi et al. <a href="#ref-alawadhi2012building">2012</a>)</span>, they
must make these data available either to consultants or to the research
community. Businesses are also learning that making effective use of
their data assets can have an impact on their bottom line
<span class="citation">(Brynjolfsson, Hitt, and Kim <a href="#ref-brynjolfsson2011strength">2011</a>)</span>.</p>
<p>And the jobs have changed. The new job title of “data scientist” is
highlighted in job advertisements on CareerBuilder.com and
Burning-glass.com—in the same category as statisticians, economists,
and other quantitative social scientists if starting salaries are useful
indicators.</p>
<p>The goal of this book is to provide social scientists with an
understanding of the key elements of this new science, its value, and
the opportunities for doing better work. The goal is also to identify
the many ways in which the analytical toolkits possessed by social
scientists can be brought to bear to enhance the generalizability of the
work done by computer scientists.</p>
<p>We take a pragmatic approach, drawing on our experience of working with
data. Most social scientists set out to solve a real-world social or
economic problem: they frame the problem, identify the data, do the
analysis, and then draw inferences. At all points, of course, the social
scientist needs to consider the ethical ramifications of their work,
particularly respecting privacy and confidentiality. The book follows
the same structure. We chose a particular problem—the link between
research investments and innovation—because that is a major social
science policy issue, and one in which social scientists have been
addressing using big data techniques. While the example is specific and
intended to show how abstract concepts apply in practice, the approach
is completely generalizable. The web scraping, linkage, classification,
and text analysis methods on display here are canonical in nature. The
inference and privacy and confidentiality issues are no different than
in any other study involving human subjects, and the communication of
results through visualization is similarly generalizable.</p>
</div>
<div id="sec:1-2" class="section level2">
<h2><span class="header-section-number">1.2</span> Defining big data and its value</h2>
<p>There are almost as many definitions of big data as there are new types
of data. One approach is to define big data as <em>anything too big to fit onto your computer</em><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. Another approach is to define it as data with high
volume, high velocity, and great variety. We choose the description adopted
by the American Association of Public Opinion Research: “The term ‘Big Data’
is an imprecise description of a rich and complicated set of characteristics,
practices, techniques, ethical issues, and outcomes all associated with data”
<span class="citation">(Japec et al. <a href="#ref-japec2015big">2015</a>)</span>.</p>
<p>The value of the new types of data for social science is quite
substantial. Personal data has been hailed as the “new oil” of the
twenty-first century, and the benefits to policy, society, and public
opinion research are undeniable <span class="citation">(Greenwood et al. <a href="#ref-greenwood2014">2014</a>)</span>. Policymakers have
found that detailed data on human beings can be used to reduce crime,
improve health delivery, and manage cities better <span class="citation">(Keller, Koonin, and Shipp <a href="#ref-keller2012big">2012</a>)</span>. The
scope is broad indeed: one of this book’s editors has used such data to
not only help win political campaigns but also show its potential for
public policy. Society can gain as well—recent work shows data-driven
businesses were 5% more productive and 6% more profitable than their
competitors<span class="citation">(Brynjolfsson, Hitt, and Kim <a href="#ref-brynjolfsson2011strength">2011</a>)</span>. In short, the vision is that
social science researchers can potentially, by using data with high
velocity, variety, and volume, increase the scope of their data
collection efforts while at the same time reducing costs and respondent
burden, increasing timeliness, and increasing
precision<span class="citation">(Murphy et al. <a href="#ref-murphy2014social">2014</a>)</span>.</p>
<hr />
<p><strong>Example: New data enable new analyses</strong></p>
<p>Spotshotter data, which have fairly detailed information for each
gunfire incident, such as the precise timestamp and the nearest address,
as well as the type of shot, can be used to improve crime data
<span class="citation">(Carr and Doleac <a href="#ref-carr2015geography">2015</a>)</span>; Twitter data can be used to improve predictions
around job loss, job gain, and job postings <span class="citation">(Antenucci et al. <a href="#ref-antenucci2014using">2014</a>)</span>; and
eBay postings can be used to estimate demand elasticities
<span class="citation">(Einav and Levin <a href="#ref-einav2013data">2013</a>)</span>.</p>
<hr />
<p>But most interestingly, the new data can change the way we think about
measuring and making inferences about behavior. For example, it enables
the capture of information on the subject’s entire environment—thus,
for example, the effect of fast food caloric labeling in health
interventions <span class="citation">(Elbel, Gyamfi, and Kersh <a href="#ref-Elbel2011">2011</a>)</span>; the productivity of a cashier if he is
within eyesight of a highly productive cashier but not otherwise
<span class="citation">(Mas and Moretti <a href="#ref-Mas2009">2009</a>)</span>. So it offers the potential to understand the effects of
complex environmental inputs on human behavior. In addition, big data,
by its very nature, enables us to study the tails of a distribution in a
way that is not possible with small data. Much of interest in human
behavior is driven by the tails of the distribution—health care costs
by small numbers of ill people <span class="citation">(Stanton and Rutherford <a href="#ref-stanton2006high">2006</a>)</span>, economic activity and
employment by a small number of firms
<span class="citation">(Evans <a href="#ref-evans1987tests">1987</a>; Decker et al., <a href="#ref-decker2015has">n.d.</a>)</span>—and is impossible to study with the
small sample sizes available to researchers.</p>
<p>Instead we are still faced with the same challenges and responsibilities
as we were before in the survey and small data collection environment.
Indeed, social scientists have a great deal to offer to a (data) world
that is currently looking to computer scientists to provide answers. Two
major areas to which social scientists can contribute, based on decades
of experience and work with end users, are inference and attention to
data quality.</p>
</div>
<div id="sec:1.3" class="section level2">
<h2><span class="header-section-number">1.3</span> Social science, inference, and big data</h2>
<p>The goal of empirical social science is to make inferences about a
population from available data. That requirement exists regardless of
the data source—and is a guiding principle for this book. For
probability-based survey data, methodology has been developed to
overcome problems in the data generating process. A guiding principle
for survey methodologists is the total survey error framework, and
statistical methods for weighting, calibration, and other forms of
adjustment are commonly used to mitigate errors in the survey process.
Likewise for “broken” experimental data, techniques like propensity
score adjustment and principal stratification are widely used to fix
flaws in the data generating process. Two books provide frameworks for
<em>survey quality</em><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a><span class="citation">(Groves <a href="#ref-groves2004survey">2004</a>; Biemer and Lyberg <a href="#ref-biemer2003">2003</a>)</span>.</p>
<p>Across the social sciences, including economics, public policy,
sociology, management, (parts of) psychology and the like, we can
identify three categories of analysis with three different inferential
goals: description, causation, and prediction.</p>
<p><strong>Description</strong></p>
<p>The job of many social scientists is to provide descriptive statements
about the population of interest. These could be univariate, bivariate,
or even multivariate statements. <a href="chap-ml.html#chap:ml">Machine Learning</a> on machine learning will cover methods that go beyond simple descriptive statistics, known as <em>unsupervised learning</em>
methods.</p>
<p>Descriptive statistics are usually created based on census data or
sample surveys to generate some summary statistics like a mean, median,
or a graphical distribution to describe the population of interest. In
the case of a census, the work ends right there. With sample surveys the
point estimates come with measures of uncertainties (standard errors).
The estimation of standard errors has been worked out for most
descriptive statistics and most common survey designs, even complex ones
that include multiple layers of sampling and disproportional selection
probabilities <span class="citation">(Hansen, Hurwitz, and Madow <a href="#ref-hansen1993sample">1993</a>; Valliant, Dever, and Kreuter <a href="#ref-valliant2013practical">2013</a>)</span>.</p>
<hr />
<p><strong>Example: Descriptive statistics</strong></p>
<p>The US Bureau of Labor Statistics surveys about 60,000 households a
month and from that survey is able to describe national employment and
unemployment levels. For example, in November 2015, total nonfarm
payroll employment increased by 211,000 in November, and the
unemployment rate was unchanged at 5.0%. Job gains occurred in
construction, professional and technical services, and health care.
Mining and information lost jobs <span class="citation">(Bureau of Labor Statistics, <a href="#ref-BLS2015">n.d.</a>)</span>.</p>
<hr />
<p>Proper inference, even for purely descriptive purposes, from a sample to
the population rests usually on knowing that everyone from the target
population had the chance to be included in the survey, and knowing the
selection probability for each element in the population. The latter
does not necessarily need to be known prior to sampling, but eventually
a probability is assigned for each case. Getting the selection
probabilities right is particularly important when reporting totals
<span class="citation">(Lohr <a href="#ref-lohr2009sampling">2009</a>)</span>. Unfortunately in practice, samples that start out
as probability samples can suffer from a high rate of nonresponse.
Because the survey designer cannot completely control which units
respond, the set of units that ultimately respond cannot be considered
to be a probability sample <span class="citation">(Meyer, Mok, and Sullivan <a href="#ref-Meyer2015">2015</a>)</span>. Nevertheless, starting with a
probability sample provides some degree of comfort that a sample will
have limited coverage errors (nonzero probability of being in the
sample), and there are methods for dealing with a variety of missing
data problems <span class="citation">(Little and Rubin <a href="#ref-little2014statistical">2014</a>)</span>.</p>
<p><strong>Causation</strong></p>
<p>In many cases, social scientists wish to test hypotheses, often
originating in theory, about relationships between phenomena of
interest. Ideally such tests stem from data that allow causal inference:
typically randomized experiments or strong nonexperimental study
designs. When examining the effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>, knowing how cases were
selected into the sample or data set is much less important in the
estimation of causal effects than for descriptive studies, for example,
population means. What is important is that all elements of the
inferential population have a chance of being selected for the treatment
<span class="citation">(Imbens and Rubin <a href="#ref-imbens2015causal">2015</a>)</span>. In the debate about probability and nonprobability
surveys, this distinction is often overlooked. Medical researchers have
operated with unknown study selection mechanisms for years: for example,
randomized trials that enroll only selected samples.</p>
<hr />
<p><strong>Example: New data and causal inference</strong></p>
<p>One of the major risks with using big data without thinking about the
data source is the misallocation of resources. Overreliance on, say,
Twitter data in targeting resources after hurricanes can lead to the
misallocation of resources towards young, Internet-savvy people with
cell phones, and away from elderly or impoverished neighborhoods
<span class="citation">(Shelton et al. <a href="#ref-shelton2014mapping">2014</a>)</span>. Of course, all data collection approaches have
had similar risks. Bad survey methodology led the <em>Literary Digest</em> to
incorrectly call the 1936 election <span class="citation">(Squire <a href="#ref-squire19881936">1988</a>)</span>. Inadequate
understanding of coverage, incentive and quality issues, together with
the lack of a comparison group, has hampered the use of administrative
records—famously in the case of using administrative records on crime
to make inference about the role of death penalty policy in crime
reduction <span class="citation">(Donohue III and Wolfers <a href="#ref-donohue2006uses">2006</a>)</span>.</p>
<hr />
<p>Of course, in practice it is difficult to ensure that results are
generalizable, and there is always a concern that the treatment effect
on the treated is different than the treatment effect in the full
population of interest <span class="citation">(Stuart <a href="#ref-stuart2010matching">2010</a>)</span>. Having unknown study
selection probabilities makes it even more difficult to estimate
population causal effects, but substantial progress is being made
<span class="citation">(DuGoff, Schuler, and Stuart <a href="#ref-dugoff2014generalizing">2014</a>; Morgan and Winship <a href="#ref-morgan2014counterfactuals">2014</a>)</span>. As long as we are
able to model the selection process, there is no reason not to do causal
inference from so-called nonprobability data.</p>
<p><strong>Prediction</strong></p>
<p>Forecasting or prediction tasks are a little less common among applied
social science researchers as a whole, but are certainly an important
element for users of official statistics—in particular, in the context
of social and economic indicators—as generally for decision-makers in
government and business. Here, similar to the causal inference setting,
it is of utmost importance that we do know the process that generated
the data, and we can rule out any unknown or unobserved systematic
selection mechanism.</p>
<hr />
<p><strong>Example: Learning from the flu</strong></p>
<p>&quot;Five years ago in 2009, a team of researchers from Google announced
a remarkable achievement in one of the world’s top scientific journals,
<em>Nature</em>. Without needing the results of a single medical check-up, they
were nevertheless able to track the spread of influenza across the US.
What’s more, they could do it more quickly than the Centers for Disease
Control and Prevention (CDC). Google’s tracking had only a day’s delay,
compared with the week or more it took for the CDC to assemble a picture
based on reports from doctors’ surgeries. Google was faster because it
was tracking the outbreak by finding a correlation between what people
searched for online and whether they had flu symptoms. …</p>
<p>&quot;Four years after the original <em>Nature</em> paper was published, <em>Nature
News</em> had sad tidings to convey: the latest flu outbreak had claimed an
unexpected victim: Google Flu Trends. After reliably providing a swift
and accurate account of flu outbreaks for several winters, the
theory-free, data-rich model had lost its nose for where flu was going.
Google’s model pointed to a severe outbreak but when the slow-and-steady
data from the CDC arrived, they showed that Google’s estimates of the
spread of flu-like illnesses were overstated by almost a factor of two.</p>
<p>“The problem was that Google did not know—could not begin to
know—what linked the search terms with the spread of flu. Google’s
engineers weren’t trying to figure out what caused what. They were
merely finding statistical patterns in the data. They cared about
correlation rather than causation” <span class="citation">(Harford <a href="#ref-harford2014big">2014</a>)</span>.</p>
<hr />
</div>
<div id="sec:1-5" class="section level2">
<h2><span class="header-section-number">1.4</span> Social science, data quality, and big data</h2>
<p>Most data in the real world are noisy, inconsistent, and suffers from
missing values, regardless of its source. Even if data collection is
cheap, the costs of creating high-quality data from the source – <em>cleaning, curating, standardizing, and integrating</em><a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> – are substantial.</p>
<p>Data quality can be characterized in multiple ways <span class="citation">(Christen <a href="#ref-christen2012data">2012</a><a href="#ref-christen2012data">b</a>)</span>:</p>
<ul>
<li><p><strong>Accuracy</strong>: How accurate are the attribute values in the data?</p></li>
<li><p><strong>Completeness</strong>: Is the data complete?</p></li>
<li><p><strong>Consistency</strong>: How consistent are the values in and between the database(s)?</p></li>
<li><p><strong>Timeliness</strong>: How timely is the data?</p></li>
<li><p><strong>Accessibility</strong>: Are all variables available for analysis?</p></li>
</ul>
<p>Social scientists have decades of experience in transformingmessy,
noisy, and unstructured data into a well-defined, clearly structured,
and quality-tested data set. Preprocessing is a complex and
time-consuming process because it is “hands-on”—it requires judgment
and cannot be effectively automated. A typical workflow comprises
multiple steps from data definition to parsing and ends with filtering.
It is difficult to overstate the value of preprocessing for any data
analysis, but this is particularly true in big data. Data need to be
parsed, standardized, deduplicated, and normalized.</p>
<p><strong>Parsing</strong> is a fundamental step taken regardless of the data source, and refers to
the decomposition of a complex variable into components. For example, a
freeform address field like “1234 E 56th St” might be broken down into a
street number “1234” and a street name “E 56th St.” The street name
could be broken down further to extract the cardinal direction “E” and
the designation “St.” Another example would be a combined full name
field that takes the form of a comma-separated last name, first name,
and middle initial as in “Miller, David A.” Splitting these identifiers
into components permits the creation of more refined variables that can
be used in the matching step.</p>
<p>In the simplest case, the distinct parts of a character field are
delimited. In the name field example, it would be easy to create the
separate fields “Miller” and “David A” by splitting the original field
at the comma. In more complex cases, special code will have to be
written to parse the field. Typical steps in a parsing procedure
include:</p>
<ol style="list-style-type: decimal">
<li><p>Splitting fields into tokens (words) on the basis of delimiters,</p></li>
<li><p>Standardizing tokens by lookup tables and substitution by a standard
form,</p></li>
<li><p>Categorizing tokens,</p></li>
<li><p>Identifying a pattern of anchors, tokens, and delimiters,</p></li>
<li><p>Calling subroutines according to the identified pattern, therein
mapping of tokens to the predefined components.</p></li>
</ol>
<p><strong>Standardization</strong> refers to the process of simplifying data by replacing variant
representations of the same underlying observation by a default value in
order to improve the accuracy of field comparisons. For example, “First
Street” and “1st St” are two ways of writing the same street name, but a
simple string comparison of these values will return a poor result. By
standardizing fields—and using the same standardization rules across
files!—the number of true matches that are wrongly classified as
nonmatches (i.e., the number of false nonmatches) can be reduced.</p>
<p>Some common examples of standardization are:</p>
<ul>
<li><p>Standardization of different spellings of frequently occurring
words: for example, replacing common abbreviations in street names
(Ave, St, etc.) or titles (Ms, Dr, etc.) with a common form. These
kinds of rules are highly country- and language-specific.</p></li>
<li><p>General standardization, including converting character fields to
all uppercase and removing punctuation and digits.</p></li>
</ul>
<p><strong>Deduplication</strong> consists of removing redundant records from a single list, that is,
multiple records from the same list that refer to the same underlying
entity. After deduplication, each record in the first list will have at
most one true match in the second list and vice versa. This simplifies
the record linkage process and is necessary if the goal of record
linkage is to find the best set of one-to-one links (as opposed to a
list of all possible links). One can deduplicate a list by applying
record linkage techniques described in this chapter to link a file to
itself.</p>
<p><strong>Normalization</strong> is the process of ensuring that the fields that are being compared
across files are as similar as possible in the sense that they could
have been generated by the same process. At minimum, the same
standardization rules should be applied to both files. For additional
examples, consider a salary field in a survey. There are number
different ways that salary could be recorded: it might be truncated as a
privacy-preserving measure or rounded to the nearest thousand, and
missing values could be imputed with the mean or with zero. During
normalization we take note of exactly how fields are recorded.</p>
</div>
<div id="new-tools-for-new-data" class="section level2">
<h2><span class="header-section-number">1.5</span> New tools for new data</h2>
<p>The new data sources that we have discussed frequently require working
at scales for which the social scientist’s familiar tools are not
designed. Fortunately, the wider research and data analytics community
has developed a wide variety of often more scalable and flexible
tools—tools that we will introduce within this book.</p>
<p>Relational database management systems (DBMSs)<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> are used throughout
business as well as the sciences to organize, process, and search large
collections of structured data. NoSQL DBMSs are used for data that is
extremely large and/or unstructured, such as collections of web pages,
social media data (e.g., Twitter messages), and clinical notes.
Extensions to these systems and also specialized single-purpose DBMSs
provide support for data types that are not easily handled in
statistical packages such as geospatial data, networks, and graphs.</p>
<p>Open source programming systems such as Python (used extensively
throughout this book) and R provide high-quality implementations of
numerous data analysis and visualization methods, from regression to
statistics, text analysis, network analysis, and much more. Finally,
parallel computing systems such as Hadoop and Spark can be used to
harness parallel computer clusters for extremely large data sets and
computationally intensive analyses.</p>
<p>These various components may not always work together as smoothly as do
integrated packages such as SAS, SPSS, and Stata, but they allow
researchers to take on problems of great scale and complexity.
Furthermore, they are developing at a tremendous rate as the result of
work by thousands of people worldwide. For these reasons, the modern
social scientist needs to be familiar with their characteristics and
capabilities.</p>
</div>
<div id="sec:1-6" class="section level2">
<h2><span class="header-section-number">1.6</span> The book’s “use case”</h2>
<p>This book is about the uses of big data in social science. Our focus is
on working through the use of data as a social scientist normally
approaches research. That involves thinking through how to use such data
to address a question from beginning to end, and thereby learning about
the associated tools—rather than simply engaging in coding exercises
and then thinking about how to apply them to a potpourri of social
science examples.</p>
<p>There are many examples of the use of big data in social science
research, but relatively few that feature all the different aspects that
are covered in this book. As a result, the chapters in the book draw
heavily on a use case based on one of the first large-scale big data
social science data infrastructures. This infrastructure, based on
UMETRICS<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> data housed at the University of Michigan’s Institute for
Research on Innovation and Science (IRIS)<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> and enhanced with data from
the US Census Bureau, provides a new quantitative analysis and
understanding of science policy based on large-scale computational
analysis of new types of data.</p>
<p>The infrastructure was developed in response to a call from the
President’s Science Advisor (Jack Marburger) for a <em>science of science
policy</em> <span class="citation">(Marburger <a href="#ref-marburger2005wanted">2005</a>)</span>. He wanted a scientific response to the
questions that he was asked about the impact of investments in science.</p>
<hr />
<p><strong>Example: The Science of Science Policy</strong></p>
<p>Marburger wrote <span class="citation">(Marburger <a href="#ref-marburger2005wanted">2005</a>)</span>: &quot;How much should a nation spend
on science? What kind of science? How much from private versus public
sectors? Does demand for funding by potential science performers imply a
shortage of funding or a surfeit of performers? These and related
science policy questions tend to be asked and answered today in a highly
visible advocacy context that makes assumptions that are deserving of
closer scrutiny. A new ‘science of science policy’ is emerging, and it
may offer more compelling guidance for policy decisions and for more
credible advocacy. …</p>
<p>“Relating R&amp;D to innovation in any but a general way is a tall order,
but not a hopeless one. We need econometric models that encompass enough
variables in a sufficient number of countries to produce reasonable
simulations of the effect of specific policy choices. This need won’t be
satisfied by a few grants or workshops, but demands the attention of a
specialist scholarly community. As more economists and social scientists
turn to these issues, the effectiveness of science policy will grow, and
of science advocacy too.”</p>
<hr />
<p>Responding to this policy imperative is a tall order, because it
involves using all the social science and computer science tools
available to researchers. The new digital technologies can be used to
capture the links between the inputs into research, the way in which
those inputs are organized, and the subsequent outputs
<span class="citation">(Weinberg et al. <a href="#ref-weinberg2014science">2014</a>; Zolas et al. <a href="#ref-zolas2015wrapping">2015</a>)</span>. The social science questions
that are addressable with this data infrastructure include the effect of
research training on the placement and earnings of doctoral recipients,
how university trained scientists and engineers affect the productivity
of the firms they work for, and the return on investments in research. Figure <a href="chap-intro.html#fig:fig2">1.2</a> provides an abstract representation of the empirical approach that is needed: data about grants, the people who are funded on grants, and the subsequent scientific and economic activities.</p>
<p>First, data must be captured on what is funded, and since the data are
in text format, computational linguistics tools must be applied
(<a href="chap-text.html#chap:text">Text Analysis</a>). Second, data must be captured on who is funded,
and how they interact in teams, so network tools and analysis must be
used (<a href="chap-networks.html#chap:networks">Networks: The Basics</a>). Third, information about the type of
results must be gleaned from the web and other sources (<a href="chap-web.html#chap:web">Working with Web Data and APIs</a>).</p>
<p>Finally, the disparate
complex data sets need to be stored in databases (<a href="chap-db.html#chap:db">Databases</a>), integrated (<a href="chap-link.html#chap:link">Record Linkage</a>), analyzed (<a href="chap-ml.html#chap:ml">Machine Learning</a>), and used to make inferences (<a href="chap-errors.html#chap:errors">Errors and Inference</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:fig2"></span>
<img src="ChapterIntro/figures/figure_cameron.png" alt="A visualization of the complex links between what and who is funded, and the results; tracing the direct link between funding and results is misleading and wrong" width="70%" />
<p class="caption">
Figure 1.2: A visualization of the complex links between what and who is funded, and the results; tracing the direct link between funding and results is misleading and wrong
</p>
</div>
<p>The use case serves as the thread that ties many of the ideas together.
Rather than asking the reader to learn how to code “hello world,” we
build on data that have been put together to answer a real-world
question, and provide explicit examples based on that data. We then
provide examples that show how the approach generalizes.</p>
<p>For example, the text analysis chapter
(<a href="chap-text.html#chap:text">Text Analysis</a>) shows how to use natural language processing to
describe <em>what</em> research is being done, using proposal and award text to
identify the research topics in a portfolio
<span class="citation">(Talley et al. <a href="#ref-talley2011database">2011</a>; Evans and Foster <a href="#ref-Evans2011">2011</a>)</span>. But then it also shows how the
approach can be used to address a problem that is not just limited to
science policy—the conversion of massive amounts of knowledge that is
stored in text to usable information.</p>
<p>Similarly, the network analysis chapter
(<a href="chap-networks.html#chap:networks">Networks: The Basics</a>) gives specific examples using the UMETRICS
data and shows how such data can be used to create new units of
analysis—the networks of researchers who do science, and the networks
of vendors who supply research inputs. It also shows how networks can be
used to study a wide variety of other social science questions.</p>
<p>In another example, we use APIs<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> provided by publishers to describe the
results generated by research funding in terms of publications and other
measures of scientific impact, but also provide code that can be
repurposed for many similar APIs.</p>
<p>And, of course, since all these new types of data are provided in a
variety of different formats, some of which are quite large (or
voluminous), and with a variety of different timestamps (or velocity),
we discuss how to store the data in different types of data formats.</p>
</div>
<div id="the-structure-of-the-book" class="section level2">
<h2><span class="header-section-number">1.7</span> The structure of the book</h2>
<p>We organize the book in three parts, based around the way social
scientists approach doing research. The first set of chapters addresses
the new ways to capture, curate, and store data. The second set of
chapters describes what tools are available to process and classify
data. The last set deals with analysis and the appropriate handling of
data on individuals and organizations.</p>
<div id="part-i-capture-and-curation" class="section level3">
<h3><span class="header-section-number">1.7.1</span> Part I: Capture and curation</h3>
<p>The four chapters in Part I (see Figure <a href="chap-intro.html#fig:fig3">1.3</a>) tell you how to capture and manage data.</p>
<p><a href="chap-web.html#chap:web">Working with Web Data and APIs</a> describes how to extract information from social
media about the transmission of knowledge. The particular application
will be to develop links to authors’ articles on Twitter using PLOS
articles and to pull information about authors and articles from web
sources by using an API. You will learn how to retrieve link data from
bookmarking services, citations from Crossref, links from Facebook, and
information from news coverage. In keeping with the social science
grounding that is a core feature of the book, the chapter discusses what
data can be captured from online sources, what is potentially reliable,
and how to manage data qualityissues.</p>
<p>Big data differs from survey data in that we must typically combine data
from multiple sources to get a complete picture of the activities of
interest. Although computer scientists may sometimes simply “mash” data
sets together, social scientists are rightfully concerned about issues
of missing links, duplicative links, and erroneous links.
<a href="chap-link.html#chap:link">Record Linkage</a> provides an overview of traditional rule-based
and probabilistic approaches to data linkage, as well as the important
contributions of machine learning to the linkage problem.</p>
<p>Once data have been collected and linked into different files, it is
necessary to store and organize it. Social scientists are used to
working with one analytical file, often in statistical software tools
such as SAS or Stata. <a href="chap-db.html#chap:db">Databases</a>, which may be the most important chapter in the
book, describes different approaches to storing data in ways that permit
rapid and reliable exploration andanalysis.</p>
<p>Big data is sometimes defined as data that are too big to fit onto the
analyst’s computer. <a href="chap-parallel.html#chap:parallel">Programming with Big Data</a> provides an overview of clever programming techniques that facilitate the use of data (often using parallel
computing). While the focus is on one of the most widely used big data
programming paradigms and its most popular implementation, Apache
Hadoop, the goal of the chapter is to provide a conceptual framework to
the key challenges that the approach is designed to address.</p>
<div class="figure" style="text-align: center"><span id="fig:fig3"></span>
<img src="ChapterIntro/figures/Figure2.png" alt="The four chapters of Part I focus on *data capture* and *curation*" width="70%" />
<p class="caption">
Figure 1.3: The four chapters of Part I focus on <em>data capture</em> and <em>curation</em>
</p>
</div>
</div>
<div id="part-ii-modeling-and-analysis" class="section level3">
<h3><span class="header-section-number">1.7.2</span> Part II: Modeling and analysis</h3>
<p>The three chapters in Part II (see Figure <a href="chap-intro.html#fig:fig4">1.4</a>) introduce three of the most important tools that can be used by social scientists to do new and exciting research: machine learning, text analysis, and social network analysis.</p>
<p><a href="chap-ml.html#chap:ml">Machine Learning</a> introduces machine learning methods. It shows the
power of machine learning in a variety of different contexts,
particularly focusing on clustering and classification. You will get an
overview of basic approaches and how those approaches are applied. The
chapter builds from a conceptual framework and then shows you how the
different concepts are translated into code. There is a particular focus
on random forests and support vector machine (SVM) approaches.</p>
<p><a href="chap-text.html#chap:text">Text Analysis</a> describes how social scientists can make use of
one of the most exciting advances in big data—text analysis. Vast
amounts of data that are stored in documents can now be analyzed
andsearched so that different types of information can be retrieved.
Documents (and the underlying activities of the entities that generated
the documents) can be categorized into topics or fields as well as
summarized. In addition, machine translation can be used to compare
documents in different languages.</p>
<p>Social scientists are typically interested in describing the activities
of individuals and organizations (such as households and firms) in a
variety of economic and social contexts. The frames within which data
are collected have typically been generated from tax or other
programmatic sources. The new types of data permit new units of
analysis—particularly network analysis—largely enabled by advances
in mathematical graph theory. Thus, <a href="chap-networks.html#chap:networks">Networks: The Basics</a> describes how social scientists can use network theory to generate measurable representations of patterns of
relationships connecting entities. As the author points out, the value
of the new framework is not only in constructing different
right-hand-side variables but also in studying an entirely new unit of
analysis that lies somewhere between the largely atomistic actors that
occupy the markets of neo-classical theory and the tightly managed
hierarchies that are the traditional object of inquiry of sociologists
and organizational theorists.</p>
<div class="figure" style="text-align: center"><span id="fig:fig4"></span>
<img src="ChapterIntro/figures/Figure3.png" alt="The four chapters in Part II focus on data *modeling* and *analysis*" width="70%" />
<p class="caption">
Figure 1.4: The four chapters in Part II focus on data <em>modeling</em> and <em>analysis</em>
</p>
</div>
</div>
<div id="part-iii-inference-and-ethics" class="section level3">
<h3><span class="header-section-number">1.7.3</span> Part III: Inference and ethics</h3>
<p>The four chapters in Part III (see Figure <a href="chap-intro.html#fig:fig5">1.5</a>) cover three advanced topics relating to data inference and ethics—information visualization, errors and inference, and privacy and confidentiality—and introduce the workbooks that provide access to the practical exercises associated with the text.</p>
<p><a href="chap-viz.html#chap:viz">Information Visualization</a> introduces information visualization methods and
describes how you can use those methods to explore data and communicate
results so that data can be turned into interpretable, actionable
information. There are many ways of presenting statistical information
that convey content in a rigorous manner. The goal of this chapter is to
explore different approaches and examine the information content and
analytical validity of the different approaches. It provides an overview
of effective visualizations.</p>
<p><a href="chap-errors.html#chap:errors">Errors and Inference</a> deals with inference and the errors associated
with big data. Social scientists know only too well the cost associated
with bad data—we highlighted the classic <em>Literary Digest</em> example in
the introduction to this chapter, as well as the more recent Google Flu
Trends. Although the consequences are well understood, the new types of
data are so large and complex that their properties often cannot be
studied in traditional ways. In addition, the data generating function
is such that the data are often selective, incomplete, and erroneous.
Without proper data hygiene, errors can quickly compound. This chapter
provides a systematic way to think about the error framework in a big
data setting.</p>
<p><a href="chap-privacy.html#chap:privacy">Privacy and Confidentiality</a> addresses the issue that sits at the core of
any study of human beings—privacy and confidentiality. In a new field,
like the one covered in this book, it is critical that many researchers
have access to the data so that work can be replicated and built
on—that there be a scientific basis to data science. Yet the rules
that social scientists have traditionally used for survey data, namely
anonymity and informed consent, no longer apply when the data are
collected in the wild. This concluding chapter identifies the issues
that must be addressed for responsible and ethical research to take
place.</p>
<p>Finally, <a href="chap-workbooks.html#chap:workbooks">Workbooks</a> provides an overview of the practical work
that accompanies each chapter—the workbooks that are designed, using <em>Jupyter notebooks</em><a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>, to enable students and interested practitioners to apply the new
techniques and approaches in selected chapters. We hope you have a lot
of fun with them.</p>
<div class="figure" style="text-align: center"><span id="fig:fig5"></span>
<img src="ChapterIntro/figures/Figure4.png" alt="The four chapters in Part III focus on *inference* and *ethics*" width="70%" />
<p class="caption">
Figure 1.5: The four chapters in Part III focus on <em>inference</em> and <em>ethics</em>
</p>
</div>
</div>
</div>
<div id="sec:intro:resources" class="section level2">
<h2><span class="header-section-number">1.8</span> Resources</h2>
<p>For more information on the <strong>science of science policy</strong>, see Husbands et al.’s book for a full discussion of many issues <span class="citation">(Husband Fealing et al. <a href="#ref-husband2011science">2011</a>)</span> and the online resources
at the eponymous website <span class="citation">(SOSP, <a href="#ref-SOSP">n.d.</a>)</span>.</p>
<p>This book is above all a <em>practical</em> introduction to the methods and
tools that the social scientist can use to make sense of big data, and
thus <strong>programming</strong> resources are also important. We make extensive use of the Python
programming language and the MySQL database management system in both
the book and its supporting workbooks. We recommend that any social
scientist who aspires to work with large data sets become proficient in
the use of these two systems, and also one more, GitHub. All three,
fortunately, are quite accessible and are supported by excellent online
resources. Time spent mastering them will be repaid many times over in
more productive research.</p>
<p>For <strong>Python</strong><a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a>, Alex Bell’s <em>Python for Economists</em> (available online <span class="citation">(Bell <a href="#ref-BellPython">2012</a>)</span>) provides a wonderful 30-page introduction to the use of Python in the social sciences,
complete with XKCD cartoons. Economists Tom Sargent and John Stachurski
provide a very useful set of lectures and examples at
<a href="http://quant-econ.net/" class="uri">http://quant-econ.net/</a>. For more detail, we recommend Charles
Severance’s <em>Python for Informatics: Exploring Information</em>
<span class="citation">(Severance <a href="#ref-SeverancePython">2013</a>)</span>, which not only covers basic Python but also provides
material relevant to web data (the subject of
<a href="chap-web.html#chap:web">Working with Web Data and APIs</a>) and MySQL (the subject of
<a href="chap-db.html#chap:db">Databases</a>). This book is also freely available online and is
supported by excellent online lectures and exercises.</p>
<p>For <strong>MySQL</strong>, Chapter <a href="chap-db.html#chap:db">Databases</a> provides introductory material and pointers to
additional resources, so we will not say more here.</p>
<p>We also recommend that you master <strong>GitHub</strong>. A version control system is a tool
for keeping track of changes that have been made to a document over
time. GitHub is a hosting service for projects that use the Git version
control system. As Strasser explains <span class="citation">(Strasser, <a href="#ref-GitResearch">n.d.</a>)</span>, Git/GitHub makes it
straightforward for researchers to create digital lab notebooks that
record the data files, programs, papers, and other resources associated
with a project, with automatic tracking of the changes that are made to
those resources over time. GitHub also makes it easy for collaborators
to work together on a project, whether a program or a paper: changes
made by each contributor are recorded and can easily be reconciled. For
example, we used GitHub to create this book, with authors and editors
checking in changes and comments at different times and from many time
zones. We also use GitHub to provide access to the supporting workbooks.
Ram <span class="citation">(Ram <a href="#ref-ram2013git">2013</a>)</span> provides a nice description of how Git/GitHub can be
used to promote reproducibility and transparency in research.</p>
<p>One more resource that is outside the scope of this book but that you
may well want to master is the <strong>cloud</strong> <span class="citation">(Armbrust et al. <a href="#ref-armbrust2010view">2010</a>; Lifka et al. <a href="#ref-Lifka">2013</a>)</span>. It used to
be that when your data and computations became too large to analyze on
your laptop, you were out of luck unless your employer (or a friend) had
a larger computer. With the emergence of cloud storage and computing
services from the likes of Amazon Web Services, Google, and Microsoft,
powerful computers are available to anyone with a credit card. We and
many others have had positive experiences using such systems for the
analysis of urban <span class="citation">(Catlett et al. <a href="#ref-plenario">2014</a>)</span>, environmental <span class="citation">(Elliott et al. <a href="#ref-elliott2014parallel">2014</a>)</span>, and
genomic <span class="citation">(Bhuvaneshwar et al. <a href="#ref-bhuvaneshwar2015case">2015</a>)</span> data analysis and modeling, for example.
Such systems may well represent the future of research computing.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Chetty2012">
<p>Chetty, Raj. 2012. “The Transformative Potential of Administrative Data for Microeconometric Research.” <a href="http://conference.nber.org/confer/2012/SI2012/LS/ChettySlides.pdf" class="uri">http://conference.nber.org/confer/2012/SI2012/LS/ChettySlides.pdf</a>. Accessed February 1, 2016.</p>
</div>
<div id="ref-lee2012rise">
<p>Lee, Yang, WooYoung Chung, Stuart Madnick, Richard Wang, and Hongyun Zhang. 2012. “On the Rise of the Chief Data Officers in a World of Big Data.” In <em>Pre-Icis 2012 Sim Academic Workshop, Orlando, Florida</em>.</p>
</div>
<div id="ref-alawadhi2012building">
<p>Alawadhi, Suha, Armando Aldama-Nalda, Hafedh Chourabi, J. Ramon Gil-Garcia, Sofia Leung, Sehl Mellouli, Taewoo Nam, Theresa A. Pardo, Hans J. Scholl, and Shawn Walker. 2012. “Building Understanding of Smart City Initiatives.” In <em>Electronic Government</em>, 40–53. Springer.</p>
</div>
<div id="ref-brynjolfsson2011strength">
<p>Brynjolfsson, Erik, Lorin M. Hitt, and Heekyung Hellen Kim. 2011. “Strength in Numbers: How Does Data-Driven Decisionmaking Affect Firm Performance?” Available at SSRN 1819486.</p>
</div>
<div id="ref-japec2015big">
<p>Japec, Lilli, Frauke Kreuter, Marcus Berg, Paul Biemer, Paul Decker, Cliff Lampe, Julia Lane, Cathy O’Neil, and Abe Usher. 2015. “Big Data in Survey Research: AAPOR Task Force Report.” <em>Public Opinion Quarterly</em> 79 (4). AAPOR: 839–80.</p>
</div>
<div id="ref-greenwood2014">
<p>Greenwood, Daniel, Arkadiusz Stopczynski, Brian Sweatt, Thomas Hardjono, and Alex Pentland. 2014. “The New Deal on Data: A Framework for Institutional Controls.” In <em>Privacy, Big Data, and the Public Good: Frameworks for Engagement</em>, edited by Julia Lane, Victoria Stodden, Stefan Bender, and Helen Nissenbaum, 192. Cambridge University Press.</p>
</div>
<div id="ref-keller2012big">
<p>Keller, Sallie Ann, Steven E. Koonin, and Stephanie Shipp. 2012. “Big Data and City Living: What Can It Do for Us?” <em>Significance</em> 9 (4). Blackwell: 4–7.</p>
</div>
<div id="ref-murphy2014social">
<p>Murphy, Joe, Michael W Link, Jennifer Hunter Childs, Casey Langer Tesfaye, Elizabeth Dean, Michael Stern, Josh Pasek, Jon Cohen, Mario Callegaro, and Paul Harwood. 2014. “Social Media in Public Opinion Research: Report of the AAPOR Task Force on Emerging Technologies in Public Opinion Research.” <em>Public Opinion Quarterly</em> 78 (4): 788–94.</p>
</div>
<div id="ref-carr2015geography">
<p>Carr, Jillian B., and Jennifer L. Doleac. 2015. “The Geography, Incidence, and Underreporting of Gun Violence: New Evidence Using ShotSpotter Data.” Technical report, <a href="http://jenniferdoleac.com/wp-content/uploads/2015/03/Carr_Doleac_gunfire_underreporting.pdf" class="uri">http://jenniferdoleac.com/wp-content/uploads/2015/03/Carr_Doleac_gunfire_underreporting.pdf</a>.</p>
</div>
<div id="ref-antenucci2014using">
<p>Antenucci, Dolan, Michael Cafarella, Margaret Levenstein, Christopher Ré, and Matthew D. Shapiro. 2014. “Using Social Media to Measure Labor Market Flows.” National Bureau of Economic Research.</p>
</div>
<div id="ref-einav2013data">
<p>Einav, Liran, and Jonathan D. Levin. 2013. “The Data Revolution and Economic Analysis.” National Bureau of Economic Research.</p>
</div>
<div id="ref-Elbel2011">
<p>Elbel, B., J. Gyamfi, and R. Kersh. 2011. “Child and Adolescent Fast-Food Choice and the Influence of Calorie Labeling: A Natural Experiment.” <em>International Journal of Obesity</em> 35 (4). Macmillan Publishers Limited: 493–500. <a href="http://dx.doi.org/10.1038/ijo.2011.4" class="uri">http://dx.doi.org/10.1038/ijo.2011.4</a>.</p>
</div>
<div id="ref-Mas2009">
<p>Mas, Alexandre, and Enrico Moretti. 2009. “Peers at Work.” <em>American Economic Review</em> 99 (1): 112–45. <a href="http://www.aeaweb.org/articles.php?doi=10.1257/aer.99.1.112" class="uri">http://www.aeaweb.org/articles.php?doi=10.1257/aer.99.1.112</a>.</p>
</div>
<div id="ref-stanton2006high">
<p>Stanton, Mark W, and MK Rutherford. 2006. <em>The High Concentration of Us Health Care Expenditures</em>. Agency for Healthcare Research; Quality.</p>
</div>
<div id="ref-evans1987tests">
<p>Evans, David S. 1987. “Tests of Alternative Theories of Firm Growth.” <em>Journal of Political Economy</em> 95. JSTOR: 657–74.</p>
</div>
<div id="ref-decker2015has">
<p>Decker, Ryan A., John Haltiwanger, Ron S. Jarmin, and Javier Miranda. n.d. “Where Has All the Skewness Gone? The Decline in High-Growth (Young) Firms in the US.” <em>European Economic Review</em>.</p>
</div>
<div id="ref-groves2004survey">
<p>Groves, Robert M. 2004. <em>Survey Errors and Survey Costs</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-biemer2003">
<p>Biemer, Paul P., and Lars E. Lyberg. 2003. <em>Introduction to Survey Quality</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-hansen1993sample">
<p>Hansen, Morris H., William N. Hurwitz, and William G. Madow. 1993. <em>Sample Survey Methods and Theory</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-valliant2013practical">
<p>Valliant, Richard, Jill A Dever, and Frauke Kreuter. 2013. <em>Practical Tools for Designing and Weighting Survey Samples</em>. Springer.</p>
</div>
<div id="ref-BLS2015">
<p>Bureau of Labor Statistics. n.d. “The Employment Situation—November 2015.” <a href="http://www.bls.gov/news.release/archives/empsit_12042015.pdf" class="uri">http://www.bls.gov/news.release/archives/empsit_12042015.pdf</a>.</p>
</div>
<div id="ref-lohr2009sampling">
<p>Lohr, Sharon. 2009. <em>Sampling: Design and Analysis</em>. Cengage Learning.</p>
</div>
<div id="ref-Meyer2015">
<p>Meyer, Bruce D., Wallace K. C. Mok, and James X. Sullivan. 2015. “Household Surveys in Crisis.” <em>Journal of Economic Perspectives</em> 29 (4): 199–226.</p>
</div>
<div id="ref-little2014statistical">
<p>Little, Roderick J. A., and Donald B Rubin. 2014. <em>Statistical Analysis with Missing Data</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-imbens2015causal">
<p>Imbens, Guido W., and Donald B. Rubin. 2015. <em>Causal Inference in Statistics, Social, and Biomedical Sciences</em>. Cambridge University Press.</p>
</div>
<div id="ref-shelton2014mapping">
<p>Shelton, Taylor, Ate Poorthuis, Mark Graham, and Matthew Zook. 2014. “Mapping the Data Shadows of Hurricane Sandy: Uncovering the Sociospatial Dimensions of ‘Big Data’.” <em>Geoforum</em> 52. Elsevier: 167–79.</p>
</div>
<div id="ref-squire19881936">
<p>Squire, Peverill. 1988. “Why the 1936 Literary Digest Poll Failed.” <em>Public Opinion Quarterly</em> 52 (1). AAPOR: 125–33.</p>
</div>
<div id="ref-donohue2006uses">
<p>Donohue III, John J., and Justin Wolfers. 2006. “Uses and Abuses of Empirical Evidence in the Death Penalty Debate.” National Bureau of Economic Research.</p>
</div>
<div id="ref-stuart2010matching">
<p>Stuart, Elizabeth A. 2010. “Matching Methods for Causal Inference: A Review and a Look Forward.” <em>Statistical Science</em> 25 (1). NIH Public Access: 1.</p>
</div>
<div id="ref-dugoff2014generalizing">
<p>DuGoff, Eva H., Megan Schuler, and Elizabeth A. Stuart. 2014. “Generalizing Observational Study Results: Applying Propensity Score Methods to Complex Surveys.” <em>Health Services Research</em> 49 (1). Wiley Online Library: 284–303.</p>
</div>
<div id="ref-morgan2014counterfactuals">
<p>Morgan, Stephen L., and Christopher Winship. 2014. <em>Counterfactuals and Causal Inference</em>. Cambridge University Press.</p>
</div>
<div id="ref-harford2014big">
<p>Harford, Tim. 2014. “Big Data: A Big Mistake?” <em>Significance</em> 11 (5). Wiley Online Library: 14–19.</p>
</div>
<div id="ref-christen2012data">
<p>Christen, Peter. 2012b. <em>Data Matching: Concepts and Techniques for Record Linkage, Entity Resolution, and Duplicate Detection</em>. Springer.</p>
</div>
<div id="ref-marburger2005wanted">
<p>Marburger, John H. 2005. “Wanted: Better Benchmarks.” <em>Science</em> 308 (5725). American Association for the Advancement of Science: 1087.</p>
</div>
<div id="ref-weinberg2014science">
<p>Weinberg, Bruce A., Jason Owen-Smith, Rebecca F Rosen, Lou Schwarz, Barbara McFadden Allen, Roy E. Weiss, and Julia Lane. 2014. “Science Funding and Short-Term Economic Activity.” <em>Science</em> 344 (6179). NIH Public Access: 41.</p>
</div>
<div id="ref-zolas2015wrapping">
<p>Zolas, Nikolas, Nathan Goldschlag, Ron Jarmin, Paula Stephan, Jason Owen-Smith, Rebecca F Rosen, Barbara McFadden Allen, Bruce A Weinberg, and Julia Lane. 2015. “Wrapping It up in a Person: Examining Employment and Earnings Outcomes for Ph.D. Recipients.” <em>Science</em> 350 (6266). American Association for the Advancement of Science: 1367–71.</p>
</div>
<div id="ref-talley2011database">
<p>Talley, Edmund M., David Newman, David Mimno, Bruce W. Herr II, Hanna M. Wallach, Gully A. P. C. Burns, A. G. Miriam Leenders, and Andrew McCallum. 2011. “Database of NIH Grants Using Machine-Learned Categories and Graphical Clustering.” <em>Nature Methods</em> 8 (6). Nature Publishing Group: 443–44.</p>
</div>
<div id="ref-Evans2011">
<p>Evans, J. A., and J. G. Foster. 2011. “Metaknowledge.” <em>Science</em> 331 (6018): 721–25.</p>
</div>
<div id="ref-husband2011science">
<p>Husband Fealing, Kaye, Julia Ingrid Lane, Jack Marburger, and Stephanie Shipp. 2011. <em>Science of Science Policy: The Handbook</em>. Stanford University Press.</p>
</div>
<div id="ref-SOSP">
<p>SOSP. n.d. “Science of Science Policy.” <a href="http://www.scienceofsciencepolicy.net/" class="uri">http://www.scienceofsciencepolicy.net/</a>. Accessed February 1, 2016.</p>
</div>
<div id="ref-BellPython">
<p>Bell, Alex. 2012. “Python for Economists.” <a href="http://cs.brown.edu/~ambell/pyseminar/pyseminar.html" class="uri">http://cs.brown.edu/~ambell/pyseminar/pyseminar.html</a>.</p>
</div>
<div id="ref-SeverancePython">
<p>Severance, Charles. 2013. “Python for Informatics: Exploring Information.” <a href="http://www.pythonlearn.com/book.php" class="uri">http://www.pythonlearn.com/book.php</a>; CreateSpace.</p>
</div>
<div id="ref-GitResearch">
<p>Strasser, Carly. n.d. “Git/GitHub: A Primer for Researchers.” <a href="http://datapub.cdlib.org/2014/05/05/github-a-primer-for-researchers/" class="uri">http://datapub.cdlib.org/2014/05/05/github-a-primer-for-researchers/</a>.</p>
</div>
<div id="ref-ram2013git">
<p>Ram, Karthik. 2013. “Git Can Facilitate Greater Reproducibility and Increased Transparency in Science.” <em>Source Code for Biology and Medicine</em> 8 (1): 7.</p>
</div>
<div id="ref-armbrust2010view">
<p>Armbrust, Michael, Armando Fox, Rean Griffith, Anthony D. Joseph, Randy Katz, Andy Konwinski, Gunho Lee, et al. 2010. “A View of Cloud Computing.” <em>Communications of the ACM</em> 53 (4). ACM: 50–58.</p>
</div>
<div id="ref-Lifka">
<p>Lifka, D., I. Foster, S. Mehringer, M. Parashar, P. Redfern, C. Stewart, and S. Tuecke. 2013. “XSEDE Cloud Survey Report.” Technical report, National Science Foundation, USA, <a href="http://hdl.handle.net/2142/45766" class="uri">http://hdl.handle.net/2142/45766</a>.</p>
</div>
<div id="ref-plenario">
<p>Catlett, Charlie, Tanu Malik, Brett Goldstein, Jonathan Giuffrida, Yetong Shao, Alessandro Panella, Derek Eder, et al. 2014. “Plenario: An Open Data Discovery and Exploration Platform for Urban Science.” <em>Bulletin of the IEEE Computer Society Technical Committee on Data Engineering</em>, 27–42.</p>
</div>
<div id="ref-elliott2014parallel">
<p>Elliott, Joshua, David Kelly, James Chryssanthacopoulos, Michael Glotter, Kanika Jhunjhnuwala, Neil Best, Michael Wilde, and Ian Foster. 2014. “The Parallel System for Integrating Impact Models and Sectors (pSIMS).” <em>Environmental Modelling &amp; Software</em> 62. Elsevier: 509–16.</p>
</div>
<div id="ref-bhuvaneshwar2015case">
<p>Bhuvaneshwar, Krithika, Dinanath Sulakhe, Robinder Gauba, Alex Rodriguez, Ravi Madduri, Utpal Dave, Lukasz Lacinski, Ian Foster, Yuriy Gusev, and Subha Madhavan. 2015. “A Case Study for Cloud Based High Throughput Analysis of NGS Data Using the Globus Genomics System.” <em>Computational and Structural Biotechnology Journal</em> 13. Elsevier: 64–74.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>This topic is discussed in more detail in Chapter 5<a href="chap-intro.html#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>This topic is discussed in more detail in Chapter 10.<a href="chap-intro.html#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>This topic is discussed in more detail in Chapter 3.<a href="chap-intro.html#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>This topic is discussed in more detail in Chapter 4.<a href="chap-intro.html#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>UMETRICS: Universities Measuring the Impact of Research on Innovation and Science <span class="citation">(Lane et al. <a href="#ref-lane2015new">2015</a>)</span><a href="chap-intro.html#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>iris.isr.umich.edu<a href="chap-intro.html#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p>Application Programming Interfaces<a href="chap-intro.html#fnref7" class="footnote-back">↩</a></p></li>
<li id="fn8"><p>See jupyter.org.<a href="chap-intro.html#fnref8" class="footnote-back">↩</a></p></li>
<li id="fn9"><p>Read this! <a href="http://alexbell.net/pyseminar/pyseminar.html" class="uri">http://alexbell.net/pyseminar/pyseminar.html</a><a href="chap-intro.html#fnref9" class="footnote-back">↩</a></p></li>
</ol>
</div>
<div id="disqus_thread"></div>
<script>
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = '//big-data-and-social-science.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the
<a href="https://disqus.com/?ref_noscript">
  comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chap-web.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/Coleridge-Initiative/big-data-and-social-science/edit/master/01-ChapterIntro.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
