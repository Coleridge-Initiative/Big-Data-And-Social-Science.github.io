<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Big Data and Social Science</title>
  <meta name="description" content="Big Data and Social Science">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Big Data and Social Science" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="Coleridge-Initiative/big-data-and-social-science" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Big Data and Social Science" />
  
  
  

<meta name="author" content="Ian Foster">
<meta name="author" content="Rayid Ghani">
<meta name="author" content="Ron S. Jarmin">
<meta name="author" content="Frauke Kreuter">
<meta name="author" content="Julia Lane">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="chap-ml.html">
<link rel="next" href="chap-networks.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Big Data and Social Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="chap-intro.html"><a href="chap-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-1"><i class="fa fa-check"></i><b>1.1</b> Why this book?</a></li>
<li class="chapter" data-level="1.2" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-2"><i class="fa fa-check"></i><b>1.2</b> Defining big data and its value</a></li>
<li class="chapter" data-level="1.3" data-path="chap-intro.html"><a href="chap-intro.html#sec:1.3"><i class="fa fa-check"></i><b>1.3</b> Social science, inference, and big data</a></li>
<li class="chapter" data-level="1.4" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-5"><i class="fa fa-check"></i><b>1.4</b> Social science, data quality, and big data</a></li>
<li class="chapter" data-level="1.5" data-path="chap-intro.html"><a href="chap-intro.html#new-tools-for-new-data"><i class="fa fa-check"></i><b>1.5</b> New tools for new data</a></li>
<li class="chapter" data-level="1.6" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-6"><i class="fa fa-check"></i><b>1.6</b> The book’s “use case”</a></li>
<li class="chapter" data-level="1.7" data-path="chap-intro.html"><a href="chap-intro.html#the-structure-of-the-book"><i class="fa fa-check"></i><b>1.7</b> The structure of the book</a><ul>
<li class="chapter" data-level="1.7.1" data-path="chap-intro.html"><a href="chap-intro.html#part-i-capture-and-curation"><i class="fa fa-check"></i><b>1.7.1</b> Part I: Capture and curation</a></li>
<li class="chapter" data-level="1.7.2" data-path="chap-intro.html"><a href="chap-intro.html#part-ii-modeling-and-analysis"><i class="fa fa-check"></i><b>1.7.2</b> Part II: Modeling and analysis</a></li>
<li class="chapter" data-level="1.7.3" data-path="chap-intro.html"><a href="chap-intro.html#part-iii-inference-and-ethics"><i class="fa fa-check"></i><b>1.7.3</b> Part III: Inference and ethics</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="chap-intro.html"><a href="chap-intro.html#sec:intro:resources"><i class="fa fa-check"></i><b>1.8</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chap-web.html"><a href="chap-web.html"><i class="fa fa-check"></i><b>2</b> Working with Web Data and APIs</a><ul>
<li class="chapter" data-level="2.1" data-path="chap-web.html"><a href="chap-web.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="chap-web.html"><a href="chap-web.html#sec:4-1"><i class="fa fa-check"></i><b>2.2</b> Scraping information from the web</a><ul>
<li class="chapter" data-level="2.2.1" data-path="chap-web.html"><a href="chap-web.html#sec:4-1.1"><i class="fa fa-check"></i><b>2.2.1</b> Obtaining data from websites</a></li>
<li class="chapter" data-level="2.2.2" data-path="chap-web.html"><a href="chap-web.html#sec:4-1.2"><i class="fa fa-check"></i><b>2.2.2</b> Limits of scraping</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="chap-web.html"><a href="chap-web.html#sec:4-3"><i class="fa fa-check"></i><b>2.3</b> Application Programming Interfaces (APIs)</a><ul>
<li class="chapter" data-level="2.3.1" data-path="chap-web.html"><a href="chap-web.html#sec:4-3.1"><i class="fa fa-check"></i><b>2.3.1</b> Relevant APIs and resources</a></li>
<li class="chapter" data-level="2.3.2" data-path="chap-web.html"><a href="chap-web.html#sec:4-3.2"><i class="fa fa-check"></i><b>2.3.2</b> RESTful APIs, returned data, and Python wrappers</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="chap-web.html"><a href="chap-web.html#sec:4-4"><i class="fa fa-check"></i><b>2.4</b> Using an API</a></li>
<li class="chapter" data-level="2.5" data-path="chap-web.html"><a href="chap-web.html#sec:4-4.1"><i class="fa fa-check"></i><b>2.5</b> Another example: Using the ORCID API via a wrapper</a></li>
<li class="chapter" data-level="2.6" data-path="chap-web.html"><a href="chap-web.html#sec:4-6"><i class="fa fa-check"></i><b>2.6</b> Integrating data from multiple sources</a></li>
<li class="chapter" data-level="2.7" data-path="chap-web.html"><a href="chap-web.html#sec:4-9"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="chap-web.html"><a href="chap-web.html#acknowledgements-and-copyright"><i class="fa fa-check"></i><b>2.8</b> Acknowledgements and copyright</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chap-link.html"><a href="chap-link.html"><i class="fa fa-check"></i><b>3</b> Record Linkage</a><ul>
<li class="chapter" data-level="3.1" data-path="chap-link.html"><a href="chap-link.html#motivation"><i class="fa fa-check"></i><b>3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2" data-path="chap-link.html"><a href="chap-link.html#sec:recordlinkage"><i class="fa fa-check"></i><b>3.2</b> Introduction to record linkage</a></li>
<li class="chapter" data-level="3.3" data-path="chap-link.html"><a href="chap-link.html#preprocessing-data-for-record-linkage"><i class="fa fa-check"></i><b>3.3</b> Preprocessing data for record linkage</a></li>
<li class="chapter" data-level="3.4" data-path="chap-link.html"><a href="chap-link.html#S:indexing"><i class="fa fa-check"></i><b>3.4</b> Indexing and blocking</a></li>
<li class="chapter" data-level="3.5" data-path="chap-link.html"><a href="chap-link.html#matching"><i class="fa fa-check"></i><b>3.5</b> Matching</a><ul>
<li class="chapter" data-level="3.5.1" data-path="chap-link.html"><a href="chap-link.html#rule-based-approaches"><i class="fa fa-check"></i><b>3.5.1</b> Rule-based approaches</a></li>
<li class="chapter" data-level="3.5.2" data-path="chap-link.html"><a href="chap-link.html#probabilistic-record-linkage"><i class="fa fa-check"></i><b>3.5.2</b> Probabilistic record linkage</a></li>
<li class="chapter" data-level="3.5.3" data-path="chap-link.html"><a href="chap-link.html#machine-learning-approaches-to-linking"><i class="fa fa-check"></i><b>3.5.3</b> Machine learning approaches to linking</a></li>
<li class="chapter" data-level="3.5.4" data-path="chap-link.html"><a href="chap-link.html#disambiguating-networks"><i class="fa fa-check"></i><b>3.5.4</b> Disambiguating networks</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="chap-link.html"><a href="chap-link.html#classification"><i class="fa fa-check"></i><b>3.6</b> Classification</a><ul>
<li class="chapter" data-level="3.6.1" data-path="chap-link.html"><a href="chap-link.html#S:thresholds"><i class="fa fa-check"></i><b>3.6.1</b> Thresholds</a></li>
<li class="chapter" data-level="3.6.2" data-path="chap-link.html"><a href="chap-link.html#one-to-one-links"><i class="fa fa-check"></i><b>3.6.2</b> One-to-one links</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="chap-link.html"><a href="chap-link.html#record-linkage-and-data-protection"><i class="fa fa-check"></i><b>3.7</b> Record linkage and data protection</a></li>
<li class="chapter" data-level="3.8" data-path="chap-link.html"><a href="chap-link.html#summary"><i class="fa fa-check"></i><b>3.8</b> Summary</a></li>
<li class="chapter" data-level="3.9" data-path="chap-link.html"><a href="chap-link.html#resources"><i class="fa fa-check"></i><b>3.9</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chap-db.html"><a href="chap-db.html"><i class="fa fa-check"></i><b>4</b> Databases</a><ul>
<li class="chapter" data-level="4.1" data-path="chap-db.html"><a href="chap-db.html#sec:db:intro"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="chap-db.html"><a href="chap-db.html#sec:db:when"><i class="fa fa-check"></i><b>4.2</b> DBMS: When and why</a></li>
<li class="chapter" data-level="4.3" data-path="chap-db.html"><a href="chap-db.html#relational-dbmss"><i class="fa fa-check"></i><b>4.3</b> Relational DBMSs</a><ul>
<li class="chapter" data-level="4.3.1" data-path="chap-db.html"><a href="chap-db.html#structured-query-language-sql"><i class="fa fa-check"></i><b>4.3.1</b> Structured Query Language (SQL)</a></li>
<li class="chapter" data-level="4.3.2" data-path="chap-db.html"><a href="chap-db.html#sec:db:sql"><i class="fa fa-check"></i><b>4.3.2</b> Manipulating and querying data</a></li>
<li class="chapter" data-level="4.3.3" data-path="chap-db.html"><a href="chap-db.html#sec:db:schema"><i class="fa fa-check"></i><b>4.3.3</b> Schema design and definition</a></li>
<li class="chapter" data-level="4.3.4" data-path="chap-db.html"><a href="chap-db.html#loading-data"><i class="fa fa-check"></i><b>4.3.4</b> Loading data</a></li>
<li class="chapter" data-level="4.3.5" data-path="chap-db.html"><a href="chap-db.html#transactions-and-crash-recovery"><i class="fa fa-check"></i><b>4.3.5</b> Transactions and crash recovery</a></li>
<li class="chapter" data-level="4.3.6" data-path="chap-db.html"><a href="chap-db.html#sec:db:index"><i class="fa fa-check"></i><b>4.3.6</b> Database optimizations</a></li>
<li class="chapter" data-level="4.3.7" data-path="chap-db.html"><a href="chap-db.html#caveats-and-challenges"><i class="fa fa-check"></i><b>4.3.7</b> Caveats and challenges</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="chap-db.html"><a href="chap-db.html#linking-dbmss-and-other-tools"><i class="fa fa-check"></i><b>4.4</b> Linking DBMSs and other tools</a></li>
<li class="chapter" data-level="4.5" data-path="chap-db.html"><a href="chap-db.html#sec:db:nosql"><i class="fa fa-check"></i><b>4.5</b> NoSQL databases</a><ul>
<li class="chapter" data-level="4.5.1" data-path="chap-db.html"><a href="chap-db.html#challenges-of-scale-the-cap-theorem"><i class="fa fa-check"></i><b>4.5.1</b> Challenges of scale: The CAP theorem</a></li>
<li class="chapter" data-level="4.5.2" data-path="chap-db.html"><a href="chap-db.html#nosql-and-keyvalue-stores"><i class="fa fa-check"></i><b>4.5.2</b> NoSQL and key–value stores</a></li>
<li class="chapter" data-level="4.5.3" data-path="chap-db.html"><a href="chap-db.html#other-nosql-databases"><i class="fa fa-check"></i><b>4.5.3</b> Other NoSQL databases</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="chap-db.html"><a href="chap-db.html#sec:db:spatial"><i class="fa fa-check"></i><b>4.6</b> Spatial databases</a></li>
<li class="chapter" data-level="4.7" data-path="chap-db.html"><a href="chap-db.html#which-database-to-use"><i class="fa fa-check"></i><b>4.7</b> Which database to use?</a><ul>
<li class="chapter" data-level="4.7.1" data-path="chap-db.html"><a href="chap-db.html#relational-dbmss-1"><i class="fa fa-check"></i><b>4.7.1</b> Relational DBMSs</a></li>
<li class="chapter" data-level="4.7.2" data-path="chap-db.html"><a href="chap-db.html#nosql-dbmss"><i class="fa fa-check"></i><b>4.7.2</b> NoSQL DBMSs</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="chap-db.html"><a href="chap-db.html#summary-1"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
<li class="chapter" data-level="4.9" data-path="chap-db.html"><a href="chap-db.html#resources-1"><i class="fa fa-check"></i><b>4.9</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chap-parallel.html"><a href="chap-parallel.html"><i class="fa fa-check"></i><b>5</b> Programming with Big Data</a><ul>
<li class="chapter" data-level="5.1" data-path="chap-parallel.html"><a href="chap-parallel.html#introduction-1"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="chap-parallel.html"><a href="chap-parallel.html#sec:intro"><i class="fa fa-check"></i><b>5.2</b> The MapReduce programming model</a></li>
<li class="chapter" data-level="5.3" data-path="chap-parallel.html"><a href="chap-parallel.html#apache-hadoop-mapreduce"><i class="fa fa-check"></i><b>5.3</b> Apache Hadoop MapReduce</a><ul>
<li class="chapter" data-level="5.3.1" data-path="chap-parallel.html"><a href="chap-parallel.html#the-hadoop-distributed-file-system"><i class="fa fa-check"></i><b>5.3.1</b> The Hadoop Distributed File System</a></li>
<li class="chapter" data-level="5.3.2" data-path="chap-parallel.html"><a href="chap-parallel.html#hadoop-bringing-compute-to-the-data"><i class="fa fa-check"></i><b>5.3.2</b> Hadoop: Bringing compute to the data</a></li>
<li class="chapter" data-level="5.3.3" data-path="chap-parallel.html"><a href="chap-parallel.html#hardware-provisioning"><i class="fa fa-check"></i><b>5.3.3</b> Hardware provisioning</a></li>
<li class="chapter" data-level="5.3.4" data-path="chap-parallel.html"><a href="chap-parallel.html#programming-language-support"><i class="fa fa-check"></i><b>5.3.4</b> Programming language support</a></li>
<li class="chapter" data-level="5.3.5" data-path="chap-parallel.html"><a href="chap-parallel.html#fault-tolerance"><i class="fa fa-check"></i><b>5.3.5</b> Fault tolerance</a></li>
<li class="chapter" data-level="5.3.6" data-path="chap-parallel.html"><a href="chap-parallel.html#limitations-of-hadoop"><i class="fa fa-check"></i><b>5.3.6</b> Limitations of Hadoop</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="chap-parallel.html"><a href="chap-parallel.html#apache-spark"><i class="fa fa-check"></i><b>5.4</b> Apache Spark</a></li>
<li class="chapter" data-level="5.5" data-path="chap-parallel.html"><a href="chap-parallel.html#summary-2"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
<li class="chapter" data-level="5.6" data-path="chap-parallel.html"><a href="chap-parallel.html#resources-2"><i class="fa fa-check"></i><b>5.6</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chap-ml.html"><a href="chap-ml.html"><i class="fa fa-check"></i><b>6</b> Machine Learning</a><ul>
<li class="chapter" data-level="6.1" data-path="chap-ml.html"><a href="chap-ml.html#introduction-2"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="chap-ml.html"><a href="chap-ml.html#what-is-machine-learning"><i class="fa fa-check"></i><b>6.2</b> What is machine learning?</a></li>
<li class="chapter" data-level="6.3" data-path="chap-ml.html"><a href="chap-ml.html#the-machine-learning-process"><i class="fa fa-check"></i><b>6.3</b> The machine learning process</a></li>
<li class="chapter" data-level="6.4" data-path="chap-ml.html"><a href="chap-ml.html#problem-formulation-mapping-a-problem-to-machine-learning-methods"><i class="fa fa-check"></i><b>6.4</b> Problem formulation: Mapping a problem to machine learning methods</a></li>
<li class="chapter" data-level="6.5" data-path="chap-ml.html"><a href="chap-ml.html#methods"><i class="fa fa-check"></i><b>6.5</b> Methods</a><ul>
<li class="chapter" data-level="6.5.1" data-path="chap-ml.html"><a href="chap-ml.html#unsupervised-learning-methods"><i class="fa fa-check"></i><b>6.5.1</b> Unsupervised learning methods</a></li>
<li class="chapter" data-level="6.5.2" data-path="chap-ml.html"><a href="chap-ml.html#sec:MLchapter:super"><i class="fa fa-check"></i><b>6.5.2</b> Supervised learning</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="chap-ml.html"><a href="chap-ml.html#evaluation"><i class="fa fa-check"></i><b>6.6</b> Evaluation</a><ul>
<li class="chapter" data-level="6.6.1" data-path="chap-ml.html"><a href="chap-ml.html#methodology"><i class="fa fa-check"></i><b>6.6.1</b> Methodology</a></li>
<li class="chapter" data-level="6.6.2" data-path="chap-ml.html"><a href="chap-ml.html#metrics"><i class="fa fa-check"></i><b>6.6.2</b> Metrics</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="chap-ml.html"><a href="chap-ml.html#practical-tips"><i class="fa fa-check"></i><b>6.7</b> Practical tips</a><ul>
<li class="chapter" data-level="6.7.1" data-path="chap-ml.html"><a href="chap-ml.html#features"><i class="fa fa-check"></i><b>6.7.1</b> Features</a></li>
<li class="chapter" data-level="6.7.2" data-path="chap-ml.html"><a href="chap-ml.html#machine-learning-pipeline"><i class="fa fa-check"></i><b>6.7.2</b> Machine learning pipeline</a></li>
<li class="chapter" data-level="6.7.3" data-path="chap-ml.html"><a href="chap-ml.html#multiclass-problems"><i class="fa fa-check"></i><b>6.7.3</b> Multiclass problems</a></li>
<li class="chapter" data-level="6.7.4" data-path="chap-ml.html"><a href="chap-ml.html#skewed-or-imbalanced-classification-problems"><i class="fa fa-check"></i><b>6.7.4</b> Skewed or imbalanced classification problems</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="chap-ml.html"><a href="chap-ml.html#how-can-social-scientists-benefit-from-machine-learning"><i class="fa fa-check"></i><b>6.8</b> How can social scientists benefit from machine learning?</a></li>
<li class="chapter" data-level="6.9" data-path="chap-ml.html"><a href="chap-ml.html#advanced-topics"><i class="fa fa-check"></i><b>6.9</b> Advanced topics</a></li>
<li class="chapter" data-level="6.10" data-path="chap-ml.html"><a href="chap-ml.html#summary-3"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
<li class="chapter" data-level="6.11" data-path="chap-ml.html"><a href="chap-ml.html#ml:res"><i class="fa fa-check"></i><b>6.11</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chap-text.html"><a href="chap-text.html"><i class="fa fa-check"></i><b>7</b> Text Analysis</a><ul>
<li class="chapter" data-level="7.1" data-path="chap-text.html"><a href="chap-text.html#understanding-what-people-write"><i class="fa fa-check"></i><b>7.1</b> Understanding what people write</a></li>
<li class="chapter" data-level="7.2" data-path="chap-text.html"><a href="chap-text.html#how-to-analyze-text"><i class="fa fa-check"></i><b>7.2</b> How to analyze text</a><ul>
<li class="chapter" data-level="7.2.1" data-path="chap-text.html"><a href="chap-text.html#processing-text-data"><i class="fa fa-check"></i><b>7.2.1</b> Processing text data</a></li>
<li class="chapter" data-level="7.2.2" data-path="chap-text.html"><a href="chap-text.html#how-much-is-a-word-worth"><i class="fa fa-check"></i><b>7.2.2</b> How much is a word worth?</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="chap-text.html"><a href="chap-text.html#sec:appapp"><i class="fa fa-check"></i><b>7.3</b> Approaches and applications</a><ul>
<li class="chapter" data-level="7.3.1" data-path="chap-text.html"><a href="chap-text.html#sec:lda"><i class="fa fa-check"></i><b>7.3.1</b> Topic modeling</a></li>
<li class="chapter" data-level="7.3.2" data-path="chap-text.html"><a href="chap-text.html#sec:ir"><i class="fa fa-check"></i><b>7.3.2</b> Information retrieval and clustering</a></li>
<li class="chapter" data-level="7.3.3" data-path="chap-text.html"><a href="chap-text.html#sec:other"><i class="fa fa-check"></i><b>7.3.3</b> Other approaches</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="chap-text.html"><a href="chap-text.html#sec:eval"><i class="fa fa-check"></i><b>7.4</b> Evaluation</a></li>
<li class="chapter" data-level="7.5" data-path="chap-text.html"><a href="chap-text.html#text-analysis-tools"><i class="fa fa-check"></i><b>7.5</b> Text analysis tools</a></li>
<li class="chapter" data-level="7.6" data-path="chap-text.html"><a href="chap-text.html#summary-4"><i class="fa fa-check"></i><b>7.6</b> Summary</a></li>
<li class="chapter" data-level="7.7" data-path="chap-text.html"><a href="chap-text.html#resources-3"><i class="fa fa-check"></i><b>7.7</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chap-networks.html"><a href="chap-networks.html"><i class="fa fa-check"></i><b>8</b> Networks: The Basics</a><ul>
<li class="chapter" data-level="8.1" data-path="chap-networks.html"><a href="chap-networks.html#introduction-3"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="chap-networks.html"><a href="chap-networks.html#network-data"><i class="fa fa-check"></i><b>8.2</b> Network data</a><ul>
<li class="chapter" data-level="8.2.1" data-path="chap-networks.html"><a href="chap-networks.html#forms-of-network-data"><i class="fa fa-check"></i><b>8.2.1</b> Forms of network data</a></li>
<li class="chapter" data-level="8.2.2" data-path="chap-networks.html"><a href="chap-networks.html#inducing-one-mode-networks-from-two-mode-data"><i class="fa fa-check"></i><b>8.2.2</b> Inducing one-mode networks from two-mode data</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="chap-networks.html"><a href="chap-networks.html#network-measures"><i class="fa fa-check"></i><b>8.3</b> Network measures</a><ul>
<li class="chapter" data-level="8.3.1" data-path="chap-networks.html"><a href="chap-networks.html#reachability"><i class="fa fa-check"></i><b>8.3.1</b> Reachability</a></li>
<li class="chapter" data-level="8.3.2" data-path="chap-networks.html"><a href="chap-networks.html#whole-network-measures"><i class="fa fa-check"></i><b>8.3.2</b> Whole-network measures</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="chap-networks.html"><a href="chap-networks.html#comparing-collaboration-networks"><i class="fa fa-check"></i><b>8.4</b> Comparing collaboration networks</a></li>
<li class="chapter" data-level="8.5" data-path="chap-networks.html"><a href="chap-networks.html#summary-5"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="chap-networks.html"><a href="chap-networks.html#resources-4"><i class="fa fa-check"></i><b>8.6</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chap-viz.html"><a href="chap-viz.html"><i class="fa fa-check"></i><b>9</b> Information Visualization</a><ul>
<li class="chapter" data-level="9.1" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-1"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2"><i class="fa fa-check"></i><b>9.2</b> Developing effective visualizations</a></li>
<li class="chapter" data-level="9.3" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-3"><i class="fa fa-check"></i><b>9.3</b> A data-by-tasks taxonomy</a><ul>
<li class="chapter" data-level="9.3.1" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.1"><i class="fa fa-check"></i><b>9.3.1</b> Multivariate data</a></li>
<li class="chapter" data-level="9.3.2" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.2"><i class="fa fa-check"></i><b>9.3.2</b> Spatial data</a></li>
<li class="chapter" data-level="9.3.3" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.4"><i class="fa fa-check"></i><b>9.3.3</b> Temporal data</a></li>
<li class="chapter" data-level="9.3.4" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.5"><i class="fa fa-check"></i><b>9.3.4</b> Hierarchical data</a></li>
<li class="chapter" data-level="9.3.5" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.6"><i class="fa fa-check"></i><b>9.3.5</b> Network data</a></li>
<li class="chapter" data-level="9.3.6" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.7"><i class="fa fa-check"></i><b>9.3.6</b> Text data</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4"><i class="fa fa-check"></i><b>9.4</b> Challenges</a><ul>
<li class="chapter" data-level="9.4.1" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4.1"><i class="fa fa-check"></i><b>9.4.1</b> Scalability</a></li>
<li class="chapter" data-level="9.4.2" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4.2"><i class="fa fa-check"></i><b>9.4.2</b> Evaluation</a></li>
<li class="chapter" data-level="9.4.3" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4.3"><i class="fa fa-check"></i><b>9.4.3</b> Visual impairment</a></li>
<li class="chapter" data-level="9.4.4" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4.4"><i class="fa fa-check"></i><b>9.4.4</b> Visual literacy</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-5"><i class="fa fa-check"></i><b>9.5</b> Summary</a></li>
<li class="chapter" data-level="9.6" data-path="chap-viz.html"><a href="chap-viz.html#sec:mylabel4"><i class="fa fa-check"></i><b>9.6</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chap-errors.html"><a href="chap-errors.html"><i class="fa fa-check"></i><b>10</b> Errors and Inference</a><ul>
<li class="chapter" data-level="10.1" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-1"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-2"><i class="fa fa-check"></i><b>10.2</b> The total error paradigm</a><ul>
<li class="chapter" data-level="10.2.1" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-2.1"><i class="fa fa-check"></i><b>10.2.1</b> The traditional model</a></li>
<li class="chapter" data-level="10.2.2" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-2.2"><i class="fa fa-check"></i><b>10.2.2</b> Extending the framework to big data</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-3"><i class="fa fa-check"></i><b>10.3</b> Illustrations of errors in big data</a></li>
<li class="chapter" data-level="10.4" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-4"><i class="fa fa-check"></i><b>10.4</b> Errors in big data analytics</a><ul>
<li class="chapter" data-level="10.4.1" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-4.1"><i class="fa fa-check"></i><b>10.4.1</b> Errors resulting from volume, velocity, and variety, assuming perfect veracity</a></li>
<li class="chapter" data-level="10.4.2" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-4.2"><i class="fa fa-check"></i><b>10.4.2</b> Errors resulting from lack of veracity</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-5"><i class="fa fa-check"></i><b>10.5</b> Some methods for mitigating, detecting, and compensating for errors</a></li>
<li class="chapter" data-level="10.6" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-6"><i class="fa fa-check"></i><b>10.6</b> Summary</a></li>
<li class="chapter" data-level="10.7" data-path="chap-errors.html"><a href="chap-errors.html#resources-5"><i class="fa fa-check"></i><b>10.7</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chap-privacy.html"><a href="chap-privacy.html"><i class="fa fa-check"></i><b>11</b> Privacy and Confidentiality</a><ul>
<li class="chapter" data-level="11.1" data-path="chap-privacy.html"><a href="chap-privacy.html#introduction-4"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="chap-privacy.html"><a href="chap-privacy.html#why-is-access-important"><i class="fa fa-check"></i><b>11.2</b> Why is access important?</a></li>
<li class="chapter" data-level="11.3" data-path="chap-privacy.html"><a href="chap-privacy.html#providing-access"><i class="fa fa-check"></i><b>11.3</b> Providing access</a></li>
<li class="chapter" data-level="11.4" data-path="chap-privacy.html"><a href="chap-privacy.html#the-new-challenges"><i class="fa fa-check"></i><b>11.4</b> The new challenges</a></li>
<li class="chapter" data-level="11.5" data-path="chap-privacy.html"><a href="chap-privacy.html#legal-and-ethical-framework"><i class="fa fa-check"></i><b>11.5</b> Legal and ethical framework</a></li>
<li class="chapter" data-level="11.6" data-path="chap-privacy.html"><a href="chap-privacy.html#summary-6"><i class="fa fa-check"></i><b>11.6</b> Summary</a></li>
<li class="chapter" data-level="11.7" data-path="chap-privacy.html"><a href="chap-privacy.html#resources-6"><i class="fa fa-check"></i><b>11.7</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chap-workbooks.html"><a href="chap-workbooks.html"><i class="fa fa-check"></i><b>12</b> Workbooks</a><ul>
<li class="chapter" data-level="12.1" data-path="chap-workbooks.html"><a href="chap-workbooks.html#introduction-5"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="chap-workbooks.html"><a href="chap-workbooks.html#environment"><i class="fa fa-check"></i><b>12.2</b> Environment</a><ul>
<li class="chapter" data-level="12.2.1" data-path="chap-workbooks.html"><a href="chap-workbooks.html#running-workbooks-locally"><i class="fa fa-check"></i><b>12.2.1</b> Running workbooks locally</a></li>
<li class="chapter" data-level="12.2.2" data-path="chap-workbooks.html"><a href="chap-workbooks.html#central-workbook-server"><i class="fa fa-check"></i><b>12.2.2</b> Central workbook server</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="chap-workbooks.html"><a href="chap-workbooks.html#workbook-details"><i class="fa fa-check"></i><b>12.3</b> Workbook details</a><ul>
<li class="chapter" data-level="12.3.1" data-path="chap-workbooks.html"><a href="chap-workbooks.html#social-media-and-apis"><i class="fa fa-check"></i><b>12.3.1</b> Social Media and APIs</a></li>
<li class="chapter" data-level="12.3.2" data-path="chap-workbooks.html"><a href="chap-workbooks.html#database-basics"><i class="fa fa-check"></i><b>12.3.2</b> Database basics</a></li>
<li class="chapter" data-level="12.3.3" data-path="chap-workbooks.html"><a href="chap-workbooks.html#data-linkage"><i class="fa fa-check"></i><b>12.3.3</b> Data Linkage</a></li>
<li class="chapter" data-level="12.3.4" data-path="chap-workbooks.html"><a href="chap-workbooks.html#machine-learning"><i class="fa fa-check"></i><b>12.3.4</b> Machine Learning</a></li>
<li class="chapter" data-level="12.3.5" data-path="chap-workbooks.html"><a href="chap-workbooks.html#text-analysis"><i class="fa fa-check"></i><b>12.3.5</b> Text Analysis</a></li>
<li class="chapter" data-level="12.3.6" data-path="chap-workbooks.html"><a href="chap-workbooks.html#networks"><i class="fa fa-check"></i><b>12.3.6</b> Networks</a></li>
<li class="chapter" data-level="12.3.7" data-path="chap-workbooks.html"><a href="chap-workbooks.html#visualization"><i class="fa fa-check"></i><b>12.3.7</b> Visualization</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="chap-workbooks.html"><a href="chap-workbooks.html#resources-7"><i class="fa fa-check"></i><b>12.4</b> Resources</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Big Data and Social Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chap:text" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Text Analysis</h1>
<p><strong>Evgeny Klochikhin and Jordan Boyd-Graber</strong></p>
<p>This chapter provides an overview of how social scientists can make use
of one of the most exciting advances in big data—text analysis. Vast
amounts of data that are stored in documents can now be analyzed and
searched so that different types of information can be retrieved.
Documents (and the underlying activities of the entities that generated
the documents) can be categorized into topics or fields as well as
summarized. In addition, machine translation can be used to compare
documents in different languages.</p>
<div id="understanding-what-people-write" class="section level2">
<h2><span class="header-section-number">7.1</span> Understanding what people write</h2>
<p>You wake up and read the newspaper, a Facebook post, or an academic
article a colleague sent you. You, like other humans, can digest and
understand rich information, but an increasingly central challenge for
humans is to cope with the deluge of information we are supposed to read
and understand. In our use case of science, even Aristotle struggled
with categorizing areas of science; the vast increase in the scope of
written research has only made the challenge greater.</p>
<p>One approach is to use rule-based methods to tag documents for
categorization. Businesses used to employ human beings to read the news
and tag documents on topics of interest for senior management. The rules
on how to assign these topics and tags were developed and communicated
to these human beings beforehand. Such a manual categorization process
is still common in multiple applications, e.g., systematic literature
reviews <span class="citation">(Brody et al. <a href="#ref-brody2015">2015</a>)</span>.</p>
<p>However, as anyone who has used a search engine knows, newer approaches
exist to categorize text and help humans cope with overload:
computer-aided <em>text analysis</em>. Text data can be used to “conventional”
data sources, such as surveys and administrative data, since the words
spoken or written by individuals often provide more nuanced and
unanticipated insights.
Chapter <a href="chap-link.html#chap:link">Record Linkage</a><a href="#fn40" class="footnote-ref" id="fnref40"><sup>40</sup></a> discusses how to link data to create larger, more
diverse data sets. The linkage data sets need not just be numeric, but
can also include data sets consisting of text data.</p>
<hr />
<p><strong>Example: Using text to categorize scientific fields</strong></p>
<p>The National Center for Science and Engineering Statistics, the US
statistical agency charged with collecting statistics on science and
engineering, uses a rule-based system to manually create categories of
science; these are then used to categorize research as “physics” or
“economics” <span class="citation">(Mortensen, Bloch, and others <a href="#ref-oecd2005measurement">2005</a>; Economic Co-operation and Development <a href="#ref-manual2004summary">2004</a>)</span>. In a rule-based
system there is no ready response to the question “how much do we spend
on climate change, food safety, or biofuels?” because existing rules
have not created such categories. Text analysis techniques can be used
to provide such detail without manual collation. For example, data about
research awards from public sources and about people funded on research
grants from UMETRICS can be linked with data about their subsequent
publications and related student dissertations from ProQuest. Both award
and dissertation data are text documents that can be used to
characterize what research has been done, provide information about
which projects are similar within or across institutions, and
potentially identify new fields of study <span class="citation">(Talley et al. <a href="#ref-talley2011database">2011</a>)</span>.</p>
<hr />
<p>Overall, text analysis can help with specific tasks that define
application-specific subfields including the following:</p>
<ul>
<li><p><strong>Searches and information retrieval</strong>: Text analysis tools can help find relevant information in large
databases. For example, we used these techniques in systematic
literature reviews to facilitate the discovery and retrieval of
relevant publications related to early grade reading in Latin
America and the Caribbean.</p></li>
<li><p><strong>Clustering and text categorization</strong>: Tools like topic modeling can provide a big picture of the contents
of thousands of documents in a comprehensible format by discovering
only the most important words and phrases in those documents.</p></li>
<li><p><strong>Text summarization</strong>: Similar to clustering, text summarization can provide value in
processing large documents and text corpora. For example, Wang et
al. <span class="citation">(Y. Wang et al. <a href="#ref-wang-09">2009</a>)</span> use topic modeling to produce category-sensitive text
summaries and annotations on large-scale document collections.</p></li>
<li><p><strong>Machine translation</strong>: Machine translation is an example of a text analysis method that
provides quick insights into documents written in other</p></li>
</ul>
</div>
<div id="how-to-analyze-text" class="section level2">
<h2><span class="header-section-number">7.2</span> How to analyze text</h2>
<p>Human language is complex and nuanced, which makes analysis difficult.
We often make simplifying assumptions: we assume our input is perfect
text; we ignore humor <span class="citation">(Halevy, Norvig, and Pereira <a href="#ref-halevy-09">2009</a>)</span> and deception <span class="citation">(Niculae et al. <a href="#ref-niculae-15">2015</a>; Ott et al. <a href="#ref-ott-11">2011</a>)</span>;
and we assume “standard” English <span class="citation">(Kong et al. <a href="#ref-kong-14">2014</a>)</span><a href="#fn41" class="footnote-ref" id="fnref41"><sup>41</sup></a>.</p>
<p>Recognizing this complexity, the goal of text mining is to reduce the
complexity of text and extract important messages in a comprehensible
and meaningful way. This objective is usually achieved through text
categorization or automatic classification<a href="#fn42" class="footnote-ref" id="fnref42"><sup>42</sup></a>. These tools can be used in
multiple applications to gain salient insights into the relationships
between words and documents. Examples include using machine learning to
analyze the flow and topic segmentation of political debates and
behaviors <span class="citation">(Nguyen, Boyd-Graber, and Resnik <a href="#ref-nguyen-12">2012</a>; Nguyen et al. <a href="#ref-Nguyen:Boyd-Graber:Resnik:Miler-2015">2015</a>)</span> and to
assign automated tags to documents <span class="citation">(Tuarob, Pouchard, and Giles <a href="#ref-tuarob-13">2013</a>)</span>.</p>
<p>Information retrieval has a similar objective of extracting the most
important messages from textual data that would answer a particular
query. The process analyzes the full text or metadata related to
documents and allows only relevant knowledge to be discovered and
returned to the query maker. Typical information retrieval tasks include
knowledge discovery <span class="citation">(Mukherjea <a href="#ref-Mukherjea-05">2005</a>)</span>, word sense disambiguation
<span class="citation">(Navigli et al. <a href="#ref-navigli-11">2011</a>)</span>, and sentiment analysis <span class="citation">(Pang and Lee <a href="#ref-pang-08">2008</a>)</span>.</p>
<p>The choice of appropriate tools to address specific tasks significantly
depends on the context and application. For example, document
classification techniques can be used to gain insights into the general
contents of a large corpus of documents <span class="citation">(Talley et al. <a href="#ref-talley2011database">2011</a>)</span>, or to
discover a particular knowledge area, or to link corpora based on
implicit semantic relationships <span class="citation">(Bron, Huurnink, and Rijke <a href="#ref-bron-11">2011</a>)</span>.</p>
<p>In practical terms, some of the questions can be: How much does the US
government invest in climate change research and nanotechnology? Or what
are the main topics in the political debate on guns in the United
States? Or how can we build a salient and dynamic taxonomy of all
scientific research?</p>
<p>We begin with a review of established techniques to begin the process of
analyzing text. Section <a href="chap-text.html#sec:appapp">Approaches and applications</a> provides an overview of topic modeling,
information retrieval and clustering, and other approaches accompanied
by practical examples and applications.
Section <a href="chap-text.html#sec:eval">Evaluation</a>
reviews key evaluation techniques used to assess the validity,
robustness and utility of derived results.</p>
<div id="processing-text-data" class="section level3">
<h3><span class="header-section-number">7.2.1</span> Processing text data</h3>
<p>The first important step in working with text data is cleaning and
processing<a href="#fn43" class="footnote-ref" id="fnref43"><sup>43</sup></a>. Textual data are often messy and unstructured, which makes
many researchers and practitioners overlook their value. Depending on
the source, cleaning and processing these data can require varying
amounts of effort but typically involve a set of established techniques.</p>
<p><strong>Text corpora</strong></p>
<p>A set of multiple similar documents is called a <em>corpus</em>. For example,
the Brown University Standard Corpus of Present-Day American English, or
just the Brown Corpus <span class="citation">(Francis and Kucera <a href="#ref-browncorpus">1979</a>)</span>, is a collection of processed
documents from works published in the United States in 1961. The Brown
Corpus was a historical milestone: it was a machine-readable collection
of a million words across 15 balanced genres with each word tagged with
its part of speech (e.g., noun, verb, preposition). The British National
Corpus <span class="citation">(University of Oxford <a href="#ref-bnc">2006</a>)</span> repeated the same process for British English at a larger
scale. The Penn Treebank <span class="citation">(Marcus, Santorini, and Marcinkiewicz <a href="#ref-marcus-93">1993</a>)</span> provides additional information:
in addition to part-of-speech annotation, it provides <em>syntactic</em>
annotation. For example, what is the object of the sentence “The man
bought the hat”? These standard corpora serve as training data to train
the classifiers and machine learning techniques to automatically analyze
text <span class="citation">(Halevy, Norvig, and Pereira <a href="#ref-halevy-09">2009</a>)</span>.</p>
<p>However, not every corpus is effective for every purpose: the number and
scope of documents determine the range of questions that you can ask and
the quality of the answers you will get back: too few documents result
in a lack of coverage, too many of the wrong kind of documents invite
confusing noise.</p>
<p><strong>Tokenization</strong></p>
<p>The first step in processing text is deciding what terms and phrases are
meaningful. Tokenization separates sentences and terms from each other.
The Natural Language Toolkit (NLTK) <span class="citation">(Bird, Klein, and Loper <a href="#ref-bird-09">2009</a>)</span> provides simple reference
implementations of standard natural language processing algorithms such
as tokenization—for example, sentences are separated from each other
using punctuation such as period, question mark, or exclamation mark.
However, this does not cover all cases such as quotes, abbreviations, or
informal communication on social media. While separating sentences in a
single language is hard enough, some documents “code-switch,” combining
multiple languages in a single document. These complexities are best
addressed through data-driven machine learning frameworks <span class="citation">(Kiss and Strunk <a href="#ref-kiss-06">2006</a>)</span>.</p>
<p><strong>Stop words</strong></p>
<p>Once the tokens are clearly separated, it is possible to perform further
text processing at a more granular, token level. Stop words are a
category of words that have limited semantic meaning regardless of the
document contents. Such words can be prepositions, articles, common
nouns, etc. For example, the word “the” accounts for about 7% of all
words in the Brown Corpus, and “to” and “of” are more than 3% each
<span class="citation">(Malmkjær <a href="#ref-malmkjar-02">2002</a>)</span>.</p>
<p><em>Hapax legomena</em> are rarely occurring words that might have only one
instance in the entire corpus. These words—names, misspellings, or
rare technical terms—are also unlikely to bear significant contextual
meaning. Similar to stop words, these tokens are often disregarded in
further modeling either by the design of the method or by manual removal
from the corpus before the actual analysis.</p>
<p><strong><span class="math inline">\(N\)</span>-grams</strong></p>
<p>However, individual words are sometimes not the correct unit of
analysis. For example, blindly removing stop words can obscure important
phrases such as “systems of innovation,” “cease and desist,” or
“commander in chief.” Identifying these <span class="math inline">\(N\)</span>-grams requires looking for
statistical patterns to discover phrases that often appear together in
fixed patterns <span class="citation">(Dunning <a href="#ref-Dunning-93">1993</a>)</span>. These combinations of phrases are often
called <em>collocations</em>, as their overall meaning is more than the sum of
their parts.</p>
<p><strong>Stemming and lemmatization</strong></p>
<p>Text normalization is another important aspect of preprocessing textual
data. Given the complexity of natural language, words can take multiple
forms dependent on the syntactic structure with limited change of their
original meaning. For example, the word “system” morphologically has a
plural “systems” or an adjective “systematic.” All these words are
semantically similar and—for many tasks—should be treated the same.
For example, if a document has the word “system” occurring three times,
“systems” once, and “systematic” twice, one can assume that the word
“system” with similar meaning and morphological structure can cover all
instances and that variance should be reduced to “system” with six
instances.</p>
<p>The process for text normalization is often implemented using
established lemmatization and stemming algorithms. A <em>lemma</em> is the
original dictionary form of a word. For example, “go,” “went,” and
“goes” will all have the lemma “go.” The stem is a central part of a
given word bearing its primary semantic meaning and uniting a group of
similar lexical units. For example, the words “order” and “ordering”
will have the same stem “ord.” Morphy (a lemmatizer provided by the
electronic dictionary WordNet), Lancaster Stemmer, and Snowball Stemmer
are common tools used to derive lemmas and stems for tokens, and all
have implementations in the NLTK <span class="citation">(Bird, Klein, and Loper <a href="#ref-bird-09">2009</a>)</span>.</p>
<p>All text-processing steps are critical to successful analysis. Some of
them bear more importance than others, depending on the specific
application, research questions, and properties of the corpus. Having
all these tools ready is imperative to producing a clean input for
subsequent modeling and analysis. Some simple rules should be followed
to prevent typical errors. For example, stop words should not be removed
before performing <span class="math inline">\(n\)</span>-gram indexing, and a stemmer should not be used
where data are complex and require accounting for all possible forms and
meanings of words. Reviewing interim results at every stage of the
process can be helpful.</p>
</div>
<div id="how-much-is-a-word-worth" class="section level3">
<h3><span class="header-section-number">7.2.2</span> How much is a word worth?</h3>
<p>Not all words are worth the same; in an article about electronics,
“capacitor” is more important than “aspect.” Appropriately weighting<a href="#fn44" class="footnote-ref" id="fnref44"><sup>44</sup></a> and
calibrating words is important for both human and machine consumers of
text data: humans do not want to see “the” as the most frequent word of
every document in summaries, and classification algorithms benefit from
knowing which features are actually important to making a decision.</p>
<p>Weighting words requires balancing how often a word appears in a local
context (such as a document) with how much it appears overall in the
document collection. Term frequency–inverse document frequency (TFIDF)
<span class="citation">(Salton <a href="#ref-salton-68">1968</a>)</span> is a weighting scheme to explicitly balance these factors
and prioritize the most meaningful words. The TFIDF model takes into
account both the term frequency of a given token and its document
frequency (Box 7.1) so that if a highly frequent word also appears
in almost all documents, its meaning for the specific context of the
corpus is negligible. Stop words are a good example when highly frequent
words also bear limited meaning since they appear in virtually all
documents of a givencorpus.</p>
<div class="F00">
<p>
<strong>Box 7.1: TFIDF</strong> For every token <span class="math inline"><span class="math inline">\(t\)</span></span> and every document <span class="math inline"><span class="math inline">\(d\)</span></span> in the corpus <span class="math inline"><span class="math inline">\(D\)</span></span>, TFIDF is calculated as <span class="math display"><span class="math display">\[tfidf(t,d,D) = tf(t,d) \times
idf(t,D),\]</span></span> where term frequency is either a simple count, <span class="math display"><span class="math display">\[tf(t,d)=f(t,d),\]</span></span> or a more balanced quantity, <span class="math display"><span class="math display">\[tf(t,d) = 0.5+\frac{0.5 \times
  f(t,d)}{\max\{f(t,d):t\in d\}},\]</span></span> and inverse document frequency is <span class="math display"><span class="math display">\[\
idf(t,D) = \log\frac{N}{|\{d\in D:t\in d\}|}.\]</span></span>
</p>
</div>

</div>
</div>
<div id="sec:appapp" class="section level2">
<h2><span class="header-section-number">7.3</span> Approaches and applications</h2>
<p>In this section, we discuss several approaches that allow users to
perform an unsupervised analysis of large text corpora. That is,
approaches that do not require extensive investment of time from experts
or programmers to begin to understand large text corpora. The ease of
using these approaches provides additional opportunities for social
scientists and policymakers to gain insights into policy and research
questions through text analysis.</p>
<p>First, we discuss topic modeling, an approach that discovers <em>topics</em>
that constitute the high-level themes of a corpus. Topic modeling is
often described as an <em>information discovery</em> process: describing what
concepts are present in a corpus. Second, we discuss information
retrieval, which finds the closest documents to a particular concept a
user wants to discover. In contrast to topic modeling (exposing the
primary concepts the corpus, heretofore unknown), information retrieval
finds documents that express already known concepts. Other approaches
can be used for document classification, sentiment analysis, and
part-of-speech tagging.</p>
<div id="sec:lda" class="section level3">
<h3><span class="header-section-number">7.3.1</span> Topic modeling</h3>
<p>As topic modeling is a broad subfield of natural language processing and
machine learning, we will restrict our focus to a single latent
Dirichlet allocation (LDA) <span class="citation">(Blei, Ng, and Jordan <a href="#ref-blei-03">2003</a>)</span>. LDA is a fully Bayesian extension
of probabilistic latent semantic indexing <span class="citation">(Hofmann <a href="#ref-hofmann-99">1999</a>)</span>, itself a
probabilistic extension of latent semantic analysis <span class="citation">(Landauer and Dumais <a href="#ref-landauer-97">1997</a>)</span>. Blei
and Lafferty <span class="citation">(Blei and Lafferty <a href="#ref-blei-09">2009</a>)</span> provide a more detailed discussion of the
history of topic models.</p>
<p>LDA, like all topic models, assumes that there are topics that form the
building blocks of a corpus. Topics are distributions over words and are
often shown as a ranked list of words, with the highest probability
words at the top of the list
(Figure <a href="chap-text.html#fig:nyt-topics-3">7.1</a>). However, we do not know what the topics are
<span class="roman">a priori</span>; the challenge is to discover what they are (more on
this shortly).</p>
<img src="ChapterText/figures/nyt_topics-1.png" width="70%" style="display: block; margin: auto;" />
<img src="ChapterText/figures/nyt_topics-2.png" width="70%" style="display: block; margin: auto;" />
<div class="figure" style="text-align: center"><span id="fig:nyt-topics-3"></span>
<img src="ChapterText/figures/nyt_topics-3.png" alt="Topics are distributions over words. Here are three example topics learned by latent Dirichlet allocation from a model with 50 topics discovered from the *New York Times* [@sandhaus-08]. Topic 1 seems to be about technology, Topic 2 about business, and Topic 3 about the arts" width="70%" />
<p class="caption">
Figure 7.1: Topics are distributions over words. Here are three example topics learned by latent Dirichlet allocation from a model with 50 topics discovered from the <em>New York Times</em> <span class="citation">(Sandhaus <a href="#ref-sandhaus-08">2008</a>)</span>. Topic 1 seems to be about technology, Topic 2 about business, and Topic 3 about the arts
</p>
</div>
<p>In addition to assuming that there exist some number of topics that
explain a corpus, LDA also assumes that each document in a corpus can be
explained by a small number of topics. For example, taking the example
topics from Figure <a href="chap-text.html#fig:nyt-topics-3">7.1</a>, a document titled “Red Light, Green Light: A
Two-Tone LED to Simplify Screens” would be about Topic 1, which appears
to be about technology. However, a document like “Forget the Bootleg,
Just Download the Movie Legally” would require all three of the topics.
The set of topics that are used by a document is called the document’s
<em>allocation</em> (Figure <a href="chap-text.html#fig:nyt-documents">7.2</a>). This terminology explains the name
<em>latent Dirichlet allocation</em>: each document has an allocation over
latent topics governed by a Dirichlet distribution.</p>
<div class="figure" style="text-align: center"><span id="fig:nyt-documents"></span>
<img src="ChapterText/figures/nyt_documents.png" alt="Allocations of documents to topics" width="70%" />
<p class="caption">
Figure 7.2: Allocations of documents to topics
</p>
</div>

<div id="inferring-topics-from-raw-text" class="section level4">
<h4><span class="header-section-number">7.3.1.1</span> Inferring topics from raw text</h4>
<p>Algorithmically, the problem can be viewed as a black box. Given a
corpus and an integer <span class="math inline">\(K\)</span> as input, provide the topics that best
describe the document collection: a process called <em>posterior
inference</em>. The most common algorithm for solving this problem is a
technique called <em>Gibbs sampling</em> <span class="citation">(Geman and Geman <a href="#ref-geman-90">1990</a>)</span>.</p>
<p>Gibbs sampling works at the word level to discover the topics that best
describe a document collection. Each word is associated with a single
topic, explaining why that word appeared in a document. For example,
consider the sentence “Hollywood studios are preparing to let people
download and buy electronic copies of movies over the Internet.” Each
word in this sentence is associated with a topic: “Hollywood” might be
associated with an arts topic; “buy” with a business topic; and
“Internet” with a technology topic
(Figure <a href="chap-text.html#fig:inference-1">7.3</a>).</p>

<div class="figure" style="text-align: center"><span id="fig:inference-1"></span>
<img src="ChapterText/figures/inference_1.png" alt="Each word is associated with a topic. Gibbs sampling inference iteratively resamples the topic assignments for each word to discover the most likely topic assignments that explain the document collection" width="70%" />
<p class="caption">
Figure 7.3: Each word is associated with a topic. Gibbs sampling inference iteratively resamples the topic assignments for each word to discover the most likely topic assignments that explain the document collection
</p>
</div>
<p>This is where we should eventually get. However, we do not know this to
start. So we can initially assign words to topics randomly. This will
result in poor topics, but we can make those topics better. We improve
these topics by taking each word, pretending that we do not know the
topic, and selecting a new topic for the word.</p>
<p>A topic model wants to do two things: it does not want to use many
topics in a document, and it does not want to use many words in a topic.
So the algorithm will keep track of how many times a document <span class="math inline">\(d\)</span> has
used a topic <span class="math inline">\(k\)</span>, <span class="math inline">\(N_{d,k}\)</span>, and how many times a topic <span class="math inline">\(k\)</span> has used a
word <span class="math inline">\(w\)</span>, <span class="math inline">\(V_{k,w}\)</span>. For notational convenience, it will also be useful
to keep track of marginal counts of how many words are in a document,
<span class="math display">\[N_{d, \cdot} \equiv \sum_k N_{d,k},\]</span> and how many words are
associated with a topic, <span class="math display">\[V_{k, \cdot} \equiv \sum_w V_{k, w}.\]</span> The
algorithm removes the counts for a word from <span class="math inline">\(N_{d,k}\)</span> and <span class="math inline">\(V_{k,w}\)</span> and
then changes the topic of a word (hopefully to a better topic than the
one it had before). Through many thousands of iterations of this
process, the algorithm can find topics that are coherent, useful, and
characterize the data well.</p>
<p>The two goals of topic modeling—balancing document allocations to
topics and topics’ distribution over words—come together in an
equation that multiplies them together. A good topic will be both common
in a document and explain a word’s appearance well.</p>
<hr />
<p><strong>Example: Gibbs sampling for topic models</strong></p>
<p>The topic assignment <span class="math inline">\(z_{d,n}\)</span> of word <span class="math inline">\(n\)</span> in document <span class="math inline">\(d\)</span> is
proportional to
<span class="math display">\[p(z_{d,n}=k) \propto \left( \underset{how much doc likes the topic}{\frac{N_{d,k} + \alpha}{N_{d, \cdot} + K \alpha}} \right) \left(\underset{how much topic likes the word}{\frac{V_{k,w_{d,n}} + \beta}{V_{k, \cdot} + V \beta}} \right),\]</span>
where <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are smoothing factors that prevent a topic
from having zero probability if a topic does not use a word or a
document does not use a topic <span class="citation">(Wallach, Mimno, and McCallum <a href="#ref-wallach-09b">2009</a>)</span>. Recall that we do not
include the token that we are sampling in the counts for <span class="math inline">\(N\)</span> or <span class="math inline">\(V\)</span>.</p>
<p>For the sake of concreteness, assume that we have three documents with
the following topic assignments:</p>
<ul>
<li><p>Document 1: <span class="math inline">\(^A\)</span>dog<span class="math inline">\(_3\)</span> <span class="math inline">\(^B\)</span>cat<span class="math inline">\(_2\)</span> <span class="math inline">\(^C\)</span>cat<span class="math inline">\(_3\)</span> <span class="math inline">\(^D\)</span>pig<span class="math inline">\(_1\)</span></p></li>
<li><p>Document 2: <span class="math inline">\(^E\)</span>hamburger<span class="math inline">\(_2\)</span> <span class="math inline">\(^F\)</span>dog<span class="math inline">\(_3\)</span> <span class="math inline">\(^G\)</span>hamburger<span class="math inline">\(_1\)</span></p></li>
<li><p>Document 3: <span class="math inline">\(^H\)</span>iron<span class="math inline">\(_1\)</span> <span class="math inline">\(^I\)</span>iron<span class="math inline">\(_3\)</span> <span class="math inline">\(^J\)</span>pig<span class="math inline">\(_2\)</span> <span class="math inline">\(^K\)</span>iron<span class="math inline">\(_2\)</span></p></li>
</ul>
<p>If we want to sample token B (the first instance of of “cat” in
document 1), we compute the conditional probability for each of the
three topics (<span class="math inline">\(z=1,2,3\)</span>): <span class="math display">\[\begin{aligned}
p(z_B = 1) = &amp; \frac{1 + 1.000}{3 + 3.000} \times \frac{0
    + 1.000}{3 + 5.000} = 0.333 \times 0.125 = 0.042, \\[4pt]
p(z_B = 2) = &amp; \frac{0 + 1.000}{3 + 3.000} \times \frac{0
    + 1.000}{3 + 5.000} = 0.167 \times 0.125 = 0.021\mbox{, and} \\[4pt]
p(z_B = 3) = &amp; \frac{2 + 1.000}{3 + 3.000} \times \frac{1 + 1.000}{4 + 5.000} = 0.500 \times 0.222 = 0.111.\end{aligned}\]</span>
To reiterate, we do not include token B in these counts: in computing
these conditional probabilities, we consider topic 2 as never appearing
in the document and “cat” as never appearing in topic 2. However, “cat”
does appear in topic 3 (token C), so it has a higher probability than
the other topics. After renormalizing, our conditional probabilities are
<span class="math inline">\((0.24, 0.12, 0.64)\)</span>. We then sample the new assignment of token B to be
topic 3 two times out of three. Griffiths and Steyvers <span class="citation">(Griffiths and Steyvers <a href="#ref-griffiths-04">2004</a>)</span>
provide more details on the derivation of this equation.</p>
<p><strong>Example code</strong></p>
<p>Listing 7.1 provides a function to compute the conditional
probability of a single word and return the (unnormalized) probability
to sample from.</p>
<hr />
<pre id="list:7.1" style="PythonStyle" numbers="none" caption="Python code to compute conditional probability of a single word and return the probability from which to sample" label="list:7.1" belowskip="-6pt"><code>def class_sample(docs, vocab, d, n, alpha,
               beta, theta, phi, num_topics):
  # Get the vocabulary ID of the word we are sampling
  type = docs[d][n]

  # Dictionary to store final result
  result = {}

  # Consider each topic possibility
  for kk in xrange(num_topics):
    # theta stores the number of times the document d uses
    # each topic kk; alpha is a smoothing parameter
    doc_contrib = (theta[d][kk] + alpha) / \
         (sum(theta[d].values()) + num_topics * alpha)

    # phi stores the number of times topic kk uses
    # this word type; beta is a smoothing parameter
    topic_contrib = (phi[kk][type] + beta) / \
            (sum(phi[kk].values()) + len(vocab) * beta)

    result[kk] = doc_contrib * topic_contrib
  return result</code></pre>
<div style="text-align: center">
Listing 7.1. Python code to compute conditional probability of a single word and return the probability from which to sample
</div>
</div>
<div id="applications-of-topic-models" class="section level4">
<h4><span class="header-section-number">7.3.1.2</span> Applications of topic models</h4>
<p>Topic modeling is most often used for topic exploration, allowing users
to understand the contents of large text corpora. Thus, topic models
have been used, for example, to understand what the National Institutes
of Health funds <span class="citation">(Talley et al. <a href="#ref-talley2011database">2011</a>)</span>; to compare and contrast what was
discussed in the North and South in the Civil War <span class="citation">(Nelson <a href="#ref-nelson-10">2010</a>)</span>; and to
understand how individuals code in large programming projects
<span class="citation">(Maskeri, Sarkar, and Heafield <a href="#ref-maskeri-08">2008</a>)</span>.</p>
<p>Topic models can also be used as features to more elaborate algorithms
such as machine translation <span class="citation">(Hu et al. <a href="#ref-Hu:Zhai:Eidelman:Boyd-Graber-2014">2014</a>)</span>,
detecting objects in images <span class="citation">(C. Wang, Blei, and Fei-Fei <a href="#ref-wang-09b">2009</a>)</span>, or identifying political
polarization <span class="citation">(Paul and Girju <a href="#ref-paul-10">2010</a>)</span>.</p>
</div>
</div>
<div id="sec:ir" class="section level3">
<h3><span class="header-section-number">7.3.2</span> Information retrieval and clustering</h3>
<p>Information retrieval is a large subdiscipline that encompasses a
variety of methods and approaches. Its main advantage is using
large-scale empirical data to make analytical inferences and class
assignments. Compared to topic modeling, discussed above, information
retrieval techniques can use external knowledge repositories to
categorize given corpora as well as discover smaller and emerging areas
within a large database.</p>
<p>A major concept of information retrieval is a search query that is
usually a short phrase presented by a human or machine to retrieve a
relevant answer to a question or discover relevant knowledge. A good
example of a large-scale information retrieval system is a search
engine, such as Google or Yahoo!, that provides the user with an
opportunity to search the entire Internet almost instantaneously. Such
fast searches are achieved by complex techniques that are linguistic
(set-theoretic), algebraic, probabilistic, or feature-based.</p>
<p><strong>Set-theoretic operations and Boolean logic</strong></p>
<p>Set-theoretic operations proceed from the assumption that any query is a
set of linked components all of which need to be present in the returned
result for it to be relevant. Boolean logic serves as the basis for such
queries; it uses Boolean operators such as , , and to combine query
components. For example, the query</p>
<p><code>induction AND (physics OR logic)</code></p>
<p>will retrieve all documents in which the word “induction” is used,
whether in a physical or logical sense. The <em>extended Boolean model</em> and
<em>fuzzy retrieval</em> are enhanced approaches to calculating the relevance
of retrieved documents based on such queries <span class="citation">(Lee and Fox <a href="#ref-lee:boolean-88">1988</a>)</span>.</p>
<p>Search queries can be also enriched by wildcards and other connectors.
For example, the character &quot;&quot; typically substitutes for any possible
character or characters depending on the settings of the query engine.
(In some instances, search queries can run in <em>nongreedy</em> mode, in which
case, for example, the phrase <code>inform*</code> might retrieve only text up to the end of
the sentence. On the other hand, a <em>greedy</em> query might retrieve full
text following the word or part of word, denoted as <code>inform*</code>, up to the end of
the document, which would essentially mean the same as <code>inform*$</code>.) The wildcard
“<code>?</code>” expects either one or no character in its place, and the wildcard “<code>.</code>”
expects exactly one character. Search queries enhanced with such symbols
and Boolean operators are referred to as <em>regular expressions</em>.</p>
<p>Various databases and search engines can interpret Boolean operators and
wildcards differently depending on their settings and therefore are
prone to return rather different results. This behavior should be
expected and controlled for while running searches on different data
sources.</p>

<hr />
<p><strong>Example: Discover food safety awards</strong></p>
<p>Food safety is an interdisciplinary research area that spans multiple
scientific disciplines including biological sciences, agriculture, and
food science. To retrieve food safety-related awards, we have to
construct a Boolean-based search string that would look for terms and
phrases in those documents and return only relevant results.</p>
<p>An example of such a string would typically be subdivided by category or
search group connected to each other by the <code>AND</code> or <code>OR</code> operator:</p>
<ol style="list-style-type: decimal">
<li><p><strong>General terms</strong>: <code>(food safety OR food securit* OR foodinsecurit*)</code></p></li>
<li><p><strong>Food pathogens</strong>: <code>(food*) AND (acanthamoeba OR actinobacteri* OR (anaerobic organ*) OR DDT OR ...)</code></p></li>
<li><p><strong>Biochemistry and toxicology</strong>: <code>(food*) AND (toxicolog* OR(activated carbon*) OR (acid-hydrol?zed vegetableprotein*) OR aflatoxin* OR ...)</code></p></li>
<li><p><strong>Food processing and preservation</strong>: <code>(food*) AND (process* ORpreserv* OR fortif* OR extrac* OR ...)</code></p></li>
<li><p><strong>Food quality and quality control</strong>: <code>(food*) AND (qualit* OR (danger zon*) OR test* OR (risk analys*) OR ...)</code></p></li>
<li><p><strong>Food-related diseases</strong>: <code>(food* OR foodbo?rn* OR food-rela*)AND (diseas* OR hygien* OR allerg* OR diarrh?ea* ORnutrit* OR ...)</code></p></li>
</ol>
<p>Different websites and databases use different search functions to
return most relevant results given a query (e.g., “food safety”).
Ideally, a user has access to a full database and can apply the same
Python code based on regular expressions to all textual data. However,
this is not always possible (e.g., when using proprietary databases,
such as Web of Science). In those cases, it is important to follow the
conventions of the information retrieval system. For example, one source
might need phrases to be embedded in parentheses (i.e., <code>(x-ray crystallograph.*)</code>) while another database interface would require such phrases to be contained within quotation marks (i.e., <code>\``x-ray crystallograph.*´´</code>). It is then critical to explore the search tips and rules on those databases to ensure that the most complete information is gathered and further analyzed.</p>

<p><strong>Example code</strong></p>
<p>Python’s built-in <code>re</code> package provides all the capability needed to
construct and search with complex regular expressions. Listing 7.2
provides an example.</p>
<pre id="list:7.2" style="PythonStyle" numbers="none" float="b" label="list:7.2" caption="Python code to identify food safety related NSF awards with regular expressions"><code>def fs_regex(nsf_award_abstracts,outfilename):

  # Construct simple search string divided by search groups
  food = &quot;food.*&quot;
  general = &quot;safety|secur.*|insecur.*&quot;
  pathogens = &quot;toxicolog.*|acid-hydrolyzed vegetable protein.*|activated carbon.*&quot;
  process = &quot;process.*|preserv.*|fortif.*&quot; #and so on

  # Open csv table with all NSF award abstracts in 2000-2014
  inpfile = open(nsf_award_abstracts,&#39;rb&#39;)
  inpdata = csv.reader(infile)
  outfile = open(outfilename,&#39;wb&#39;)
  output = csv.writer(outfile)
  for line in inpdata:
    award_id = line[0]
    title = line[1]
    abstract = line[2]
    if re.search(food,abstract) and (re.search(general,abstract)
      or re.search(pathogens,abstract) or re.search(process,abstract)):
      output.writerow(i+[&#39;food safety award&#39;])</code></pre>
<div style="text-align: center">
Listing 7.2. Python code to identify food safety related NSF awards with regular expressions
</div>
<hr />
<p><strong>Algebraic models</strong></p>
<p>Algebraic models turn text into numbers to run mathematical operations
and discover inherent interdependencies between terms and phrases, also
defining the most important and meaningful among them. The vector space
representation is a typical way of converting words into numbers,
wherein every token is assigned with a sequential ID and a respective
weight, be it a simple term frequency, TFIDF value, or any other
assigned number.</p>
<p>Latent Dirichlet allocation, discussed in the preceding section, is a
good example of a probabilistic model, while unsupervised machine
learning techniques, such as random forest, can be used for
feature-based modeling and information retrieval.</p>
<p><strong>Similarity measures and approaches</strong></p>
<p>Based on algebraic models, the user can either compare documents between
each other or train a model that can be further inferred on a different
corpus. Typical metrics involved in this process include cosine
similarity and Kullback–Leibler divergence <span class="citation">(Kullback and Leibler <a href="#ref-kullback1951information">1951</a>)</span>.</p>
<p>Cosine similarity is a popular measure in document classification. Given
two documents <span class="math inline">\(d_a\)</span> and <span class="math inline">\(d_b\)</span> presented as term vectors
<span class="math inline">\(\overrightarrow{t_a}\)</span> and <span class="math inline">\(\overrightarrow{t_b}\)</span>, the cosine similarity
is</p>
<p><span class="math display">\[SIM_C(\overrightarrow{t_a},\overrightarrow{t_b}) = \frac{\overrightarrow{t_a} \cdot
     \overrightarrow{t_b}}{|\overrightarrow{t_a}|*|\overrightarrow{t_b}|}.\]</span></p>
<hr />
<p><strong>Example: Measuring cosine similarity between documents</strong></p>
<p>NSF awards are not labeled by scientific field—they are labeled by
program. This administrative classification is not always useful to
assess the effects of certain funding mechanisms on disciplines and
scientific communities. One approach is to understand how awards align
with each other even if they were funded by different programs. Cosine
similarity allows us to do just that.</p>
<p><strong>Example code</strong></p>
<p>The Python <code>numpy</code> module is a powerful library of tools for efficient linear
algebra computation. Among other things, it can be used to compute the
cosine similarity of two documents represented by numeric vectors, as
described above. The <code>gensim</code> module that is often used as a Python-based topic
modeling implementation can be used to produce vector space
representations of textual data.
Listing 7.3 provides an example of measuring cosine similarity
using these modules.</p>
<pre id="list:7.3" style="PythonStyleInline" basicstyle="\scriptsize\ttfamily" backgroundcolor="\color{codeBG}" label="list:7.3" caption="Python code to measure cosine similarity between Climate Change and all other Earth Science NSF awards"><code># Define cosine similarity function
def coss(v1,v2):
  return np.dot(v1,v2) /
        (np.sqrt(np.sum(np.square(v1))) *
         np.sqrt(np.sum(np.square(v2))))

def coss_nsf(nsf_climate_change,nsf_earth_science,outfile):

  # Open the source and compared to documents
  source = csv.reader(file(nsf_climate_change,&#39;rb&#39;))
  comparison = csv.reader(file(nsf_earth_science,&#39;rb&#39;))

  # Create an output file
  output = csv.writer(open(outfile,&#39;wb&#39;))

  # Read through the source and store value in static data container
  data = {}
  for row in source:
    award_id = row[0]
    abstract = row[1]
    data[award_id] = abstract

  # Read through the comparison file and compute similarity
  for row in comparison:
    award_id = row[0]
    # Assuming that abstract is cleaned, processed, tokenized, and
    # stored as a space-separated string of tokens
    abstract = row[1]
    abstract_for_dict = abstract.split(&quot; &quot;)
    # Construct dictionary of tokens and IDs
    dict_abstract = corpora.dictionary.Dictionary(abstract_for_dict)
    # Construct vector from dictionary
    # of all tokens and IDs in abstract
    abstr_vector = dict(dict_abstract.doc2bow(abstract))
    # Iterate through all stored abstracts in source corpus
    # and assign same token IDs using dictionary
    for key,value in data.items():
      source_id = key
      # Get all tokens from source abstract, assuming it is
      # tokenized and space-separated
      source_abstr = value.split(&quot; &quot;)
      source_vector = dict(dict_abstract.doc2bow(source_abstr))
      # Cosine similarity requires having same shape vectors.
      # Thus impute zeros for any missing tokens in source
      # abstract as compared to the target one
      add = { n:0 for n in abstr_vector.keys()
              if n not in source_dict.keys() }
      # Update source vector
      source_vector.update(add)
      source_vector = sorted(source_vector.items())
      abstr_vector = sorted(abstr_vector.items())
      # Compute cosine similarity
      similarity = coss(np.array([item[1] for item in abstr_vector]),
                        np.array([item[1] for item in source_dict])
      output.writerow([source_id,award_id,similarity])</code></pre>
<div style="text-align: center">
Listing 7.3. Python code to measure cosine similarity between Climate Change and all other Earth Science NSF awards
</div>
<hr />

<p>Kullback–Leibler (KL) divergence is an asymmetric measure that is often
enhanced by averaged calculations to ensure unbiased results when
comparing documents between each other or running a classification task.
Given two term vectors <span class="math inline">\(\overrightarrow{t_a}\)</span> and
<span class="math inline">\(\overrightarrow{t_b}\)</span>, the KL divergence from vector
<span class="math inline">\(\overrightarrow{t_a}\)</span> to <span class="math inline">\(\overrightarrow{t_b}\)</span> is
<span class="math display">\[D_{KL}(\overrightarrow{t_a}||\overrightarrow{t_b}) = \sum\limits_{t=1}^m w_{t,a}\times \log\left(\frac{w_{t,a}}{w_{t,b}}\right),\]</span>
where <span class="math inline">\(w_{t,a}\)</span> and <span class="math inline">\(w_{t,b}\)</span> are term weights in two vectors,
respectively.</p>
<p>An averaged KL divergence metric is then defined as
<span class="math display">\[D_{AvgKL}(\overrightarrow{t_a}||\overrightarrow{t_b}) = \sum\limits_{t=1}^m (\pi_1\times D(w_{t,a}||w_t)+\pi_2\times D(w_{t,b}||w_t)),\]</span>
where
<span class="math inline">\(\pi_1 = \frac{w_{t,a}}{w_{t,a}+w_{t,b}}, \pi_2 = \frac{w_{t,b}}{w_{t,a}+w_{t,b}}\)</span>,
and <span class="math inline">\(w_t = \pi_1\times w_{t,a} + \pi_2\times w_{t,b}\)</span> <span class="citation">(Huang <a href="#ref-huang-08">2008</a>)</span>.</p>
<p>A Python-based <code>scikit-learn</code> library provides an implementation of these measures as
well as other machine learning models and approaches.</p>
<p><strong>Knowledge repositories</strong></p>
<p>Information retrieval can be significantly enriched by the use of
established knowledge repositories that can provide enormous amounts of
organized empirical data for modeling and relevance calculations.
Established corpora, such as the Brown Corpus and
Lancaster–Oslo–Bergen Corpus, are one type of such preprocessed
repositories.</p>
<p>Wikipedia and WordNet are examples of another type of lexical and
semantic resources that are dynamic in nature and that can provide a
valuable basis for consistent and salient information retrieval and
clustering. These repositories have the innate hierarchy, or ontology,
of words (and concepts) that are explicitly linked to each other either
by inter-document links (Wikipedia) or by the inherent structure of the
repository (WordNet). In Wikipedia, concepts thus can be considered as
titles of individual Wikipedia pages and the contents of these pages can
be considered as their extended semantic representation.</p>
<p>Information retrieval techniques build on these advantages of WordNet
and Wikipedia. For example, Meij et al. <span class="citation">(Meij et al. <a href="#ref-meij-09">2009</a>)</span> mapped search queries
to the DBpedia ontology (derived from Wikipedia topics and their
relationships), and found that this mapping enriches the search queries
with additional context and concept relationships. One way of using
these ontologies is to retrieve a predefined list of Wikipedia pages
that would match a specific taxonomy. For example, scientific
disciplines are an established way of tagging documents— some are in
physics, others in chemistry, engineering, or computer science. If a
user retrieves four Wikipedia pages on “Physics,” “Chemistry,”
“Engineering,” and “Computer Science,” they can be further mapped to a
given set of scientific documents to label and classify them, such as a
corpus of award abstracts from the US National Science Foundation.</p>
<p><em>Personalized PageRank</em> is a similarity system that can help with the
task. This system uses WordNet to assess semantic relationships and
relevance between a search query (document <span class="math inline">\(d\)</span>) and possible results
(the most similar Wikipedia article or articles). This system has been
applied to text categorization <span class="citation">(Navigli et al. <a href="#ref-navigli-11">2011</a>)</span> by comparing documents to
<em>semantic model vectors</em> of Wikipedia pages constructed using WordNet.
These vectors account for the term frequency and their relative
importance given their place in the WordNet hierarchy, so that the
overall <span class="math inline">\(wiki\)</span> vector is defined as:</p>
<p><span class="math display">\[SMV_{wiki}(s) = \sum\nolimits_{w\in Synonyms(s)} \frac{tf_{wiki}(w)}{|Synsets(w)|}\]</span>,</p>
<p>where <span class="math inline">\(w\)</span> is a token within <span class="math inline">\(wiki\)</span>, <span class="math inline">\(s\)</span> is a WordNet synset that is
associated with every token <span class="math inline">\(w\)</span> in WordNet hierarchy, <span class="math inline">\(Synonyms(s)\)</span> is
the set of words (i.e., synonyms) in the synset <span class="math inline">\(s\)</span>, <span class="math inline">\(tf_{wiki}(w)\)</span> is
the term frequency of the word <span class="math inline">\(w\)</span> in the Wikipedia article <span class="math inline">\(wiki\)</span>, and
<span class="math inline">\(Synsets(w)\)</span> is the set of synsets for the word <span class="math inline">\(w\)</span>.</p>
<p>The overall probability of a candidate document <span class="math inline">\(d\)</span> (e.g., an NSF award
abstract or a PhD dissertation abstract) matching the target query, or
in our case a Wikipedia article <span class="math inline">\(wiki\)</span>, is
<span class="math display">\[wiki_{BEST}=\sum\nolimits_{w_t\in doc} \max_{s\in Synsets(w_t)} SMV_{wiki}(s),\]</span>
where <span class="math inline">\(Synsets(w_t)\)</span> is the set of synsets for the word <span class="math inline">\(w_t\)</span> in the
target document document (e.g., NSF award abstract) and <span class="math inline">\(SMV_{wiki}(s)\)</span>
is the semantic model vector of a Wikipedia page, as defined above.</p>
<p><strong>Applications</strong></p>
<p>Information retrieval can be used in a number of applications. Knowledge
discovery, or information extraction, is perhaps its primary mission; in
contrast, for users, the purpose of information retrieval applications
is to retrieve the most relevant response to a query.</p>
<p>Document classification is another popular task where information
retrieval methods can be helpful. Such systems, however, typically
require a two-step process: The first phase defines all relevant
information needed to answer the query. The second phase clusters the
documents according to a set of rules or by allowing the machine to
actively learn the patterns and classes. For example, one approach is to
generate a taxonomy of concepts with associated Wikipedia pages and then
map other documents to these pages through Personalized PageRank. In
this case, disciplines, such as physics, chemistry, and engineering, can
be used as the original labels, and NSF award abstracts can be mapped to
these disciplinary categories through the similarity metrics (i.e.,
whichever of these disciplines scores the highest is the most likely to
fit the disciplinary profile of an award abstract).</p>
<p>Another approach is to use the Wikipedia structure as a clustering
mechanism in itself. For example, the article about “nanotechnology”
links to a number of other Wikipedia pages as referenced in its content.
“Quantum realm,” “nanometer” or “National Nanotechnology Initiative” are
among the meaningful concepts used in the description of nanotechnology
that also have their own individual Wikipedia pages. Using these pages,
we can assume that if a scientific document, such as an NSF award
abstract, has enough similarity with any one of the articles associated
with nanotechnology, it can be tagged as such in the classification
exercise.</p>
<p>The process can also be turned around: if the user knows exactly the
clusters of documents in a given corpus, these can be mapped to an
external knowledge repository, such as Wikipedia, to discover yet
unknown and emerging relationships between concepts that are not
explicitly mentioned in the Wikipedia ontology at the current moment.
This situation is likely given the time lag between the discovery of new
phenomena, their introduction to the research community, and their
adoption by the wider user community responsible for writing Wikipedia
pages.</p>
<p><strong>Examples</strong></p>
<p>Some examples from our recent work can demonstrate how Wikipedia-based
labeling and labeled LDA
<span class="citation">(Ramage et al. <a href="#ref-ramage-09">2009</a>; Nguyen et al. <a href="#ref-Nguyen:Boyd-Graber:Resnik:Chang-2014">2014</a>)</span> cope with the task
of document classification and labeling in the scientific domain. See
Table <a href="chap-text.html#tab:table7-1">7.1</a>.</p>
<table>
<caption><span id="tab:table7-1">Table 7.1: </span> Wikipedia articles as potential labels generated by <span class="math inline">\(n\)</span>-gram indexing of NSF awards</caption>
<colgroup>
<col width="72%" />
<col width="13%" />
<col width="3%" />
<col width="10%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Abstract excerpt</strong></th>
<th><strong>ProQuest subject category</strong></th>
<th><strong>Labeled LDA</strong></th>
<th><strong>Wikipedia-based labeling</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Reconfigurable computing platform for smallscale resource-constrained robot.</strong> Specific applications often require robots of small size for reasons such as costs, access, and stealth. Smallscale robots impose constraints on resources such as power or space for modules…</td>
<td>Engineering, Electronics and Electrical; Engineering, Robotics</td>
<td>Motor controller</td>
<td>Robotics, Robot, Fieldprogrammable gate array</td>
</tr>
<tr class="even">
<td><strong>Genetic mechanisms of thalamic nuclei specification and the influence of thalamocortical axons in regulating neocortical area formation.</strong> Sensory information from the periphery is essential for all animal species to learn, adapt, and survive in their environment. The thalamus, a critical structure in the diencephalon, receives sensory information…</td>
<td>Biology, Neurobiology</td>
<td>HSD2 neurons</td>
<td>Sonic hedgehog, Induced stem cell, Nervous system</td>
</tr>
<tr class="odd">
<td><strong>Poetry ’n acts: The cultural politics of twentiethcentury American poets’ theater.</strong> This study focuses on the disciplinary blind spot that obscures the productive overlap between poetry and dramatic theater and prevents us from seeing the cultural work that this combination can perform…</td>
<td>Literature, American; Theater</td>
<td>Audience</td>
<td>Counterculture of the 1960s, Novel, Modernism</td>
</tr>
</tbody>
</table>
</div>
<div id="sec:other" class="section level3">
<h3><span class="header-section-number">7.3.3</span> Other approaches</h3>
<p>Our focus in this chapter is on approaches that are language independent
and require little (human) effort to analyze text data. In addition to
topic modeling and information retrieval discussed above, natural
language processing and computational linguistics are rich,
well-developed subdisciplines of computer science that can help analyze
text data. While covering these subfields is beyond this chapter, we
briefly discuss some of the most widely used approaches to process and
understand natural language texts.</p>
<p>In contrast to the <em>unsupervised</em> approaches discussed above, most
techniques in natural language processing are <em>supervised</em> machine
learning algorithms<a href="#fn45" class="footnote-ref" id="fnref45"><sup>45</sup></a>. Supervised machine learning produce labels <span class="math inline">\(y\)</span>
given inputs <span class="math inline">\(x\)</span>—the algorithm’s job is to learn how to automatically
produce correct labels given automatic inputs <span class="math inline">\(x\)</span>.</p>
<p>However, the algorithm must have access to many examples of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>,
often of the order of thousands of examples. This is expensive, as the
labels often require linguistic expertise <span class="citation">(Marcus, Santorini, and Marcinkiewicz <a href="#ref-marcus-93">1993</a>)</span>. While it is
possible to annotate data using crowdsourcing <span class="citation">(Snow et al. <a href="#ref-snow-08">2008</a>)</span>, this is not a
panacea, as it often forces compromises in the complexity of the task or
the quality of the labels.</p>
<p>In the sequel, we discuss how different definitions of <span class="math inline">\(x\)</span> and
<span class="math inline">\(y\)</span>—both in the scope and structure of the examples and
labels—define unique analyses of linguistic data.</p>

<p><strong>Document classification</strong></p>
<p>If the examples <span class="math inline">\(x\)</span> are documents and <span class="math inline">\(y\)</span> are what these documents are
about, the problem is called <em>document classification</em>. In contrast to
the techniques in Section <a href="chap-text.html#sec:lda" reference-type="ref" reference="sec:lda">7.3.1</a>, document classification is used when you know the
specific document types for which you are looking <em>and</em> you have many
examples of those document types.</p>
<p>One simple but ubiquitous example of document classification is spam
detection: an email is either an unwanted advertisement (spam) or it is
not. Document classification techniques such as naïve Bayes <span class="citation">(Lewis <a href="#ref-lewis-05">1998</a>)</span>
touch essentially every email sent worldwide, making email usable even
though most emails are spam.</p>

<p><strong>Sentiment analysis</strong></p>
<p>Instead of being what a document is about, a label <span class="math inline">\(y\)</span> could also reveal
the speaker. A recent subfield of natural language processing is to use
machine learning to reveal the internal state of speakers based on what
they say about a subject <span class="citation">(Pang and Lee <a href="#ref-pang-08">2008</a>)</span>. For example, given an example of
sentence <span class="math inline">\(x\)</span>, can we determine whether the speaker is a Liberal or a
Conservative? Is the speaker happy or sad?</p>
<p>Simple approaches use dictionaries and word counting methods
<span class="citation">(Pennebaker and Francis <a href="#ref-pennebaker-99">1999</a>)</span>, but more nuanced approaches make use of
<em>domain</em>-specific information to make better predictions. One uses
different approaches to praise a toaster than to praise an air
conditioner <span class="citation">(Blitzer, Dredze, and Pereira <a href="#ref-blitzer-07">2007</a>)</span>; liberals and conservatives each frame health
care differently from how they frame energy policy <span class="citation">(Nguyen, Boyd-Graber, and Resnik <a href="#ref-nguyen-13:shlda">2013</a>)</span>.</p>

<p><strong>Part-of-speech tagging</strong></p>
<p>When the examples <span class="math inline">\(x\)</span> are individual words and the labels <span class="math inline">\(y\)</span> represent
the grammatical function of a word (e.g., whether a word is a noun,
verb, or adjective), the task is called part-of-speech tagging. This
level of analysis can be useful for discovering simple patterns in text:
distinguishing between when “hit” is used as a noun (a Hollywood hit)
and when “hit” is used as a verb (the car hit the guard rail).</p>
<p>Unlike document classification, the examples <span class="math inline">\(x\)</span> are not independent:
knowing whether the previous word was an adjective makes it far more
likely that the next word will be a noun than a verb. Thus, the
classification algorithms need to incorporate structure into the
decisions. Two common algorithms for this problem are hidden Markov
models <span class="citation">(Rabiner <a href="#ref-rabiner-89">1989</a>)</span> and conditional random fields <span class="citation">(Lafferty, McCallum, and Pereira <a href="#ref-lafferty-01">2001</a>)</span>.</p>
</div>
</div>
<div id="sec:eval" class="section level2">
<h2><span class="header-section-number">7.4</span> Evaluation</h2>
<p>Evaluation techniques are common in economics, policy analysis, and
development. They allow researchers to justify their conclusions using
statistical means of validation and assessment. Text, however, is less
amenable to standard definitions of error: it is clear that predicting
that revenue will be $110 when it is really $100 is far better than
predicting $900; however, it is hard to say how far “potato harvest” is
from “journalism” if you are attempting to automatically label
documents. Documents are hard to transform into numbers without losing
semantic meanings and context.</p>
<p>Content analysis, discourse analysis, and bibliometrics are all common
tools used by social scientists in their text mining exercises
<span class="citation">(Stemler <a href="#ref-Stemler2001">2001</a>; Glänzel <a href="#ref-glanzel-12">2012</a>)</span>. However, they are rarely presented with
robust evaluation metrics, such as type I and type II error rates, when
retrieving data for further analysis<a href="#fn46" class="footnote-ref" id="fnref46"><sup>46</sup></a>. For example, bibliometricians
often rely on search strings derived from expert interviews and
workshops. However, it is hard to certify that those search strings are
optimal. For instance, in nanotechnology research, Porter et
al. <span class="citation">(Porter et al. <a href="#ref-porter-08">2008</a>)</span> developed a canonical search strategy for retrieving
nano-related papers from major scientific databases. Nevertheless,
others adopt their own search string modifications and claim similar
validity <span class="citation">(Terekhov <a href="#ref-terekhov-11">2012</a>; Guan and Ma <a href="#ref-guan-07">2007</a>)</span>.</p>
<p>Evaluating these methods depends on reference corpora. We discuss
metrics that help you understand whether a collection of documents for a
query is a good one or not or whether a labeling of a document
collection is consistent with an existing set of labels.</p>
<p><strong>Purity</strong></p>
<p>Suppose you are tasked with categorizing a collection of documents based
on what they are about. Reasonable people may disagree: I might put
“science and medicine” together, while another person may create
separate categories for “energy,” “scientific research,” and “health
care,” none of which is a strict subset of my “science and medicine”
category. Nevertheless, we still want to know whether two
categorizations are consistent.</p>
<p>Let us first consider the case where the labels differ but all
categories match (i.e., even though you call one category “taxes” and I
call it “taxation,” it has exactly the same constituent documents). This
should be the best case; it should have the highest score possible. Let
us say that this maximum score should be 1.</p>
<p>The opposite case is if we both simply assign labels randomly. There
will still be some overlap in our labeling: we will agree sometimes,
purely by chance. On average, if we both assign one label, selected from
the same set of <span class="math inline">\(K\)</span> labels, to each document, then we should expect to
agree on about <span class="math inline">\(\frac{1}{K}\)</span> of the labels. This is a lower bound on
performance.</p>
<p>The formalization of this measure is called <em>purity</em>: how much overlap
there is between each of my labels and the “best” match from your
labels. Box 7.2 shows how to calculate it.</p>
<div class="F00">
<p>
<strong>Box 7.2: Purity calculation</strong> We compute purity by assigning each cluster to the class that is most frequent in the cluster, and then measuring the accuracy of this assignment by counting correctly assigned documents and dividing by the number of all documents, <span class="math inline"><span class="math inline">\(N\)</span></span> <span class="citation"><span class="citation">(Manning, Raghavan, and Schütze <a href="#ref-manning2008">2008</a>)</span></span>. In formal terms, <span class="math display"><span class="math display">\[\mathrm{Purity}(\Omega,\mathbb{C}) = \frac{1}{N}\sum_{k} \max\limits_{j}|w_k\cap c_j|,\]</span></span> where <span class="math inline"><span class="math inline">\(\Omega = \{w_1, w_2,\ldots, w_k\}\)</span></span> is the set of candidate clusters and <span class="math inline"><span class="math inline">\(\mathbb{C} = \{c_1, c_2,\ldots, c_j\}\)</span></span> is the gold set of classes.
</p>
</div>
<p><strong>Precision and recall</strong></p>
<p>Chapter <a href="chap-ml.html#chap:ml">Machine Learning</a> already touched on the importance of precision and
recall for evaluating the results of information retrieval and machine
learning models (Box 7.3 provides a reminder of the formulae). Here we
look at a particular example of how these metrics can be computed when
working with scientific documents.</p>
<div class="F00">
<p>
<strong>Box 7.3: Precision and recall</strong> These two metrics are commonly used in information retrieval and computational linguistics <span class="citation"><span class="citation">(Resnik and Lin <a href="#ref-resnik-10b">2010</a>)</span></span>. Precision computes the type I errors—<em>false positives</em>—in a similar manner to the purity measure; it is formally defined as <span class="math display"><span class="math display">\[\mathrm{Precision} = \frac{|\{\mathrm{relevant\ documents}\}\cap \{\mathrm{retrieved\ documents}\}|}{|\{\mathrm{retrieved\ documents}\}|}.\]</span></span> Recall accounts for type II errors—<em>false negatives</em>—and is defined as <span class="math display"><span class="math display">\[\mathrm{Recall}=\frac{|\{\mathrm{relevant\ documents}\}\cap \{\mathrm{retrieved\ documents}\}|}{|\{\mathrm{relevant\ documents}\}|}.\]</span></span>
</p>
</div>
<p>We assume that a user has three sets of documents <span class="math inline">\(D_a =\{d_{a1},d_{a2},\ldots, d_n\}\)</span>, <span class="math inline">\(D_b=\{d_{b1}, d_{b2}, \ldots, d_k\}\)</span>, and <span class="math inline">\(D_c =\{d_{c1},d_{c2},\ldots,d_i\}\)</span>. All three sets are clearly tagged with a
disciplinary label: <span class="math inline">\(D_a\)</span> are computer science documents, <span class="math inline">\(D_b\)</span> are
physics, and <span class="math inline">\(D_c\)</span> are chemistry.</p>
<p>The user also has a different set of documents—Wikipedia pages on
“Computer Science,” “Chemistry,” and “Physics.” Knowing that all
documents in <span class="math inline">\(D_a\)</span>, <span class="math inline">\(D_b\)</span>, and <span class="math inline">\(D_c\)</span> have clear disciplinary
assignments, let us map the given Wikipedia pages to all documents
within those three sets. For example, the Wikipedia-based query on
“Computer Science” should return all computer science documents and none
in physics or chemistry. So, if the query based on the “Computer
Science” Wikipedia page returns only 50% of all computer science
documents, then 50% of the relevant documents are lost: the recall is
0.5.</p>
<p>On the other hand, if the same “Computer Science” query returns 50% of
all computer science documents but also 20% of the physics documents and
50% of the chemistry documents, then all of the physics and chemistry
documents returned are false positives. Assuming that all document sets
are of equal size, so that <span class="math inline">\(|D_a| = 10\)</span>, <span class="math inline">\(|D_b|=10\)</span> and <span class="math inline">\(|D_c| = 10\)</span>,
then the precision is <span class="math inline">\(\frac{5}{12} = 0.42\)</span>.</p>
<p><strong><em>F</em> score</strong></p>
<p>The <em>F score</em> takes precision and recall measures a step further and
considers the general accuracy of the model. In formal terms, the <span class="math inline">\(F\)</span>
score is a weighted average of the precision and recall:
<span class="math display">\[\label{eq:text:F1}
F_1 = 2\cdot \frac{\mathrm{Precision}\cdot \mathrm{Recall}}{\mathrm{Precision}+\mathrm{Recall}}.\]</span>
In terms of type I and type II errors:
<span class="math display">\[F_\beta = \frac{(1+\beta^2)\cdot \mathrm{true\ positive}}{(1+\beta^2)\cdot \mathrm{true\ positive} + \beta^2\cdot \mathrm{false\ negative} + \mathrm{false\ positive}},\]</span>
where <span class="math inline">\(\beta\)</span> is the balance between precision and recall. Thus, <span class="math inline">\(F_2\)</span>
puts more emphasis on the recall measure and <span class="math inline">\(F_{0.5}\)</span> puts more
emphasis on precision.</p>
</div>
<div id="text-analysis-tools" class="section level2">
<h2><span class="header-section-number">7.5</span> Text analysis tools</h2>
<p>We are fortunate to have access to a set of powerful open source text
analysis tools. We describe three here.</p>
<p><strong>The Natural Language Toolkit</strong></p>
<p>The NLTK is a commonly used natural language toolkit that provides a
large number of relevant solutions for text analysis. It is Python-based
and can be easily integrated into data processing and analytical scripts
by a simple <code>import nltk</code> (or similar for any one of its submodules).</p>
<p>The NLTK includes a set of tokenizers, stemmers, lemmatizers and other
natural language processing tools typically applied in text analysis and
machine learning. For example, a user canextract tokens from a document
<em>doc</em> by running the command <code>tokens = nltk.word_tokenize(doc)</code>.</p>
<p>Useful text corpora are also present in the NLTK distribution. For
example, the stop words list can be retrieved by running the command <code>stops=nltk.corpus.stopwords.words(language)</code>. These stop words are available for several languages within NTLK, including English, French, and Spanish.</p>
<p>Similarly, the Brown Corpus or WordNet can be called by running <code>from nltk.corpus import wordnet/brown</code>. After the corpora are loaded, their various properties can be explored and used in text analysis; for example, <code>dogsyn = wordnet.synsets('dog')</code> will return a list of WordNet synsets related to the word “dog.”</p>
<p>Term frequency distribution and <span class="math inline">\(n\)</span>-gram indexing are other techniques
implemented in NLTK. For example, a user can compute frequency
distribution of individual terms within a document <em>doc</em> by running a
command in Python: <code>fdist=nltk.FreqDist(text)</code>. This command returns a dictionary of all tokens with associated frequency within <em>doc</em>.</p>
<p><span class="math inline">\(N\)</span>-gram indexing is implemented as a chain-linked collocations
algorithm that takes into account the probability of any given two,
three, or more words appearing together in the entire corpus. In
general, <span class="math inline">\(n\)</span>-grams can be discovered as easily as running <code>bigrams = nltk.bigrams(text)</code>. However, a more sophisticated approach is needed to discover statistically significant word collocations, as we show in Listing 7.4.</p>
<p>Bird et al. <span class="citation">(Bird, Klein, and Loper <a href="#ref-bird-09">2009</a>)</span> provide a detailed description of NLTK tools and
techniques. See also the official NLTK website <span class="citation">(NLTK Project, <a href="#ref-NLTKweb">n.d.</a>)</span>.</p>
<pre id="list:7.4" style="PythonStyle" numbers="none" label="list:7.4" caption="Python code to find bigrams using NLTK"><code>def bigram_finder(texts):
  # NLTK bigrams from a corpus of documents separated by new line
  tokens_list = nltk.word_tokenize(re.sub(&quot;\n&quot;,&quot; &quot;,texts))
  bgm    = nltk.collocations.BigramAssocMeasures()
  finder = nltk.collocations.BigramCollocationFinder.from_words(tokens_list)
  scored = finder.score_ngrams( bgm.likelihood_ratio  )

  # Group bigrams by first word in bigram.
  prefix_keys = collections.defaultdict(list)
  for key, scores in scored:
      prefix_keys[key[0]].append((key[1], scores))

  # Sort keyed bigrams by strongest association.
  for key in prefix_keys:
      prefix_keys[key].sort(key = lambda x: -x[1])</code></pre>
<div style="text-align: center">
Listing 7.4. Python code to find bigrams using NLTK
</div>
<p><strong>Stanford CoreNLP</strong></p>
<p>While NLTK’s emphasis is on simple reference implementations, Stanford’s
CoreNLP <span class="citation">(Stanford, <a href="#ref-corenlp">n.d.</a>; Manning et al. <a href="#ref-manning2014stanford">2014</a>)</span> is focused on fast
implementations of cutting-edge algorithms, particularly for syntactic
analysis (e.g., determining the subject of a sentence).</p>
<p><strong>MALLET</strong></p>
<p>For probabilistic models of text, MALLET, the MAchine Learning for
LanguagE Toolkit <span class="citation">(McCallum <a href="#ref-mallet">2002</a>)</span>, often strikes the right balance between
usefulness and usability. It is written to be fast and efficient but
with enough documentation and easy enough interfaces to be used by
novices. It offers fast, popular implementations of conditional random
fields (for part-of- speech tagging), text classification, and topic
modeling.</p>
</div>
<div id="summary-4" class="section level2">
<h2><span class="header-section-number">7.6</span> Summary</h2>
<p>Much “big data” of interest to social scientists is text: tweets,
Facebook posts, corporate emails, and the news of the day. However, the
meaning of these documents is buried beneath the ambiguities and
noisiness of the informal, inconsistent ways by which humans communicate
with each other. Despite attempts to formalize the meaning of text data
through asking users to tag people, apply metadata, or to create
structured representations, these attempts to manually curate meaning
are often incomplete, inconsistent, or both.</p>
<p>These aspects make text data difficult to work with, but also a
rewarding object of study. Unlocking the meaning of a piece of text
helps bring machines closer to human-level intelligence—as language is
one of the most quintessentially human activities—and helps overloaded
information professionals do their jobs more effectively: understand
large corpora, find the right documents, or automate repetitive tasks.
And as an added bonus, the better computers become at understanding
natural language, the easier it is for information professionals to
communicate their needs: one day using computers to grapple with big
data may be as natural as sitting down to a conversation over coffee
with a knowledgeable, trusted friend.</p>
</div>
<div id="resources-3" class="section level2">
<h2><span class="header-section-number">7.7</span> Resources</h2>
<p>Text analysis is one of the more complex tasks in big data analysis.
Because it is unstructured, text (and natural language overall) requires
significant processing and cleaning before we can engage in interesting
analysis and learning. In this chapter we have referenced several
resources that can be helpful in mastering text mining techniques:</p>
<ul>
<li><p>The Natural Language Toolkit is one of the most popular Python-based
tools for natural language processing. It has a variety of methods
and examples that are easily accessible online <span class="citation">(NLTK Project, <a href="#ref-NLTKweb">n.d.</a>)</span>. The book
by Bird et al. <span class="citation">(Bird, Klein, and Loper <a href="#ref-bird-09">2009</a>)</span>, available online, contains multiple
examples and tips on how to use NLTK.</p></li>
<li><p>The book <em>Pattern Recognition and Machine Learning</em> by Christopher
Bishop <span class="citation">(Bishop <a href="#ref-bishop-06">2006</a>)</span> is a useful introduction to computational
techniques, including probabilistic methods, text analysis, and
machine learning. It has a number of tips and examples that are
helpful to both learning and experienced researchers.</p></li>
<li><p>A paper by Anna Huang <span class="citation">(Huang <a href="#ref-huang-08">2008</a>)</span> provides a brief overview of the
key similarity measures for text document clustering discussed in
this chapter, including their strengths and weaknesses in different
contexts.</p></li>
<li><p>Materials at the MALLET website <span class="citation">(McCallum <a href="#ref-mallet">2002</a>)</span> can be specialized for the
unprepared reader but are helpful when looking for specific
solutions with topic modeling and machine classification using this
toolkit.</p></li>
<li><p>David Blei, one of the authors of the latent Dirichlet allocation
algorithm (topic modeling), maintains a helpful web page with
introductory resources for those interested in topic modeling
<span class="citation">(Blei, <a href="#ref-BleiTM">n.d.</a>)</span>.</p></li>
<li><p>We provide an example of how to run topic modeling using MALLET on
textual data from the National Science Foundation and Norwegian
Research Council award abstracts <span class="citation">(Boyd-Graber, <a href="#ref-NSFsearch">n.d.</a>)</span>.</p></li>
<li><p>Weka, developed at the University of Waikato in New Zealand, is a
useful resource for running both complex text analysis and other
machine learning tasks and evaluations <span class="citation">(M. Hall et al. <a href="#ref-hall2009weka">2009</a>; University of Waikato, <a href="#ref-WekaWeb">n.d.</a>)</span>.</p></li>
</ul>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-brody2015">
<p>Brody, C., T. de Hoop, M. Vojtkova, R. Warnock, M. Dunbar, P. Murthy, and S. L. Dworkin. 2015. “Economic Self-Help Group Programs for Women’s Empowerment: A Systematic Review.” <em>Campbell Systematic Reviews</em> 11 (19).</p>
</div>
<div id="ref-oecd2005measurement">
<p>Mortensen, Peter Stendahl, Carter Walter Bloch, and others. 2005. <em>Oslo Manual: Guidelines for Collecting and Interpreting Innovation Data</em>. Organisation for Economic Co-operation; Development.</p>
</div>
<div id="ref-manual2004summary">
<p>Economic Co-operation, Organisation of, and Development. 2004. “A Summary of the Frascati Manual.” <em>Main Definitions and Conventions for the Measurement of Research and Experimental Development</em> 84.</p>
</div>
<div id="ref-talley2011database">
<p>Talley, Edmund M., David Newman, David Mimno, Bruce W. Herr II, Hanna M. Wallach, Gully A. P. C. Burns, A. G. Miriam Leenders, and Andrew McCallum. 2011. “Database of NIH Grants Using Machine-Learned Categories and Graphical Clustering.” <em>Nature Methods</em> 8 (6). Nature Publishing Group: 443–44.</p>
</div>
<div id="ref-wang-09">
<p>Wang, Yi, Hongjie Bai, Matt Stanton, Wen-Yen Chen, and Edward Y. Chang. 2009. “PLDA: Parallel Latent Dirichlet Allocation for Large-Scale Applications.” In <em>International Conference on Algorithmic Aspects in Information and Management</em>.</p>
</div>
<div id="ref-halevy-09">
<p>Halevy, Alon, Peter Norvig, and Fernando Pereira. 2009. “The Unreasonable Effectiveness of Data.” <em>IEEE Intelligent Systems</em> 24 (2). Piscataway, NJ: IEEE Educational Activities Department: 8–12.</p>
</div>
<div id="ref-niculae-15">
<p>Niculae, Vlad, Srijan Kumar, Jordan Boyd-Graber, and Cristian Danescu-Niculescu-Mizil. 2015. “Linguistic Harbingers of Betrayal: A Case Study on an Online Strategy Game.” In <em>Association for Computational Linguistics</em>. Beijing, China. <a href="docs/2015_acl_diplomacy.pdf" class="uri">docs/2015_acl_diplomacy.pdf</a>.</p>
</div>
<div id="ref-ott-11">
<p>Ott, Myle, Yejin Choi, Claire Cardie, and Jeffrey T. Hancock. 2011. “Finding Deceptive Opinion Spam by Any Stretch of the Imagination.” In <em>Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies—Volume 1</em>, 309–19. HLT ’11. Stroudsburg, PA: Association for Computational Linguistics. <a href="http://dl.acm.org/citation.cfm?id=2002472.2002512" class="uri">http://dl.acm.org/citation.cfm?id=2002472.2002512</a>.</p>
</div>
<div id="ref-kong-14">
<p>Kong, Lingpeng, Nathan Schneider, Swabha Swayamdipta, Archna Bhatia, Chris Dyer, and Noah A. Smith. 2014. “A Dependency Parser for Tweets.” In <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (Emnlp)</em>, 1001–12. Association for Computational Linguistics. <a href="http://www.aclweb.org/anthology/D14-1108" class="uri">http://www.aclweb.org/anthology/D14-1108</a>.</p>
</div>
<div id="ref-nguyen-12">
<p>Nguyen, Viet-An, Jordan Boyd-Graber, and Philip Resnik. 2012. “SITS: A Hierarchical Nonparametric Model Using Speaker Identity for Topic Segmentation in Multiparty Conversations.” In <em>Proceedings of the Association for Computational Linguistics</em>. Jeju, South Korea.</p>
</div>
<div id="ref-Nguyen:Boyd-Graber:Resnik:Miler-2015">
<p>Nguyen, Viet-An, Jordan Boyd-Graber, Philip Resnik, and Kristina Miler. 2015. “Tea Party in the House: A Hierarchical Ideal Point Topic Model and Its Application to Republican Legislators in the 112th Congress.” In <em>Association for Computational Linguistics</em>. Beijing, China.</p>
</div>
<div id="ref-tuarob-13">
<p>Tuarob, Suppawong, Line C. Pouchard, and C. Lee Giles. 2013. “Automatic Tag Recommendation for Metadata Annotation Using Probabilistic Topic Modeling.” In <em>Proceedings of the 13th Acm/Ieee-Cs Joint Conference on Digital Libraries</em>, 239–48. JCDL ’13. ACM. <a href="https://doi.org/10.1145/2467696.2467706" class="uri">https://doi.org/10.1145/2467696.2467706</a>.</p>
</div>
<div id="ref-Mukherjea-05">
<p>Mukherjea, Sougata. 2005. “Information Retrieval and Knowledge Discovery Utilising a Biomedical Semantic Web.” <em>Briefings in Bioinformatics</em> 6 (3): 252–62. <a href="https://doi.org/10.1093/bib/6.3.252" class="uri">https://doi.org/10.1093/bib/6.3.252</a>.</p>
</div>
<div id="ref-navigli-11">
<p>Navigli, Roberto, Stefano Faralli, Aitor Soroa, Oier de Lacalle, and Eneko Agirre. 2011. “Two Birds with One Stone: Learning Semantic Models for Text Categorization and Word Sense Disambiguation.” In <em>Proceedings of the 20th Acm International Conference on Information and Knowledge Management</em>. ACM.</p>
</div>
<div id="ref-pang-08">
<p>Pang, Bo, and Lillian Lee. 2008. <em>Opinion Mining and Sentiment Analysis</em>. Paperback; Now Publishers.</p>
</div>
<div id="ref-bron-11">
<p>Bron, Marc, Bouke Huurnink, and Maarten de Rijke. 2011. “Linking Archives Using Document Enrichment and Term Selection.” In <em>Proceedings of the 15th International Conference on Theory and Practice of Digital Libraries: Research and Advanced Technology for Digital Libraries</em>, 360–71. Springer.</p>
</div>
<div id="ref-browncorpus">
<p>Francis, W. N., and H. Kucera. 1979. “Brown Corpus Manual.” Department of Linguistics, Brown University, Providence, Rhode Island, US. <a href="http://icame.uib.no/brown/bcm.html" class="uri">http://icame.uib.no/brown/bcm.html</a>.</p>
</div>
<div id="ref-bnc">
<p>University of Oxford. 2006. “British National Corpus.” <a href="http://www.natcorp.ox.ac.uk/" class="uri">http://www.natcorp.ox.ac.uk/</a>.</p>
</div>
<div id="ref-marcus-93">
<p>Marcus, Mitchell P., Beatrice Santorini, and Mary A. Marcinkiewicz. 1993. “Building a Large Annotated Corpus of English: The Penn Treebank.” <em>Computational Linguistics</em> 19 (2): 313–30.</p>
</div>
<div id="ref-bird-09">
<p>Bird, Steven, Ewan Klein, and Edward Loper. 2009. <em>Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit</em>. O’Reilly Media.</p>
</div>
<div id="ref-kiss-06">
<p>Kiss, Tibor, and Jan Strunk. 2006. “Unsupervised Multilingual Sentence Boundary Detection.” <em>Computational Linguistics</em> 32 (4). Cambridge, MA: MIT Press: 485–525.</p>
</div>
<div id="ref-malmkjar-02">
<p>Malmkjær, K. 2002. <em>The Linguistics Encyclopedia</em>. Routledge. <a href="https://books.google.ca/books?id=uCrXOLvD7fMC" class="uri">https://books.google.ca/books?id=uCrXOLvD7fMC</a>.</p>
</div>
<div id="ref-Dunning-93">
<p>Dunning, Ted. 1993. “Accurate Methods for the Statistics of Surprise and Coincidence.” <em>Computational Linguistics</em> 19 (1). Cambridge, MA: MIT Press: 61–74. <a href="http://dl.acm.org/citation.cfm?id=972450.972454" class="uri">http://dl.acm.org/citation.cfm?id=972450.972454</a>.</p>
</div>
<div id="ref-salton-68">
<p>Salton, Gerard. 1968. <em>Automatic Information Organization and Retrieval.</em> McGraw-Hill.</p>
</div>
<div id="ref-blei-03">
<p>Blei, David M., Andrew Ng, and Michael Jordan. 2003. “Latent Dirichlet Allocation.” <em>Journal of Machine Learning Research</em> 3: 993–1022.</p>
</div>
<div id="ref-hofmann-99">
<p>Hofmann, Thomas. 1999. “Probabilistic Latent Semantic Analysis.” In <em>Proceedings of Uncertainty in Artificial Intelligence</em>.</p>
</div>
<div id="ref-landauer-97">
<p>Landauer, T., and S. Dumais. 1997. “Solutions to Plato’s Problem: The Latent Semantic Analysis Theory of Acquisition, Induction and Representation of Knowledge.” <em>Psychological Review</em> 104 (2): 211–40.</p>
</div>
<div id="ref-blei-09">
<p>Blei, David M., and John Lafferty. 2009. “Topic Models.” In <em>Text Mining: Theory and Applications</em>, edited by Ashok Srivastava and Mehran Sahami. Taylor &amp; Francis.</p>
</div>
<div id="ref-sandhaus-08">
<p>Sandhaus, Evan. 2008. “The New York Times Annotated Corpus.” Philadelphia: <em>Linguistic Data Consortium</em>, <a href="http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp? catalogId=LDC2008T19" class="uri">http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp? catalogId=LDC2008T19</a>.</p>
</div>
<div id="ref-geman-90">
<p>Geman, S., and D. Geman. 1990. “Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images.” In <em>Readings in Uncertain Reasoning</em>, edited by Glenn Shafer and Judea Pearl, 452–72. Morgan Kaufmann.</p>
</div>
<div id="ref-wallach-09b">
<p>Wallach, Hanna, David Mimno, and Andrew McCallum. 2009. “Rethinking LDA: Why Priors Matter.” In <em>Advances in Neural Information Processing Systems</em>.</p>
</div>
<div id="ref-griffiths-04">
<p>Griffiths, Thomas L., and Mark Steyvers. 2004. “Finding Scientific Topics.” <em>Proceedings of the National Academy of Sciences</em> 101 (Suppl. 1): 5228–35.</p>
</div>
<div id="ref-nelson-10">
<p>Nelson, Robert K. 2010. “Mining the Dispatch.” <a href="http://dsl.richmond.edu/dispatch/" class="uri">http://dsl.richmond.edu/dispatch/</a>.</p>
</div>
<div id="ref-maskeri-08">
<p>Maskeri, Girish, Santonu Sarkar, and Kenneth Heafield. 2008. “Mining Business Topics in Source Code Using Latent Dirichlet Allocation.” In <em>Proceedings of the 1st India Software Engineering Conference</em>, 113–20. ACM. <a href="https://doi.org/http://doi.acm.org/10.1145/1342211.1342234" class="uri">https://doi.org/http://doi.acm.org/10.1145/1342211.1342234</a>.</p>
</div>
<div id="ref-Hu:Zhai:Eidelman:Boyd-Graber-2014">
<p>Hu, Yuening, Ke Zhai, Vlad Eidelman, and Jordan Boyd-Graber. 2014. “Polylingual Tree-Based Topic Models for Translation Domain Adaptation.” In <em>Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</em>. Baltimore, MD.</p>
</div>
<div id="ref-wang-09b">
<p>Wang, Chong, David Blei, and Li Fei-Fei. 2009. “Simultaneous Image Classification and Annotation.” In <em>Computer Vision and Pattern Recognition</em>.</p>
</div>
<div id="ref-paul-10">
<p>Paul, Michael, and Roxana Girju. 2010. “A Two-Dimensional Topic-Aspect Model for Discovering Multi-Faceted Topics.” In <em>Association for the Advancement of Artificial Intelligence</em>.</p>
</div>
<div id="ref-lee:boolean-88">
<p>Lee, Whay C., and Edward A. Fox. 1988. “Experimental Comparison of Schemes for Interpreting Boolean Queries.” TR-88-27. Computer Science, Virginia Polytechnic Institute; State University.</p>
</div>
<div id="ref-kullback1951information">
<p>Kullback, Solomon, and Richard A. Leibler. 1951. “On Information and Sufficiency.” <em>Annals of Mathematical Statistics</em> 22 (1). JSTOR: 79–86.</p>
</div>
<div id="ref-huang-08">
<p>Huang, Anna. 2008. “Similarity Measures for Text Document Clustering.” Paper presented at New Zealand Computer Science Research Student Conference, Christchurch, New Zealand, April 14–18.</p>
</div>
<div id="ref-meij-09">
<p>Meij, Edgar, Marc Bron, Laura Hollink, Bouke Huurnink, and Maarten Rijke. 2009. “Learning Semantic Query Suggestions.” In <em>Proceedings of the 8th International Semantic Web Conference</em>, 424–40. ISWC ’09. Chantilly, VA: Springer. <a href="https://doi.org/10.1007/978-3-642-04930-9_27" class="uri">https://doi.org/10.1007/978-3-642-04930-9_27</a>.</p>
</div>
<div id="ref-ramage-09">
<p>Ramage, Daniel, David Hall, Ramesh Nallapati, and Christopher Manning. 2009. “Labeled LDA: A Supervised Topic Model for Credit Attribution in Multi-Labeled Corpora.” In <em>Proceedings of Empirical Methods in Natural Language Processing</em>.</p>
</div>
<div id="ref-Nguyen:Boyd-Graber:Resnik:Chang-2014">
<p>Nguyen, Viet-An, Jordan Boyd-Graber, Philip Resnik, and Jonathan Chang. 2014. “Learning a Concept Hierarchy from Multi-Labeled Documents.” In <em>Proceedings of the Annual Conference on Neural Information Processing Systems</em>. Morgan Kaufmann.</p>
</div>
<div id="ref-snow-08">
<p>Snow, Rion, Brendan O’Connor, Daniel Jurafsky, and Andrew Ng. 2008. “Cheap and Fast—but Is It Good? Evaluating Non-Expert Annotations for Natural Language Tasks.” In <em>Proceedings of Empirical Methods in Natural Language Processing</em>.</p>
</div>
<div id="ref-lewis-05">
<p>Lewis, David D. 1998. “Naive (Bayes) at Forty: The Independence Assumption in Information Retrieval.” In <em>Proceedings of European Conference of Machine Learning</em>, 4–15.</p>
</div>
<div id="ref-pennebaker-99">
<p>Pennebaker, James W., and Martha E. Francis. 1999. <em>Linguistic Inquiry and Word Count</em>. Loose Leaf; Lawrence Erlbaum.</p>
</div>
<div id="ref-blitzer-07">
<p>Blitzer, John, Mark Dredze, and Fernando Pereira. 2007. “Biographies, Bollywood, Boom-Boxes and Blenders: Domain Adaptation for Sentiment Classification.” In <em>Proceedings of the Association for Computational Linguistics</em>.</p>
</div>
<div id="ref-nguyen-13:shlda">
<p>Nguyen, Viet-An, Jordan Boyd-Graber, and Philip Resnik. 2013. “Lexical and Hierarchical Topic Regression.” In <em>Advances in Neural Information Processing Systems</em>. Lake Tahoe, Nevada.</p>
</div>
<div id="ref-rabiner-89">
<p>Rabiner, Lawrence R. 1989. “A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.” <em>Proceedings of the IEEE</em> 77 (2): 257–86. <a href="https://doi.org/10.1109/5.18626" class="uri">https://doi.org/10.1109/5.18626</a>.</p>
</div>
<div id="ref-lafferty-01">
<p>Lafferty, John D., Andrew McCallum, and Fernando C. N. Pereira. 2001. “Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.” In <em>Proceedings of the Eighteenth International Conference on Machine Learning</em>, 282–89. Morgan Kaufmann.</p>
</div>
<div id="ref-Stemler2001">
<p>Stemler, Steve. 2001. “An Overview of Content Analysis.” <em>Practical Assessment, Research &amp; Evaluation</em> 7 (17). <a href="http://pareonline.net/getvn.asp?v=7&amp;n=17" class="uri">http://pareonline.net/getvn.asp?v=7&amp;n=17</a>.</p>
</div>
<div id="ref-glanzel-12">
<p>Glänzel, Wolfgang. 2012. “Bibliometric Methods for Detecting and Analysing Emerging Research Topics.” <em>El Profesional de La Información</em> 21 (1): 194–201. <a href="http://eprints.rclis.org/16947/" class="uri">http://eprints.rclis.org/16947/</a>.</p>
</div>
<div id="ref-porter-08">
<p>Porter, Alan L., Jan Youtie, Philip Shapira, and David J. Schoeneck. 2008. “Refining Search Terms for Nanotechnology.” <em>Journal of Nanoparticle Research</em> 10 (5). Springer Netherlands: 715–28. <a href="https://doi.org/10.1007/s11051-007-9266-y" class="uri">https://doi.org/10.1007/s11051-007-9266-y</a>.</p>
</div>
<div id="ref-terekhov-11">
<p>Terekhov, Alexander I. 2012. “Evaluating the Performance of Russia in the Research in Nanotechnology.” <em>Journal of Nanoparticle Research</em> 14 (11). Springer Netherlands. <a href="https://doi.org/10.1007/s11051-012-1250-5" class="uri">https://doi.org/10.1007/s11051-012-1250-5</a>.</p>
</div>
<div id="ref-guan-07">
<p>Guan, Jiancheng, and Nan Ma. 2007. “China’s Emerging Presence in Nanoscience and Nanotechnology: A Comparative Bibliometric Study of Several Nanoscience ‘Giants’.” <em>Research Policy</em> 36 (6): 880–86. <a href="http://EconPapers.repec.org/RePEc:eee:respol:v:36:y:2007:i:6:p:880-886" class="uri">http://EconPapers.repec.org/RePEc:eee:respol:v:36:y:2007:i:6:p:880-886</a>.</p>
</div>
<div id="ref-manning2008">
<p>Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schütze. 2008. <em>Introduction to Information Retrieval</em>. Cambridge University Press.</p>
</div>
<div id="ref-resnik-10b">
<p>Resnik, Philip, and Jimmy Lin. 2010. “Evaluation of NLP Systems.” In <em>Handbook of Computational Linguistics and Natural Language Processing</em>, edited by Alex Clark, Chris Fox, and Shalom Lappin. Wiley Blackwell.</p>
</div>
<div id="ref-NLTKweb">
<p>NLTK Project. n.d. “NLTK: The Natural Language Toolkit.” <a href="http://www.nltk.org" class="uri">http://www.nltk.org</a>. Accessed February 1, 2016.</p>
</div>
<div id="ref-corenlp">
<p>Stanford. n.d. “Stanford CoreNLP—a Suite of Core NLP Tools.” <a href="http://nlp.stanford.edu/software/corenlp.shtml" class="uri">http://nlp.stanford.edu/software/corenlp.shtml</a>. Accessed February 1, 2016.</p>
</div>
<div id="ref-manning2014stanford">
<p>Manning, Christopher D., Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. “The Stanford CoreNLP Natural Language Processing Toolkit.” In <em>Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</em>, 55–60.</p>
</div>
<div id="ref-mallet">
<p>McCallum, Andrew Kachites. 2002. “MALLET: A Machine Learning for Language Toolkit.” <a href="http://mallet.cs.umass.edu" class="uri">http://mallet.cs.umass.edu</a>.</p>
</div>
<div id="ref-bishop-06">
<p>Bishop, Christopher M. 2006. <em>Pattern Recognition and Machine Learning</em>. Springer.</p>
</div>
<div id="ref-BleiTM">
<p>Blei, David M. n.d. “Topic Modeling.” <a href="http://www.cs.columbia.edu/~blei/topicmodeling.html" class="uri">http://www.cs.columbia.edu/~blei/topicmodeling.html</a>. Accessed February 1, 2016.</p>
</div>
<div id="ref-NSFsearch">
<p>Boyd-Graber, Jordan. n.d. <a href="http://www.umiacs.umd.edu/~jbg/lda_demo" class="uri">http://www.umiacs.umd.edu/~jbg/lda_demo</a>. Accessed February 1, 2016.</p>
</div>
<div id="ref-hall2009weka">
<p>Hall, Mark, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H Witten. 2009. “The Weka Data Mining Software: An Update.” <em>ACM SIGKDD Explorations Newsletter</em> 11 (1). ACM: 10–18.</p>
</div>
<div id="ref-WekaWeb">
<p>University of Waikato. n.d. “Weka 3: Data Mining Software in Java.” <a href="http://www.cs.waikato.ac.nz/ml/weka/" class="uri">http://www.cs.waikato.ac.nz/ml/weka/</a>. Accessed February 1, 2016.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="40">
<li id="fn40"><p>See Chapter 3.<a href="chap-text.html#fnref40" class="footnote-back">↩</a></p></li>
<li id="fn41"><p>See Chapter 6 for a discussion of speech recognition, which can turn spoken
language into text.<a href="chap-text.html#fnref41" class="footnote-back">↩</a></p></li>
<li id="fn42"><p>Classification, a machine
learning method, is discussed in Chapter 6.<a href="chap-text.html#fnref42" class="footnote-back">↩</a></p></li>
<li id="fn43"><p>Cleaning and processing are discussed extensively in
Chapter 3.<a href="chap-text.html#fnref43" class="footnote-back">↩</a></p></li>
<li id="fn44"><p>Term weighting is an example of feature engineering discussed in Chapter 6.<a href="chap-text.html#fnref44" class="footnote-back">↩</a></p></li>
<li id="fn45"><p>Chapter 6 reviews supervised machine learning approaches.<a href="chap-text.html#fnref45" class="footnote-back">↩</a></p></li>
<li id="fn46"><p>Chapter 10 discusses
how to measure and diagnose errors in big data.<a href="chap-text.html#fnref46" class="footnote-back">↩</a></p></li>
</ol>
</div>
<div id="disqus_thread"></div>
<script>
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = '//big-data-and-social-science.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the
<a href="https://disqus.com/?ref_noscript">
  comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<a href="chap-ml.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chap-networks.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/Coleridge-Initiative/big-data-and-social-science/edit/master/07-TextChapter.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
