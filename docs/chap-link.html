<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Record Linkage | Big Data and Social Science</title>
  <meta name="description" content="Chapter 3 Record Linkage | Big Data and Social Science" />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Record Linkage | Big Data and Social Science" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="Coleridge-Initiative/big-data-and-social-science" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Record Linkage | Big Data and Social Science" />
  
  
  

<meta name="author" content="Ian Foster, Rayid Ghani, Ron S. Jarmin, Frauke Kreuter and Julia Lane" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chap-web.html"/>
<link rel="next" href="chap-db.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157005492-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157005492-1');
</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Big Data and Social Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface to the 2nd edition</a></li>
<li class="chapter" data-level="1" data-path="chap-intro.html"><a href="chap-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-1"><i class="fa fa-check"></i><b>1.1</b> Why this book?</a></li>
<li class="chapter" data-level="1.2" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-2"><i class="fa fa-check"></i><b>1.2</b> Defining big data and its value</a></li>
<li class="chapter" data-level="1.3" data-path="chap-intro.html"><a href="chap-intro.html#sec:1.3"><i class="fa fa-check"></i><b>1.3</b> The importance of inference</a></li>
<li class="chapter" data-level="1.4" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-4"><i class="fa fa-check"></i><b>1.4</b> The importance of understanding how data are generated</a></li>
<li class="chapter" data-level="1.5" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-5"><i class="fa fa-check"></i><b>1.5</b> New tools for new data</a></li>
<li class="chapter" data-level="1.6" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-6"><i class="fa fa-check"></i><b>1.6</b> The book’s “use case”</a></li>
<li class="chapter" data-level="1.7" data-path="chap-intro.html"><a href="chap-intro.html#the-structure-of-the-book"><i class="fa fa-check"></i><b>1.7</b> The structure of the book</a><ul>
<li class="chapter" data-level="1.7.1" data-path="chap-intro.html"><a href="chap-intro.html#part-i-capture-and-curation"><i class="fa fa-check"></i><b>1.7.1</b> Part I: Capture and curation</a></li>
<li class="chapter" data-level="1.7.2" data-path="chap-intro.html"><a href="chap-intro.html#part-ii-modeling-and-analysis"><i class="fa fa-check"></i><b>1.7.2</b> Part II: Modeling and analysis</a></li>
<li class="chapter" data-level="1.7.3" data-path="chap-intro.html"><a href="chap-intro.html#part-iii-inference-and-ethics"><i class="fa fa-check"></i><b>1.7.3</b> Part III: Inference and ethics</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="chap-intro.html"><a href="chap-intro.html#sec:intro:resources"><i class="fa fa-check"></i><b>1.8</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chap-web.html"><a href="chap-web.html"><i class="fa fa-check"></i><b>2</b> Working with Web Data and APIs</a><ul>
<li class="chapter" data-level="2.1" data-path="chap-web.html"><a href="chap-web.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="chap-web.html"><a href="chap-web.html#scraping-information-from-the-web"><i class="fa fa-check"></i><b>2.2</b> Scraping information from the web</a><ul>
<li class="chapter" data-level="2.2.1" data-path="chap-web.html"><a href="chap-web.html#obtaining-data-from-websites"><i class="fa fa-check"></i><b>2.2.1</b> Obtaining data from websites</a></li>
<li class="chapter" data-level="2.2.2" data-path="chap-web.html"><a href="chap-web.html#limits-of-scraping"><i class="fa fa-check"></i><b>2.2.2</b> Limits of scraping</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="chap-web.html"><a href="chap-web.html#application-programming-interfaces-apis"><i class="fa fa-check"></i><b>2.3</b> Application Programming Interfaces (APIs)</a><ul>
<li class="chapter" data-level="2.3.1" data-path="chap-web.html"><a href="chap-web.html#relevant-apis-and-resources"><i class="fa fa-check"></i><b>2.3.1</b> Relevant APIs and resources</a></li>
<li class="chapter" data-level="2.3.2" data-path="chap-web.html"><a href="chap-web.html#restful-apis-returned-data-and-python-wrappers"><i class="fa fa-check"></i><b>2.3.2</b> RESTful APIs, returned data, and Python wrappers</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="chap-web.html"><a href="chap-web.html#using-an-api"><i class="fa fa-check"></i><b>2.4</b> Using an API</a></li>
<li class="chapter" data-level="2.5" data-path="chap-web.html"><a href="chap-web.html#another-example-using-the-orcid-api-via-a-wrapper"><i class="fa fa-check"></i><b>2.5</b> Another example: Using the ORCID API via a wrapper</a></li>
<li class="chapter" data-level="2.6" data-path="chap-web.html"><a href="chap-web.html#integrating-data-from-multiple-sources"><i class="fa fa-check"></i><b>2.6</b> Integrating data from multiple sources</a></li>
<li class="chapter" data-level="2.7" data-path="chap-web.html"><a href="chap-web.html#summary"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chap-link.html"><a href="chap-link.html"><i class="fa fa-check"></i><b>3</b> Record Linkage</a><ul>
<li class="chapter" data-level="3.1" data-path="chap-link.html"><a href="chap-link.html#motivation"><i class="fa fa-check"></i><b>3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2" data-path="chap-link.html"><a href="chap-link.html#sec:recordlinkage"><i class="fa fa-check"></i><b>3.2</b> Introduction to record linkage</a></li>
<li class="chapter" data-level="3.3" data-path="chap-link.html"><a href="chap-link.html#preprocessing-data-for-record-linkage"><i class="fa fa-check"></i><b>3.3</b> Preprocessing data for record linkage</a></li>
<li class="chapter" data-level="3.4" data-path="chap-link.html"><a href="chap-link.html#S:indexing"><i class="fa fa-check"></i><b>3.4</b> Indexing and blocking</a></li>
<li class="chapter" data-level="3.5" data-path="chap-link.html"><a href="chap-link.html#matching"><i class="fa fa-check"></i><b>3.5</b> Matching</a><ul>
<li class="chapter" data-level="3.5.1" data-path="chap-link.html"><a href="chap-link.html#rule-based-approaches"><i class="fa fa-check"></i><b>3.5.1</b> Rule-based approaches</a></li>
<li class="chapter" data-level="3.5.2" data-path="chap-link.html"><a href="chap-link.html#probabilistic-record-linkage"><i class="fa fa-check"></i><b>3.5.2</b> Probabilistic record linkage</a></li>
<li class="chapter" data-level="3.5.3" data-path="chap-link.html"><a href="chap-link.html#machine-learning-approaches-to-record-linkage"><i class="fa fa-check"></i><b>3.5.3</b> Machine learning approaches to record linkage</a></li>
<li class="chapter" data-level="3.5.4" data-path="chap-link.html"><a href="chap-link.html#disambiguating-networks"><i class="fa fa-check"></i><b>3.5.4</b> Disambiguating networks</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="chap-link.html"><a href="chap-link.html#classification"><i class="fa fa-check"></i><b>3.6</b> Classification</a><ul>
<li class="chapter" data-level="3.6.1" data-path="chap-link.html"><a href="chap-link.html#S:thresholds"><i class="fa fa-check"></i><b>3.6.1</b> Thresholds</a></li>
<li class="chapter" data-level="3.6.2" data-path="chap-link.html"><a href="chap-link.html#one-to-one-links"><i class="fa fa-check"></i><b>3.6.2</b> One-to-one links</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="chap-link.html"><a href="chap-link.html#record-linkage-and-data-protection"><i class="fa fa-check"></i><b>3.7</b> Record linkage and data protection</a></li>
<li class="chapter" data-level="3.8" data-path="chap-link.html"><a href="chap-link.html#summary-1"><i class="fa fa-check"></i><b>3.8</b> Summary</a></li>
<li class="chapter" data-level="3.9" data-path="chap-link.html"><a href="chap-link.html#resources"><i class="fa fa-check"></i><b>3.9</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chap-db.html"><a href="chap-db.html"><i class="fa fa-check"></i><b>4</b> Databases</a><ul>
<li class="chapter" data-level="4.1" data-path="chap-db.html"><a href="chap-db.html#sec:db:intro"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="chap-db.html"><a href="chap-db.html#sec:db:when"><i class="fa fa-check"></i><b>4.2</b> DBMS: When and why</a></li>
<li class="chapter" data-level="4.3" data-path="chap-db.html"><a href="chap-db.html#relational-dbmss"><i class="fa fa-check"></i><b>4.3</b> Relational DBMSs</a><ul>
<li class="chapter" data-level="4.3.1" data-path="chap-db.html"><a href="chap-db.html#structured-query-language-sql"><i class="fa fa-check"></i><b>4.3.1</b> Structured Query Language (SQL)</a></li>
<li class="chapter" data-level="4.3.2" data-path="chap-db.html"><a href="chap-db.html#sec:db:sql"><i class="fa fa-check"></i><b>4.3.2</b> Manipulating and querying data</a></li>
<li class="chapter" data-level="4.3.3" data-path="chap-db.html"><a href="chap-db.html#sec:db:schema"><i class="fa fa-check"></i><b>4.3.3</b> Schema design and definition</a></li>
<li class="chapter" data-level="4.3.4" data-path="chap-db.html"><a href="chap-db.html#loading-data"><i class="fa fa-check"></i><b>4.3.4</b> Loading data</a></li>
<li class="chapter" data-level="4.3.5" data-path="chap-db.html"><a href="chap-db.html#transactions-and-crash-recovery"><i class="fa fa-check"></i><b>4.3.5</b> Transactions and crash recovery</a></li>
<li class="chapter" data-level="4.3.6" data-path="chap-db.html"><a href="chap-db.html#sec:db:index"><i class="fa fa-check"></i><b>4.3.6</b> Database optimizations</a></li>
<li class="chapter" data-level="4.3.7" data-path="chap-db.html"><a href="chap-db.html#caveats-and-challenges"><i class="fa fa-check"></i><b>4.3.7</b> Caveats and challenges</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="chap-db.html"><a href="chap-db.html#linking-dbmss-and-other-tools"><i class="fa fa-check"></i><b>4.4</b> Linking DBMSs and other tools</a></li>
<li class="chapter" data-level="4.5" data-path="chap-db.html"><a href="chap-db.html#sec:db:nosql"><i class="fa fa-check"></i><b>4.5</b> NoSQL databases</a><ul>
<li class="chapter" data-level="4.5.1" data-path="chap-db.html"><a href="chap-db.html#challenges-of-scale-the-cap-theorem"><i class="fa fa-check"></i><b>4.5.1</b> Challenges of scale: The CAP theorem</a></li>
<li class="chapter" data-level="4.5.2" data-path="chap-db.html"><a href="chap-db.html#nosql-and-keyvalue-stores"><i class="fa fa-check"></i><b>4.5.2</b> NoSQL and key–value stores</a></li>
<li class="chapter" data-level="4.5.3" data-path="chap-db.html"><a href="chap-db.html#other-nosql-databases"><i class="fa fa-check"></i><b>4.5.3</b> Other NoSQL databases</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="chap-db.html"><a href="chap-db.html#sec:db:spatial"><i class="fa fa-check"></i><b>4.6</b> Spatial databases</a></li>
<li class="chapter" data-level="4.7" data-path="chap-db.html"><a href="chap-db.html#which-database-to-use"><i class="fa fa-check"></i><b>4.7</b> Which database to use?</a><ul>
<li class="chapter" data-level="4.7.1" data-path="chap-db.html"><a href="chap-db.html#relational-dbmss-1"><i class="fa fa-check"></i><b>4.7.1</b> Relational DBMSs</a></li>
<li class="chapter" data-level="4.7.2" data-path="chap-db.html"><a href="chap-db.html#nosql-dbmss"><i class="fa fa-check"></i><b>4.7.2</b> NoSQL DBMSs</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="chap-db.html"><a href="chap-db.html#summary-2"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
<li class="chapter" data-level="4.9" data-path="chap-db.html"><a href="chap-db.html#resources-1"><i class="fa fa-check"></i><b>4.9</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chap-parallel.html"><a href="chap-parallel.html"><i class="fa fa-check"></i><b>5</b> Scaling up through Parallel and Distributed Computing</a><ul>
<li class="chapter" data-level="5.1" data-path="chap-parallel.html"><a href="chap-parallel.html#introduction-1"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="chap-parallel.html"><a href="chap-parallel.html#mapreduce"><i class="fa fa-check"></i><b>5.2</b> MapReduce</a></li>
<li class="chapter" data-level="5.3" data-path="chap-parallel.html"><a href="chap-parallel.html#apache-hadoop-mapreduce"><i class="fa fa-check"></i><b>5.3</b> Apache Hadoop MapReduce</a><ul>
<li class="chapter" data-level="5.3.1" data-path="chap-parallel.html"><a href="chap-parallel.html#the-hadoop-distributed-file-system"><i class="fa fa-check"></i><b>5.3.1</b> The Hadoop Distributed File System</a></li>
<li class="chapter" data-level="5.3.2" data-path="chap-parallel.html"><a href="chap-parallel.html#hadoop-setup-bringing-compute-to-the-data"><i class="fa fa-check"></i><b>5.3.2</b> Hadoop Setup: Bringing compute to the data</a></li>
<li class="chapter" data-level="5.3.3" data-path="chap-parallel.html"><a href="chap-parallel.html#hardware-provisioning"><i class="fa fa-check"></i><b>5.3.3</b> Hardware provisioning</a></li>
<li class="chapter" data-level="5.3.4" data-path="chap-parallel.html"><a href="chap-parallel.html#programming-in-hadoop"><i class="fa fa-check"></i><b>5.3.4</b> Programming in Hadoop</a></li>
<li class="chapter" data-level="5.3.5" data-path="chap-parallel.html"><a href="chap-parallel.html#programming-language-support"><i class="fa fa-check"></i><b>5.3.5</b> Programming language support</a></li>
<li class="chapter" data-level="5.3.6" data-path="chap-parallel.html"><a href="chap-parallel.html#benefits-and-limitations-of-hadoop"><i class="fa fa-check"></i><b>5.3.6</b> Benefits and Limitations of Hadoop</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="chap-parallel.html"><a href="chap-parallel.html#other-mapreduce-implementations"><i class="fa fa-check"></i><b>5.4</b> Other MapReduce Implementations</a></li>
<li class="chapter" data-level="5.5" data-path="chap-parallel.html"><a href="chap-parallel.html#apache-spark"><i class="fa fa-check"></i><b>5.5</b> Apache Spark</a></li>
<li class="chapter" data-level="5.6" data-path="chap-parallel.html"><a href="chap-parallel.html#summary-3"><i class="fa fa-check"></i><b>5.6</b> Summary</a></li>
<li class="chapter" data-level="5.7" data-path="chap-parallel.html"><a href="chap-parallel.html#resources-2"><i class="fa fa-check"></i><b>5.7</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chap-viz.html"><a href="chap-viz.html"><i class="fa fa-check"></i><b>6</b> Information Visualization</a><ul>
<li class="chapter" data-level="6.1" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-1"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2"><i class="fa fa-check"></i><b>6.2</b> Developing effective visualizations</a></li>
<li class="chapter" data-level="6.3" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-3"><i class="fa fa-check"></i><b>6.3</b> A data-by-tasks taxonomy</a><ul>
<li class="chapter" data-level="6.3.1" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.1"><i class="fa fa-check"></i><b>6.3.1</b> Multivariate data</a></li>
<li class="chapter" data-level="6.3.2" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.2"><i class="fa fa-check"></i><b>6.3.2</b> Spatial data</a></li>
<li class="chapter" data-level="6.3.3" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.4"><i class="fa fa-check"></i><b>6.3.3</b> Temporal data</a></li>
<li class="chapter" data-level="6.3.4" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.5"><i class="fa fa-check"></i><b>6.3.4</b> Hierarchical data</a></li>
<li class="chapter" data-level="6.3.5" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.6"><i class="fa fa-check"></i><b>6.3.5</b> Network data</a></li>
<li class="chapter" data-level="6.3.6" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.7"><i class="fa fa-check"></i><b>6.3.6</b> Text data</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4"><i class="fa fa-check"></i><b>6.4</b> Challenges</a><ul>
<li class="chapter" data-level="6.4.1" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4.1"><i class="fa fa-check"></i><b>6.4.1</b> Scalability</a></li>
<li class="chapter" data-level="6.4.2" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4.2"><i class="fa fa-check"></i><b>6.4.2</b> Evaluation</a></li>
<li class="chapter" data-level="6.4.3" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4.3"><i class="fa fa-check"></i><b>6.4.3</b> Visual impairment</a></li>
<li class="chapter" data-level="6.4.4" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4.4"><i class="fa fa-check"></i><b>6.4.4</b> Visual literacy</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-5"><i class="fa fa-check"></i><b>6.5</b> Summary</a></li>
<li class="chapter" data-level="6.6" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-6"><i class="fa fa-check"></i><b>6.6</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chap-ml.html"><a href="chap-ml.html"><i class="fa fa-check"></i><b>7</b> Machine Learning</a><ul>
<li class="chapter" data-level="7.1" data-path="chap-ml.html"><a href="chap-ml.html#introduction-2"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="chap-ml.html"><a href="chap-ml.html#what-is-machine-learning"><i class="fa fa-check"></i><b>7.2</b> What is machine learning?</a></li>
<li class="chapter" data-level="7.3" data-path="chap-ml.html"><a href="chap-ml.html#types-of-analysis"><i class="fa fa-check"></i><b>7.3</b> Types of analysis</a></li>
<li class="chapter" data-level="7.4" data-path="chap-ml.html"><a href="chap-ml.html#the-machine-learning-process"><i class="fa fa-check"></i><b>7.4</b> The Machine Learning process</a></li>
<li class="chapter" data-level="7.5" data-path="chap-ml.html"><a href="chap-ml.html#problem-formulation-mapping-a-problem-to-machine-learning-methods"><i class="fa fa-check"></i><b>7.5</b> Problem formulation: Mapping a problem to machine learning methods</a><ul>
<li class="chapter" data-level="7.5.1" data-path="chap-ml.html"><a href="chap-ml.html#features"><i class="fa fa-check"></i><b>7.5.1</b> Features</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="chap-ml.html"><a href="chap-ml.html#methods"><i class="fa fa-check"></i><b>7.6</b> Methods</a><ul>
<li class="chapter" data-level="7.6.1" data-path="chap-ml.html"><a href="chap-ml.html#unsupervised-learning-methods"><i class="fa fa-check"></i><b>7.6.1</b> Unsupervised learning methods</a></li>
<li class="chapter" data-level="7.6.2" data-path="chap-ml.html"><a href="chap-ml.html#sec:MLchapter:super"><i class="fa fa-check"></i><b>7.6.2</b> Supervised learning</a></li>
<li class="chapter" data-level="7.6.3" data-path="chap-ml.html"><a href="chap-ml.html#binary-vs-multiclass-classification-problems"><i class="fa fa-check"></i><b>7.6.3</b> Binary vs Multiclass classification problems</a></li>
<li class="chapter" data-level="7.6.4" data-path="chap-ml.html"><a href="chap-ml.html#skewed-or-imbalanced-classification-problems"><i class="fa fa-check"></i><b>7.6.4</b> Skewed or imbalanced classification problems</a></li>
<li class="chapter" data-level="7.6.5" data-path="chap-ml.html"><a href="chap-ml.html#model-interpretability"><i class="fa fa-check"></i><b>7.6.5</b> Model interpretability</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="chap-ml.html"><a href="chap-ml.html#sec:7-7"><i class="fa fa-check"></i><b>7.7</b> Evaluation</a><ul>
<li class="chapter" data-level="7.7.1" data-path="chap-ml.html"><a href="chap-ml.html#sec:7-7.1"><i class="fa fa-check"></i><b>7.7.1</b> Methodology</a></li>
<li class="chapter" data-level="7.7.2" data-path="chap-ml.html"><a href="chap-ml.html#sec:7-7.2"><i class="fa fa-check"></i><b>7.7.2</b> Metrics</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="chap-ml.html"><a href="chap-ml.html#practical-tips"><i class="fa fa-check"></i><b>7.8</b> Practical tips</a><ul>
<li class="chapter" data-level="7.8.1" data-path="chap-ml.html"><a href="chap-ml.html#avoiding-leakage"><i class="fa fa-check"></i><b>7.8.1</b> Avoiding Leakage</a></li>
<li class="chapter" data-level="7.8.2" data-path="chap-ml.html"><a href="chap-ml.html#machine-learning-pipeline"><i class="fa fa-check"></i><b>7.8.2</b> Machine learning pipeline</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="chap-ml.html"><a href="chap-ml.html#how-can-social-scientists-benefit-from-machine-learning"><i class="fa fa-check"></i><b>7.9</b> How can social scientists benefit from machine learning?</a></li>
<li class="chapter" data-level="7.10" data-path="chap-ml.html"><a href="chap-ml.html#advanced-topics"><i class="fa fa-check"></i><b>7.10</b> Advanced topics</a></li>
<li class="chapter" data-level="7.11" data-path="chap-ml.html"><a href="chap-ml.html#summary-4"><i class="fa fa-check"></i><b>7.11</b> Summary</a></li>
<li class="chapter" data-level="7.12" data-path="chap-ml.html"><a href="chap-ml.html#ml:res"><i class="fa fa-check"></i><b>7.12</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chap-text.html"><a href="chap-text.html"><i class="fa fa-check"></i><b>8</b> Text Analysis</a><ul>
<li class="chapter" data-level="8.1" data-path="chap-text.html"><a href="chap-text.html#understanding-human-generated-text"><i class="fa fa-check"></i><b>8.1</b> Understanding human generated text</a></li>
<li class="chapter" data-level="8.2" data-path="chap-text.html"><a href="chap-text.html#how-is-text-data-different-than-structured-data"><i class="fa fa-check"></i><b>8.2</b> How is text data different than “structured” data?</a></li>
<li class="chapter" data-level="8.3" data-path="chap-text.html"><a href="chap-text.html#what-can-we-do-with-text-data"><i class="fa fa-check"></i><b>8.3</b> What can we do with text data?</a></li>
<li class="chapter" data-level="8.4" data-path="chap-text.html"><a href="chap-text.html#how-to-analyze-text"><i class="fa fa-check"></i><b>8.4</b> How to analyze text</a><ul>
<li class="chapter" data-level="8.4.1" data-path="chap-text.html"><a href="chap-text.html#initial-processing"><i class="fa fa-check"></i><b>8.4.1</b> Initial Processing</a></li>
<li class="chapter" data-level="8.4.2" data-path="chap-text.html"><a href="chap-text.html#linguistic-analysis"><i class="fa fa-check"></i><b>8.4.2</b> Linguistic Analysis</a></li>
<li class="chapter" data-level="8.4.3" data-path="chap-text.html"><a href="chap-text.html#turning-text-data-into-a-matrix-how-much-is-a-word-worth"><i class="fa fa-check"></i><b>8.4.3</b> Turning text data into a matrix: How much is a word worth?</a></li>
<li class="chapter" data-level="8.4.4" data-path="chap-text.html"><a href="chap-text.html#analysis"><i class="fa fa-check"></i><b>8.4.4</b> Analysis</a></li>
<li class="chapter" data-level="8.4.5" data-path="chap-text.html"><a href="chap-text.html#sec:lda"><i class="fa fa-check"></i><b>8.4.5</b> Topic modeling</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="chap-text.html"><a href="chap-text.html#word-embeddings-and-deep-learning"><i class="fa fa-check"></i><b>8.5</b> Word Embeddings and Deep Learning</a></li>
<li class="chapter" data-level="8.6" data-path="chap-text.html"><a href="chap-text.html#text-analysis-tools"><i class="fa fa-check"></i><b>8.6</b> Text analysis tools</a></li>
<li class="chapter" data-level="8.7" data-path="chap-text.html"><a href="chap-text.html#summary-5"><i class="fa fa-check"></i><b>8.7</b> Summary</a></li>
<li class="chapter" data-level="8.8" data-path="chap-text.html"><a href="chap-text.html#resources-3"><i class="fa fa-check"></i><b>8.8</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chap-networks.html"><a href="chap-networks.html"><i class="fa fa-check"></i><b>9</b> Networks: The Basics</a><ul>
<li class="chapter" data-level="9.1" data-path="chap-networks.html"><a href="chap-networks.html#introduction-3"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="chap-networks.html"><a href="chap-networks.html#what-are-networks"><i class="fa fa-check"></i><b>9.2</b> What are networks?</a></li>
<li class="chapter" data-level="9.3" data-path="chap-networks.html"><a href="chap-networks.html#structure-for-this-chapter"><i class="fa fa-check"></i><b>9.3</b> Structure for this chapter</a></li>
<li class="chapter" data-level="9.4" data-path="chap-networks.html"><a href="chap-networks.html#turning-data-into-a-network"><i class="fa fa-check"></i><b>9.4</b> Turning Data into a Network</a><ul>
<li class="chapter" data-level="9.4.1" data-path="chap-networks.html"><a href="chap-networks.html#types-of-networks"><i class="fa fa-check"></i><b>9.4.1</b> Types of Networks</a></li>
<li class="chapter" data-level="9.4.2" data-path="chap-networks.html"><a href="chap-networks.html#inducing-one-mode-networks-from-two-mode-data"><i class="fa fa-check"></i><b>9.4.2</b> Inducing one-mode networks from two-mode data</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="chap-networks.html"><a href="chap-networks.html#network-measures"><i class="fa fa-check"></i><b>9.5</b> Network measures</a><ul>
<li class="chapter" data-level="9.5.1" data-path="chap-networks.html"><a href="chap-networks.html#reachability"><i class="fa fa-check"></i><b>9.5.1</b> Reachability</a></li>
<li class="chapter" data-level="9.5.2" data-path="chap-networks.html"><a href="chap-networks.html#whole-network-measures"><i class="fa fa-check"></i><b>9.5.2</b> Whole-network measures</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="chap-networks.html"><a href="chap-networks.html#case-study-comparing-collaboration-networks"><i class="fa fa-check"></i><b>9.6</b> Case Study: Comparing collaboration networks</a></li>
<li class="chapter" data-level="9.7" data-path="chap-networks.html"><a href="chap-networks.html#summary-6"><i class="fa fa-check"></i><b>9.7</b> Summary</a></li>
<li class="chapter" data-level="9.8" data-path="chap-networks.html"><a href="chap-networks.html#resources-4"><i class="fa fa-check"></i><b>9.8</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chap-errors.html"><a href="chap-errors.html"><i class="fa fa-check"></i><b>10</b> Data Quality and Inference Errors</a><ul>
<li class="chapter" data-level="10.1" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-1"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-2"><i class="fa fa-check"></i><b>10.2</b> The total error paradigm</a><ul>
<li class="chapter" data-level="10.2.1" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-2.1"><i class="fa fa-check"></i><b>10.2.1</b> The traditional model</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-3"><i class="fa fa-check"></i><b>10.3</b> Example: Google Flu Trends</a></li>
<li class="chapter" data-level="10.4" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-4"><i class="fa fa-check"></i><b>10.4</b> Errors in data analysis</a><ul>
<li class="chapter" data-level="10.4.1" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-4.2"><i class="fa fa-check"></i><b>10.4.1</b> Analysis errors resulting from inaccurate data</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-5"><i class="fa fa-check"></i><b>10.5</b> Detecting and Compensating for Data Errors</a><ul>
<li class="chapter" data-level="10.5.1" data-path="chap-errors.html"><a href="chap-errors.html#tableplots"><i class="fa fa-check"></i><b>10.5.1</b> TablePlots</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-6"><i class="fa fa-check"></i><b>10.6</b> Summary</a></li>
<li class="chapter" data-level="10.7" data-path="chap-errors.html"><a href="chap-errors.html#resources-5"><i class="fa fa-check"></i><b>10.7</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chap-bias.html"><a href="chap-bias.html"><i class="fa fa-check"></i><b>11</b> Bias and Fairness</a><ul>
<li class="chapter" data-level="11.1" data-path="chap-bias.html"><a href="chap-bias.html#introduction-4"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="chap-bias.html"><a href="chap-bias.html#sec:biassources"><i class="fa fa-check"></i><b>11.2</b> Sources of Bias</a><ul>
<li class="chapter" data-level="11.2.1" data-path="chap-bias.html"><a href="chap-bias.html#sample-bias"><i class="fa fa-check"></i><b>11.2.1</b> Sample Bias</a></li>
<li class="chapter" data-level="11.2.2" data-path="chap-bias.html"><a href="chap-bias.html#label-outcome-bias"><i class="fa fa-check"></i><b>11.2.2</b> Label (Outcome) Bias</a></li>
<li class="chapter" data-level="11.2.3" data-path="chap-bias.html"><a href="chap-bias.html#sec:mlbiasexamples"><i class="fa fa-check"></i><b>11.2.3</b> Machine Learning Pipeline Bias</a></li>
<li class="chapter" data-level="11.2.4" data-path="chap-bias.html"><a href="chap-bias.html#application-bias"><i class="fa fa-check"></i><b>11.2.4</b> Application Bias</a></li>
<li class="chapter" data-level="11.2.5" data-path="chap-bias.html"><a href="chap-bias.html#considering-bias-when-deploying-your-model"><i class="fa fa-check"></i><b>11.2.5</b> Considering Bias When Deploying Your Model</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="chap-bias.html"><a href="chap-bias.html#dealing-with-bias"><i class="fa fa-check"></i><b>11.3</b> Dealing with Bias</a><ul>
<li class="chapter" data-level="11.3.1" data-path="chap-bias.html"><a href="chap-bias.html#sec:metrics"><i class="fa fa-check"></i><b>11.3.1</b> Define Bias</a></li>
<li class="chapter" data-level="11.3.2" data-path="chap-bias.html"><a href="chap-bias.html#definitions"><i class="fa fa-check"></i><b>11.3.2</b> Definitions</a></li>
<li class="chapter" data-level="11.3.3" data-path="chap-bias.html"><a href="chap-bias.html#choosing-bias-metrics"><i class="fa fa-check"></i><b>11.3.3</b> Choosing Bias Metrics</a></li>
<li class="chapter" data-level="11.3.4" data-path="chap-bias.html"><a href="chap-bias.html#sec:punitiveexample"><i class="fa fa-check"></i><b>11.3.4</b> Punitive Example</a></li>
<li class="chapter" data-level="11.3.5" data-path="chap-bias.html"><a href="chap-bias.html#sec:assistiveexample"><i class="fa fa-check"></i><b>11.3.5</b> Assistive Example</a></li>
<li class="chapter" data-level="11.3.6" data-path="chap-bias.html"><a href="chap-bias.html#sec:constrainedassistive"><i class="fa fa-check"></i><b>11.3.6</b> Special Case: Resource-Constrained Programs</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="chap-bias.html"><a href="chap-bias.html#sec:applications"><i class="fa fa-check"></i><b>11.4</b> Mitigating Bias</a><ul>
<li class="chapter" data-level="11.4.1" data-path="chap-bias.html"><a href="chap-bias.html#auditing-model-results"><i class="fa fa-check"></i><b>11.4.1</b> Auditing Model Results</a></li>
<li class="chapter" data-level="11.4.2" data-path="chap-bias.html"><a href="chap-bias.html#model-selection"><i class="fa fa-check"></i><b>11.4.2</b> Model Selection</a></li>
<li class="chapter" data-level="11.4.3" data-path="chap-bias.html"><a href="chap-bias.html#other-options-for-mitigating-bias"><i class="fa fa-check"></i><b>11.4.3</b> Other Options for Mitigating Bias</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="chap-bias.html"><a href="chap-bias.html#further-considerations"><i class="fa fa-check"></i><b>11.5</b> Further Considerations</a><ul>
<li class="chapter" data-level="11.5.1" data-path="chap-bias.html"><a href="chap-bias.html#compared-to-what"><i class="fa fa-check"></i><b>11.5.1</b> Compared to What?</a></li>
<li class="chapter" data-level="11.5.2" data-path="chap-bias.html"><a href="chap-bias.html#costs-to-both-errors"><i class="fa fa-check"></i><b>11.5.2</b> Costs to Both Errors</a></li>
<li class="chapter" data-level="11.5.3" data-path="chap-bias.html"><a href="chap-bias.html#what-is-the-relevant-population"><i class="fa fa-check"></i><b>11.5.3</b> What is the Relevant Population?</a></li>
<li class="chapter" data-level="11.5.4" data-path="chap-bias.html"><a href="chap-bias.html#continuous-outcomes"><i class="fa fa-check"></i><b>11.5.4</b> Continuous Outcomes</a></li>
<li class="chapter" data-level="11.5.5" data-path="chap-bias.html"><a href="chap-bias.html#considerations-for-ongoing-measurement"><i class="fa fa-check"></i><b>11.5.5</b> Considerations for Ongoing Measurement</a></li>
<li class="chapter" data-level="11.5.6" data-path="chap-bias.html"><a href="chap-bias.html#equity-in-practice"><i class="fa fa-check"></i><b>11.5.6</b> Equity in Practice</a></li>
<li class="chapter" data-level="11.5.7" data-path="chap-bias.html"><a href="chap-bias.html#other-names-you-might-see"><i class="fa fa-check"></i><b>11.5.7</b> Other Names You Might See</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="chap-bias.html"><a href="chap-bias.html#case-studies"><i class="fa fa-check"></i><b>11.6</b> Case Studies</a><ul>
<li class="chapter" data-level="11.6.1" data-path="chap-bias.html"><a href="chap-bias.html#sec:compascase"><i class="fa fa-check"></i><b>11.6.1</b> Recidivism Predictions with COMPAS</a></li>
<li class="chapter" data-level="11.6.2" data-path="chap-bias.html"><a href="chap-bias.html#facial-recognition"><i class="fa fa-check"></i><b>11.6.2</b> Facial Recognition</a></li>
<li class="chapter" data-level="11.6.3" data-path="chap-bias.html"><a href="chap-bias.html#facebook-advertisement-targeting"><i class="fa fa-check"></i><b>11.6.3</b> Facebook Advertisement Targeting</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="chap-bias.html"><a href="chap-bias.html#aequitas---a-toolkit-for-auditing-bias-and-fairness-in-machine-learning-models"><i class="fa fa-check"></i><b>11.7</b> Aequitas - A Toolkit for Auditing Bias and Fairness in Machine Learning Models</a></li>
<li class="chapter" data-level="11.8" data-path="chap-bias.html"><a href="chap-bias.html#summary-7"><i class="fa fa-check"></i><b>11.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chap-privacy.html"><a href="chap-privacy.html"><i class="fa fa-check"></i><b>12</b> Privacy and Confidentiality</a><ul>
<li class="chapter" data-level="12.1" data-path="chap-privacy.html"><a href="chap-privacy.html#introduction-5"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="chap-privacy.html"><a href="chap-privacy.html#why-is-access-important"><i class="fa fa-check"></i><b>12.2</b> Why is access important?</a></li>
<li class="chapter" data-level="12.3" data-path="chap-privacy.html"><a href="chap-privacy.html#providing-access"><i class="fa fa-check"></i><b>12.3</b> Providing access</a></li>
<li class="chapter" data-level="12.4" data-path="chap-privacy.html"><a href="chap-privacy.html#non-tabular-data"><i class="fa fa-check"></i><b>12.4</b> Non-Tabular data</a></li>
<li class="chapter" data-level="12.5" data-path="chap-privacy.html"><a href="chap-privacy.html#the-new-challenges"><i class="fa fa-check"></i><b>12.5</b> The new challenges</a></li>
<li class="chapter" data-level="12.6" data-path="chap-privacy.html"><a href="chap-privacy.html#legal-and-ethical-framework"><i class="fa fa-check"></i><b>12.6</b> Legal and ethical framework</a></li>
<li class="chapter" data-level="12.7" data-path="chap-privacy.html"><a href="chap-privacy.html#summary-8"><i class="fa fa-check"></i><b>12.7</b> Summary</a></li>
<li class="chapter" data-level="12.8" data-path="chap-privacy.html"><a href="chap-privacy.html#resources-6"><i class="fa fa-check"></i><b>12.8</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="chap-workbooks.html"><a href="chap-workbooks.html"><i class="fa fa-check"></i><b>13</b> Workbooks</a><ul>
<li class="chapter" data-level="13.1" data-path="chap-workbooks.html"><a href="chap-workbooks.html#introduction-6"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="chap-workbooks.html"><a href="chap-workbooks.html#notebooks"><i class="fa fa-check"></i><b>13.2</b> Notebooks</a><ul>
<li class="chapter" data-level="13.2.1" data-path="chap-workbooks.html"><a href="chap-workbooks.html#databases"><i class="fa fa-check"></i><b>13.2.1</b> Databases</a></li>
<li class="chapter" data-level="13.2.2" data-path="chap-workbooks.html"><a href="chap-workbooks.html#dataset-exploration-and-visualization"><i class="fa fa-check"></i><b>13.2.2</b> Dataset Exploration and Visualization</a></li>
<li class="chapter" data-level="13.2.3" data-path="chap-workbooks.html"><a href="chap-workbooks.html#apis"><i class="fa fa-check"></i><b>13.2.3</b> APIs</a></li>
<li class="chapter" data-level="13.2.4" data-path="chap-workbooks.html"><a href="chap-workbooks.html#record-linkage"><i class="fa fa-check"></i><b>13.2.4</b> Record Linkage</a></li>
<li class="chapter" data-level="13.2.5" data-path="chap-workbooks.html"><a href="chap-workbooks.html#text-analysis"><i class="fa fa-check"></i><b>13.2.5</b> Text Analysis</a></li>
<li class="chapter" data-level="13.2.6" data-path="chap-workbooks.html"><a href="chap-workbooks.html#networks"><i class="fa fa-check"></i><b>13.2.6</b> Networks</a></li>
<li class="chapter" data-level="13.2.7" data-path="chap-workbooks.html"><a href="chap-workbooks.html#machine-learning-creating-labels"><i class="fa fa-check"></i><b>13.2.7</b> Machine Learning – Creating Labels</a></li>
<li class="chapter" data-level="13.2.8" data-path="chap-workbooks.html"><a href="chap-workbooks.html#machine-learning-creating-features"><i class="fa fa-check"></i><b>13.2.8</b> Machine Learning – Creating Features</a></li>
<li class="chapter" data-level="13.2.9" data-path="chap-workbooks.html"><a href="chap-workbooks.html#machine-learning-model-training-and-evaluation"><i class="fa fa-check"></i><b>13.2.9</b> Machine Learning – Model Training and Evaluation</a></li>
<li class="chapter" data-level="13.2.10" data-path="chap-workbooks.html"><a href="chap-workbooks.html#bias-and-fairness"><i class="fa fa-check"></i><b>13.2.10</b> Bias and Fairness</a></li>
<li class="chapter" data-level="13.2.11" data-path="chap-workbooks.html"><a href="chap-workbooks.html#errors-and-inference"><i class="fa fa-check"></i><b>13.2.11</b> Errors and Inference</a></li>
<li class="chapter" data-level="13.2.12" data-path="chap-workbooks.html"><a href="chap-workbooks.html#additional-workbooks"><i class="fa fa-check"></i><b>13.2.12</b> Additional Workbooks</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="chap-workbooks.html"><a href="chap-workbooks.html#resources-7"><i class="fa fa-check"></i><b>13.3</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Big Data and Social Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chap:link" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Record Linkage</h1>
<p><strong>Joshua Tokle and Stefan Bender</strong></p>
<p>As we mentioned in the last chapter, it is often necessary to combine data from multiple sources to get a complete picture of the activities of interest. In addition to just linking data to get additional information, we are also concerned about issues of missing links, duplicative links, and erroneous links. This chapter provides an overview of traditional rule-based and probabilistic approaches, as well as the modern approaches to record linkage using machine learning.</p>
<div id="motivation" class="section level2">
<h2><span class="header-section-number">3.1</span> Motivation</h2>
<p>New sources of data offer social scientists great opportunities to bring together many different types of data, from many different sources. Merging different data sets provides new ways of creating population frames that are generated from the digital traces of human activity rather than, say, tax records. These opportunities, however, create different kinds of challenges from those posed by survey data. Combining information from different sources about an individual, business, or geographic entity means that the social scientist must determine whether or not two entities in two different data sources are the same. This determination is not always easy.</p>
<p>We regularly run into situations where we need to combine data from different agencies about the same people to understand future employment or health outcomes for people on social service benefits or those who have recently been released from prison. In the UMETRICS data for example, if data are to be used to measure the impact of research grants, is David A. Miller from Stanford, CA, the same as David Andrew Miller from Fairhaven, NJ, in a list of inventors? Is IBM the same as Big Blue if the productivity and growth of R&amp;D-intensive firms is to be studied? Or, more generally, is individual A the same person as the one who appears on a list that has been compiled? Does the product that a customer is searching for match the products that business B has for sale?</p>
<p>The consequences of poor record linkage decisions can be substantial. In the business arena, Christen reports that as much as 12% of business revenues are lost due to bad linkages <span class="citation">(Christen <a href="#ref-christen2012data">2012</a><a href="#ref-christen2012data">b</a>)</span>. In the security arena, failure to match travelers to a “known terrorist” list may result in those individuals entering the country, while overzealous matching could lead to numbers of innocent citizens being detained. In finance, incorrectly detecting a legitimate purchase as a fraudulent one annoys the customer, but failing to identify a thief will lead to credit card losses. Less dramatically, in the scientific arena when studying patenting behavior, if it is decided that two inventors are the same person, when in fact they are not, then records will be incorrectly grouped together and one researcher’s productivity will be overstated. Conversely, if the records for one inventor are believed to correspond to multiple individuals, then that inventor’s productivity will be understated.</p>
<p>This chapter discusses current approaches to joining multiple data sets together—commonly called <em>record linkage</em>.<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a></p>
<p>We draw heavily here on work by Winkler, Scheuren, and Christen, in particular <span class="citation">(Herzog, Scheuren, and Winkler <a href="#ref-herzog2007data">2007</a>; Christen <a href="#ref-christen2012survey">2012</a><a href="#ref-christen2012survey">a</a>; Christen <a href="#ref-christen2012data">2012</a><a href="#ref-christen2012data">b</a>)</span>. To ground ideas, we use examples from a recent paper examining the effects of different algorithms on studies of patent productivity <span class="citation">(Ventura, Nugent, and Fuchs <a href="#ref-ventura2015seeing">2015</a>)</span>.</p>
<hr />
<p><strong>Box: Examples</strong> <a id="box:link1"></a></p>
<p>In addition to the worked examples in this chapter here are a few other papers that show the wide variety of projects using combining records from different sources.<a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a></p>
<p>Glennon <span class="citation">(<a href="#ref-Glennon2019">2019</a>)</span> used a unique matched firm-level dataset of H-1B visas and multinational firm activity show that restrictions on H-1B immigration caused increases in foreign affiliate employment. Restrictions also caused increases in foreign patenting, suggesting that there was also a change in the location of innovative activity.</p>
<p>Rodolfa et al. <span class="citation">(<a href="#ref-Rodolfa2020">2020</a>)</span> use machine learning based record linkage to link data about the same individuals together from a criminal justice case management system to help the Los Angeles City Atorney’s office develop individually-tailored social service interventions in a fair and equitable manner. Because the system lacked a global unique person-level identifier, case-level defendant data was used to link cases belonging to the same person using first and last name, date of birth, address, driver’s license number (where available), and California Information and Identification (CII) number (where available).</p>
<p>The National Center for Health Statistics (NCHS) <span class="citation">(<a href="#ref-NCHS2019">2019</a>)</span> links the data from the National Health Interview Survey (NHIS) to records from the Social Security Administration, the Centers for Medicare &amp; Medicaid Services, and the National Death Index to investigate the relationship between health and sociodemographic information reported in the surveys and medical care costs, future use of medical services and mortality.</p>
<hr />
</div>
<div id="sec:recordlinkage" class="section level2">
<h2><span class="header-section-number">3.2</span> Introduction to record linkage</h2>
<p>There are many reasons to link data sets. Linking to existing data sources to solve a measurement need instead of implementing a new survey results in cost savings (and almost certainly time savings as well) and reduced burden on potential survey respondents. For some research questions (e.g., a survey of the reasons for death of a longitudinal cohort of individuals) a new survey may not be possible. In the case of administrative data or other automatically generated data, the sample size is much greater than would be possible from a survey.</p>
<p>Record linkage can be used to compensate for data quality issues. If a large number of observations for a particular field are missing, it may be possible to link to another data source to fill in the missing values. For example, survey respondents might not want to share a sensitive datum like income. If the researcher has access to an official administrative list with income data, then those values can be used to supplement the survey <span class="citation">(Abowd, Stinson, and Benedetto <a href="#ref-abowd2006final">2006</a>)</span>.</p>
<p>Record linkage is often used to create new longitudinal data sets by linking the same entities over time <span class="citation">(Jarmin and Miranda <a href="#ref-jarmin2002longitudinal">2002</a>)</span>. More generally, linking separate data sources makes it possible to create a combined data set that is richer in coverage and measurement than any of the individual data sources <span class="citation">(Abowd, Haltiwanger, and Lane <a href="#ref-abowd2004integrated">2004</a>)</span>.</p>
<hr />
<p><strong>Example: The Administrative Data Research Network</strong></p>
<p>The UK’s Administrative Data Research Network<a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a> (ADRN) is a major investment by the United Kingdom to “improve our knowledge and understanding of the society we live in … [and] provide a sound base for policymakers to decide how to tackle a range of complex social, economic and environmental issues” by linking administrative data from a variety of sources, such as health agencies, court records, and tax records in a confidential environment for approved researchers. The linkages are done by trusted third-party providers. <span class="citation">(Economic and Social Research Council <a href="#ref-EconomicandSocialResearchCouncil2016">2016</a>)</span></p>
<hr />
<p>Linking is straightforward if each entity has a corresponding unique identifier that appears in the data sets to be linked. For example, two lists of US employees may both contain Social Security numbers. When a unique identifier exists in the data or can be created, no special techniques are necessary to join the data sets.</p>
<p>If there is no unique identifier available, then the task of identifying unique entities is challenging. One instead relies on fields that only partially identify the entity, like names, addresses, or dates of birth. The problem is further complicated by poor data quality and duplicate records, issues well attested in the record linkage literature <span class="citation">(Christen <a href="#ref-christen2012survey">2012</a><a href="#ref-christen2012survey">a</a>)</span> and sure to become more important in the context of big data. Data quality issues include input errors (typos, misspellings, truncation, extraneous letters, abbreviations, and missing values) as well as differences in the way variables are coded between the two data sets (age versus date of birth, for example). In addition to record linkage algorithms, we will discuss different data preprocessing steps that are necessary first steps for the best results in record linkage.</p>
<p>To find all possible links between two data sets it would be necessary to compare each record of the first data set with each record of the second data set. The computational complexity of this grows quadratically with the size of the data—an important consideration, especially with large amounts of data. To compensate for this complexity, the standard second step in record linkage, after preprocessing, is indexing or blocking, which uses some set of heuristics to create subsets of similar records and reduces the total number of comparisons.</p>
<p>The outcome of the matching step is a set of predicted links—record pairs that are likely to correspond to the same entity. After these are produced, the final stage of the record linkage process is to evaluate the result and estimate the resulting error rates. Unlike other areas of application for predictive algorithms, ground truth or gold standard data sets are rarely available. The only way to create a reliable truth data set sometimes is through an expensive human review process that may not be viable for a given application. Instead, error rates must be estimated.</p>
<p>An input data set may contribute to the linked data in a variety of ways, such as increasing coverage, expanding understanding of the measurement or mismeasurement of underlying latent variables, or adding new variables to the combined data set. It is therefore important to develop a well-specified reason for linking the data sets, and to specify a loss function to proxy the cost of false negative matches versus false positive matches that can be used to guide match decisions. It is also important to understand the coverage of the different data sets being linked because differences in coverage may result in bias in the linked data. For example, consider the problem of linking Twitter data to a sample-based survey—elderly adults and very young children are unlikely to use Twitter and so the set of records in the linked data set will have a youth bias, even if the original sample was representative of the population. It is also essential to engage in critical thinking about what latent variables are being captured by the measures in the different data sets—an “occupational classification” in a survey data set may be very different from a “job title” in an administrative record or a “current position” in LinkedIn data.<a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a></p>
<hr />
<p><strong>Example: Employment and earnings outcomes of doctoral recipients</strong></p>
<p>A recent paper in <em>Science</em> matched UMETRICS data on doctoral recipients to Census data on earnings and employment outcomes. The authors note that some 20% of the doctoral recipients are not matched for several reasons: (i) the recipient does not have a job in the US, either for family reasons or because he/she goes back to his/her home country; (ii) he/she starts up a business rather than choosing employment; or (iii) it is not possible to uniquely match him/her to a Census Bureau record. They correctly note that there may be biases introduced in case (iii), because Asian names are more likely duplicated and harder to uniquely match <span class="citation">(Zolas et al. <a href="#ref-zolas2015wrapping">2015</a>)</span>. Improving the linkage algorithm would increase the estimate of the effects of investments in research and the result would be more accurate.</p>
<hr />
<p>Comparing the kinds of heterogeneous records associated with big data is a new challenge for social scientists, who have traditionally used a technique first developed in the 1960s to apply computers to the problem of medical record linkage. There is a reason why this approach has survived: it has been highly successful in linking survey data to administrative data, and efficient implementations of this algorithm can be applied at the big data scale. However, the approach is most effective when the two files being linked have a number of fields in common. In the new landscape of big data, there is a greater need to link files that have few fields in common but whose noncommon fields provide additional predictive power to determine which records should be linked. In some cases, when sufficient training data can be produced, more modern machine learning techniques may be applied.</p>
<div class="figure" style="text-align: center"><span id="fig:fig3-1"></span>
<img src="ChapterLinkage/figures/fig3-1.png" alt="The preprocessing pipeline" width="70%" />
<p class="caption">
Figure 3.1: The preprocessing pipeline
</p>
</div>
<p>The canonical record linkage workflow process is shown in Figure <a href="chap-link.html#fig:fig3-1">3.1</a> for two data files, A and B. The goal is to identify all pairs of records in the two data sets that correspond to the same underlying individual. One approach is to compare all data units from file A with all units in file B and classify all of the comparison outcomes to decide whether or not the records match. In a perfect statistical world the comparison would end with a clear determination of links and nonlinks.</p>
<p>Alas, a perfect world does not exist, and there is likely to be noise in the variables that are common to both data sets and that will be the main identifiers for the record linkage. Although the original files A and B are the starting point, the identifiers must be preprocessed before they can be compared. Determining identifiers for the linkage and deciding on the associated cleaning steps are extremely important, as they result in a necessary reduction of the possible search space.</p>
<p>In the next section we begin our overview of the record linkage process with a discussion of the main steps in data preprocessing. This is followed by a section on approaches to record linkage that includes rule-based, probabilistic, and machine learning algorithms. Next we cover classification and evaluation of links, and we conclude with a discussion of data privacy in record linkage.</p>
</div>
<div id="preprocessing-data-for-record-linkage" class="section level2">
<h2><span class="header-section-number">3.3</span> Preprocessing data for record linkage</h2>
<p>As noted in the introductory chapter, all data work involves preprocessing, and data that need to be linked is no exception. Preprocessing refers to a workflow that transforms messy and noisy data into a well-defined, clearly structured, and quality-tested data set. Elsewhere in this book, we discuss general strategies for data preprocessing.<a href="#fn16" class="footnoteRef" id="fnref16"><sup>16</sup></a> In this section, we focus specifically on preprocessing steps relating to the choice of input fields for the record linkage algorithm. Preprocessing for any kind of a new data set is a complex and time-consuming process because it is “hands-on”: it requires judgment and cannot be effectively automated. It may be tempting to minimize this demanding work under the assumption that the record linkage algorithm will account for issues in the data, but it is difficult to overstate the value of preprocessing for record linkage quality. As Winkler notes: “In situations of reasonably high-quality data, preprocessing can yield a greater improvement in matching efficiency than string comparators and ‘optimized’ parameters. In some situations, 90% of the improvement in matching efficiency may be due to preprocessing” <span class="citation">(Winkler <a href="#ref-winkler09">2009</a>)</span>.</p>
<p>The first step in record linkage is to develop link keys, which are the record fields that will be used to estimate if there is a link between two records. These can include common identifiers like first and last name. Survey and administrative data sets may include a number of clearly identifying variables like address, birth date, and sex. Other data sets, like transaction records or social media data, often will not include address or birth date but may still include other identifying fields like occupation, a list of interests, or connections on a social network. Consider this chapter’s illustrative example of the US Patent and Trademark Office (USPTO) data <span class="citation">(Ventura, Nugent, and Fuchs <a href="#ref-ventura2015seeing">2015</a>)</span>:</p>
<blockquote>
<p>USPTO maintains an online database of all patents issued in the United States. In addition to identifying information about the patent, the database contains each patent’s list of inventors and assignees, the companies, organizations, individuals, or government agencies to which the patent is assigned. … However, inventors and assignees in the USPTO database are not given unique identification numbers, making it difficult to track inventors and assignees across their patents or link their information to other data sources.</p>
</blockquote>
<p>There are some basic precepts that are useful when considering identifying fields. The more different values a field can take, the less likely it is that two randomly chosen individuals in the population will agree on those values. Therefore, fields that exhibit a wider range of values are more powerful as link keys: names are much better link keys than sex or year of birth.</p>
<hr />
<p><strong>Example: Link keys in practice</strong></p>
<p>“A Harvard professor has re-identified the names of more than 40 percent of a sample of anonymous participants in a high-profile DNA study, highlighting the dangers that ever greater amounts of personal data available in the Internet era could unravel personal secrets. … Of the 1,130 volunteers Sweeney and her team reviewed, about 579 provided zip code, date of birth and gender, the three key pieces of information she needs to identify anonymous people combined with information from voter rolls or other public records. Of these, Sweeney succeeded in naming 241, or 42 percent of the total. The Personal Genome Project confirmed that 97 percent of the names matched those in its database if nicknames and first name variations were included” <span class="citation">(Tanner <a href="#ref-forbesharvard">2013</a>)</span>.</p>
<hr />
<p>Complex link keys like addresses can be broken down into components so that the components can be compared independently of one another. This way, errors due to data quality can be further isolated. For example, assigning a single comparison value to the complex fields “1600 Pennsylvania” and “160 Pennsylvania Ave” is less informative than assigning separate comparison values to the street number and street name portions of those fields. A record linkage algorithm that uses the decomposed field can make more nuanced distinctions by assigning different weights to errors in each component.</p>
<p>Sometimes a data set can include different variants of a field, like legal first name and nickname. In these cases match rates can be improved by including all variants of the field in the record comparison. For example, if only the first list includes both variants, and the second list has a single “first name” field that could be either a legal first name or a nickname, then match rates can be improved by comparing both variants and then keeping the better of the two comparison outcomes. It is important to remember, however, that some record linkage algorithms expect field comparisons to be somewhat independent. In our example, using the outcome from both comparisons as separate inputs into the probabilistic model we describe below may result in a higher rate of false negatives. If a record has the same value in the legal name and nickname fields, and if that value happens to agree with the first name field in the second file, then the agreement is being double-counted. By the same token, if a person in the first list has a nickname that differs significantly from their legal first name, then a comparison of that record to the corresponding record will unfairly penalize the outcome because at least one of those name comparisons will show a low level of agreement.</p>
<p>Preprocessing serves two purposes in record linkage. First, to correct for issues in data quality that we described above. Second, to account for the different ways that the input files were generated, which may result in the same underlying data being recorded on different scales or according to different conventions.</p>
<p>Once preprocessing is finished, it is possible to start linking the records in the different data sets. In the next section we describe a technique to improve the efficiency of the matching step.</p>
</div>
<div id="S:indexing" class="section level2">
<h2><span class="header-section-number">3.4</span> Indexing and blocking</h2>
<p>There is a practical challenge to consider when comparing the records in two files. If both files are roughly the same size, say <span class="math inline">\(100\)</span> records in the first and <span class="math inline">\(100\)</span> records in the second file, then there are <span class="math inline">\(10{,}000\)</span> possible comparisons, because the number of pairs is the product of the number of records in each file. More generally, if the number of records in each file is approximately <span class="math inline">\(n\)</span>, then the total number of possible record comparisons is approximately <span class="math inline">\(n^2\)</span>. Assuming that there are no duplicate records in the input files, the proportion of record comparisons that correspond to a link is only <span class="math inline">\(1/n\)</span>. If we naively proceed with all <span class="math inline">\(n^2\)</span> possible comparisons, the linkage algorithm will spend the bulk of its time comparing records that are not matches. Thus it is possible to speed up record linkage significantly by skipping comparisons between record pairs that are not likely to be linked.</p>
<p>Indexing refers to techniques that determine which of the possible comparisons will be made in a record linkage application. The most used technique for indexing is blocking. In this approach you construct a “blocking key” for each record by concatenating fields or parts of fields. Two records with identical blocking keys are said to be in the same block, and only records in the same block are compared. This technique is effective because performing an exact comparison of two blocking keys is a relatively quick operation compared to a full record comparison, which may involve multiple applications of a fuzzy string comparator.</p>
<hr />
<p><strong>Example: Blocking in practice</strong></p>
<p>Given two lists of individuals, one might construct the blocking key by concatenating the first letter of the last name and the postal code and then “blocking” on first character of last name and postal code. This reduces the total number of comparisons by only comparing those individuals in the two files who live in the same locality and whose last names begin with the same letter.</p>
<hr />
<p>There are important considerations when choosing the blocking key. First, the choice of blocking key creates a potential bias in the linked data because true matches that do not share the same blocking key will not be found. In the example, the blocking strategy could fail to match records for individuals whose last name changed or who moved. Second, because blocking keys are compared exactly, there is an implicit assumption that the included fields will not have typos or other data entry errors. In practice, however, the blocking fields will exhibit typos. If those typos are not uniformly distributed over the population, then there is again the possibility of bias in the linked data set<a href="#fn17" class="footnoteRef" id="fnref17"><sup>17</sup></a>. One simple strategy for dealing with imperfect blocking keys is to implement multiple rounds of blocking and matching. After the first set of matches is produced, a new blocking strategy is deployed to search for additional matches in the remaining record pairs.</p>
<p>Blocking based on exact field agreements is common in practice, but there are other approaches to indexing that attempt to be more error tolerant. For example, one may use clustering algorithms to identify sets of similar records. In this approach an index key, which is analogous to the blocking key above, is generated for both data sets and then the keys are combined into a single list. A distance function must be chosen and pairwise distances computed for all keys. The clustering algorithm is then applied to the combined list, and only record pairs that are assigned to the same cluster are compared. This is a theoretically appealing approach but it has the drawback that the similarity metric has to be computed for all pairs of records. Even so, computing the similarity measure for a pair of blocking keys is likely to be cheaper than computing the full record comparison, so there is still a gain in efficiency. Whang et al. <span class="citation">(<a href="#ref-whang2009entity">2009</a>)</span> provide a nice review of indexing approaches.</p>
<p>In addition to reducing the computational burden of record linkage, indexing plays an important secondary role. Once implemented, the fraction of comparisons made that correspond to true links will be significantly higher. For some record linkage approaches that use an algorithm to find optimal parameters—like the probabilistic approach—having a larger ratio of matches to nonmatches will produce a better result.</p>
</div>
<div id="matching" class="section level2">
<h2><span class="header-section-number">3.5</span> Matching</h2>
<p>The purpose of a record linkage algorithm is to examine pairs of records and make a prediction as to whether they correspond to the same underlying entity. (There are some sophisticated algorithms that examine sets of more than two records at a time <span class="citation">(Steorts, Hall, and Fienberg <a href="#ref-steorts2014smered">2014</a>)</span>, but pairwise comparison remains the standard approach.) At the core of every record linkage algorithm is a function that compares two records and outputs a “score” that quantifies the similarity between those records. Mathematically, the match score is a function of the output from individual field comparisons: agreement in the first name field, agreement in the last name field, etc. Field comparisons may be binary—indicating agreement or disagreement—or they may output a range of values indicating different levels of agreement. There are a variety of methods in the statistical and computer science literature that can be used to generate a match score, including nearest-neighbor matching, regression-based matching, and propensity score matching. The probabilistic approach to record linkage defines the match score in terms of a likelihood ratio <span class="citation">(Fellegi and Sunter <a href="#ref-FS69">1969</a>)</span>.</p>
<hr />
<p><strong>Example: Matching in practice</strong></p>
<p>Long strings, such as assignee and inventor names, are susceptible to typographical errors and name variations. For example, none of Sony Corporation, Sony Corporatoin and Sony Corp. will match using simple exact matching. Similarly, David vs. Dave would not match <span class="citation">(Ventura, Nugent, and Fuchs <a href="#ref-ventura2015seeing">2015</a>)</span>.</p>
<hr />
<p>Comparing fields whose values are continuous is straightforward: often one can simply take the absolute difference as the comparison value. Comparing character fields in a rigorous way is more complicated. For this purpose, different mathematical definitions of the distance between two character fields have been defined. Edit distance, for example, is defined as the minimum number of edit operations—chosen from a set of allowed operations—needed to convert one string to another. When the set of allowed edit operations is single-character insertions, deletions, and substitutions, the corresponding edit distance is also known as the Levenshtein distance. When transposition of adjacent characters is allowed in addition to those operations, the corresponding edit distance is called the Levenshtein–Damerau distance.</p>
<p>Edit distance is appealing because of its intuitive definition, but it is not the most efficient string distance to compute. Another standard string distance known as Jaro–Winkler distance was developed with record linkage applications in mind and is faster to compute. This is an important consideration because in a typical record linkage application most of the algorithm run time will be spent performing field comparisons. The definition of Jaro–Winkler distance is less intuitive than edit distance, but it works as expected: words with more characters in common will have a higher Jaro–Winkler value than those with fewer characters in common. The output value is normalized to fall between 0 and 1. Because of its history in record linkage applications, there are some standard variants of Jaro–Winkler distance that may be implemented in record linkage software. Some variants boost the weight given to agreement in the first few characters of the strings being compared. Others decrease the score penalty for letter substitutions that arise from common typos.</p>
<p>Once the field comparisons are computed, they must be combined to produce a final prediction of match status. In the following sections we describe three types of record linkage algorithms: rule-based, probabilistic, and machine learning.</p>
<div id="rule-based-approaches" class="section level3">
<h3><span class="header-section-number">3.5.1</span> Rule-based approaches</h3>
<p>A natural starting place is for a data expert to create a set of ad hoc rules that determine which pairs of records should be linked. In the classical record linkage setting where the two files have a number of identifying fields in common, this is not the optimal approach. However, if there are few fields in common but each file contains auxiliary fields that may inform a linkage decision, then an ad hoc approach may be appropriate.</p>
<hr />
<p><strong>Example: Linking in practice</strong></p>
<p>Consider the problem of linking two lists of individuals where both lists contain first name, last name, and year of birth. Here is one possible linkage rule: link all pairs of records such that</p>
<ul>
<li><p>the Jaro–Winkler comparison of first names is greater than 0.9</p></li>
<li><p>the Jaro–Winkler comparison of last names is greater than 0.9</p></li>
<li><p>the first three digits of the year of birth are the same.</p></li>
</ul>
<p>The result will depend on the rate of data errors in the year of birth field and typos in the name fields.</p>
<hr />
<p>By <em>auxiliary field</em> we mean data fields that do not appear on both data sets, but which may nonetheless provide information about whether records should be linked. Consider a situation in which the first list includes an occupation field and the second list includes educational history. In that case one might create additional rules to eliminate matches where the education was deemed to be an unlikely fit for the occupation.</p>
<p>This method may be attractive if it produces a reasonable-looking set of links from intuitive rules, but there are several pitfalls. As the number of rules grows it becomes harder to understand the ways that the different rules interact to produce the final set of links. There is no notion of a threshold that can be increased or decreased depending on the tolerance for false positive and false negative errors. The rules themselves are not chosen to satisfy any kind of optimality, unlike the probabilistic and machine learning methods. Instead, they reflect the practitioner’s domain knowledge about the data sets.</p>
</div>
<div id="probabilistic-record-linkage" class="section level3">
<h3><span class="header-section-number">3.5.2</span> Probabilistic record linkage</h3>
<p>In this section we describe the probabilistic approach to record linkage, also known as the Fellegi–Sunter algorithm <span class="citation">(Fellegi and Sunter <a href="#ref-FS69">1969</a>)</span>. This approach dominates in traditional record linkage applications and remains an effective and efficient way to solve the record linkage problem today.</p>
<p>In this section we give a somewhat formal definition of the statistical model underlying the algorithm. By understanding this model, one is better equipped to define link keys and record comparisons in an optimal way.</p>
<hr />
<p><strong>Example: Usefulness of probabilistic record linkage</strong></p>
<p>In practice, it is typically the case that a researcher will want to combine two or more data sets containing records for the same individuals or units that possibly come from different sources. Unless the sources all contain the same unique identifiers, linkage will likely require matching on standardized text strings. Even standardized data are likely to contain small differences that preclude exact matching as in the matching example above. The Census Bureau’s Longitudinal Business Database (LBD) links establishment records from administrative and survey sources. Exact numeric identifiers do most of the heavy lifting, but mergers, acquisitions, and other actions can break these linkages. Probabilistic record linkage on company names and/or addresses is used to fix these broken linkages that bias statistics on business dynamics <span class="citation">(Jarmin and Miranda <a href="#ref-jarmin2002longitudinal">2002</a>)</span>.</p>
<hr />
<p>Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be two lists of individuals whom we wish to link. The product set <span class="math inline">\(A \times B\)</span> contains all possible pairs of records where the first element of the pair comes from <span class="math inline">\(A\)</span> and the second element of the pair comes from <span class="math inline">\(B\)</span>. A fraction of these pairs will be matches, meaning that both records in the pair represent the same underlying individual, but the vast majority of them will be nonmatches. In other words, <span class="math inline">\(A \times B\)</span> is the disjoint union of the set of matches <span class="math inline">\(M\)</span> and the set of nonmatches <span class="math inline">\(U\)</span>, a fact that we denote formally by <span class="math inline">\(A \times B = M \cup U\)</span>.</p>
<p>Let <span class="math inline">\(\gamma\)</span> be a vector-valued function on <span class="math inline">\(A \times B\)</span> such that, for <span class="math inline">\(a \in A\)</span> and <span class="math inline">\(b \in B\)</span>, <span class="math inline">\(\gamma(a, b)\)</span> represents the outcome of a set of field comparisons between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. For example, if both <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> contain data on individuals’ first names, last names, and cities of residence, then <span class="math inline">\(\gamma\)</span> could be a vector of three binary values representing agreement in first name, last name, and city. In that case <span class="math inline">\(\gamma(a, b) = (1, 1, 0)\)</span> would mean that the records <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> agree on first name and last name, but disagree on city of residence.</p>
<p>For this model, the comparison outcomes in <span class="math inline">\(\gamma(a, b)\)</span> are not required to be binary, but they do have to be categorical: each component of <span class="math inline">\(\gamma(a, b)\)</span> should take only finitely many values. This means that a continuous comparison outcome—such as output from the string comparator—has to be converted to an ordinal value representing levels of agreement. For example, one might create a three-level comparison, using one level for exact agreement, one level for approximate agreement defined as a Jaro–Winkler score greater than 0.85, and one level for nonagreement corresponding to a Jaro–Winkler score less than 0.85.</p>
<p>If a variable being used in the comparison has a significant number of missing values, it can help to create a comparison outcome level to indicate missingness. Consider two data sets that both have middle initial fields, and suppose that in one of the data sets the middle initial is filled in only about half of the time. When comparing records, the case where both middle initials are filled in but are not the same should be treated differently from the case where one of the middle initials is blank, because the first case provides more evidence that the records do not correspond to the same person. We handle this in the model by defining a three-level comparison for the middle initial, with levels to indicate “equal,” “not equal,” and “missing.”</p>
<p>Probabilistic record linkage works by weighing the probability of seeing the result <span class="math inline">\(\gamma(a, b)\)</span> if <span class="math inline">\((a, b)\)</span> belongs to the set of matches <span class="math inline">\(M\)</span> against the probability of seeing the result if <span class="math inline">\((a, b)\)</span> belongs to the set of nonmatches <span class="math inline">\(U\)</span>. Conditional on <span class="math inline">\(M\)</span> or <span class="math inline">\(U\)</span>, the distribution of the individual comparisons defined by <span class="math inline">\(\gamma\)</span> are assumed to be mutually independent. The parameters that define the marginal distributions of <span class="math inline">\(\gamma | M\)</span> are called <em><span class="math inline">\(m\)</span>-weights</em>, and similarly the marginal distributions of <span class="math inline">\(\gamma | U\)</span> are called <em><span class="math inline">\(u\)</span>-weights</em>.</p>
<p>In order to apply the Fellegi–Sunter method, it is necessary to choose values for these parameters, <span class="math inline">\(m\)</span>-weights and <span class="math inline">\(u\)</span>-weights. With labeled data—a pair of lists for which the match status is known—it is straightforward to solve for optimal values. Training data are not usually available, however, and the typical approach is to use expectation maximization to find optimal values.</p>
<p>We have noted that primary motivation for record linkage is to create a linked data set for analysis that will have a richer than either of the input data sets alone. A natural application is to perform a linear regression using a combination of variables from both files as predictors. With all record linkage approaches it is a challenge to understand how errors from the linkage process will manifest in the regression. Probabilistic record linkage has an advantage over rule-based and machine learning approaches in that there are theoretical results concerning coefficient bias and errors <span class="citation">(Scheuren and Winkler <a href="#ref-scheuren1993regression">1993</a>; Lahiri and Larsen <a href="#ref-lahiri2005regression">2005</a>)</span>. More recently, Chipperfield and Chambers have developed an approach based on the bootstrap to account for record linkage errors when making inferences for cross-tabulated</p>
</div>
<div id="machine-learning-approaches-to-record-linkage" class="section level3">
<h3><span class="header-section-number">3.5.3</span> Machine learning approaches to record linkage</h3>
<p>Computer scientists have contributed extensively in parallel literature focused on linking large data sets <span class="citation">(Christen <a href="#ref-christen2012data">2012</a><a href="#ref-christen2012data">b</a>)</span>. Their focus is on identifying potential links using approaches that are fast, adaptive, and scalable, and approaches are developed based on work in network analysis and machine learning.</p>
<p>While simple blocking as described in Section <a href="chap-link.html#S:indexing">Indexing and Blocking</a> is standard in Fellegi–Sunter applications, machine learning based approaches are likely to use the more sophisticated clustering approach to indexing. Indexing may also use network information to include, for example, records for individuals that have a similar place in a social graph. When linking lists of researchers, one might specify that comparisons should be made between records that share the same address, have patents in the same patent class, or have overlapping sets of coinventors. These approaches are known as semantic blocking, and the computational requirements are similar to standard blocking <span class="citation">(Christen <a href="#ref-christen2012data">2012</a><a href="#ref-christen2012data">b</a>)</span>.</p>
<p>In recent years machine learning approaches<a href="#fn18" class="footnoteRef" id="fnref18"><sup>18</sup></a> have been applied to record Linkage. As Wick et al. <span class="citation">(<a href="#ref-wick2013joint">2013</a>)</span> note:</p>
<blockquote>
<p>Entity resolution, the task of automatically determining which mentions refer to the same real-world entity, is a crucial aspect of knowledge base construction and management. However, performing entity resolution at large scales is challenging because (1) the inference algorithms must cope with unavoidable system scalability issues and (2) the search space grows exponentially in the number of mentions. Current conventional wisdom declares that performing coreference at these scales requires decomposing the problem by first solving the simpler task of entity-linking (matching a set of mentions to a known set of KB entities), and then performing entity discovery as a post-processing step (to identify new entities not present in the KB). However, we argue that this traditional approach is harmful to both entity-linking and overall coreference accuracy. Therefore, we embrace the challenge of jointly modeling entity-linking and entity discovery as a single entity resolution problem.</p>
</blockquote>
<p>Figure <a href="chap-link.html#fig:fig3-2">3.2</a> provides a useful comparison between classical record linkage and learning-based approaches. In machine learning there is a statistical model and an algorithm for “learning” the optimal set of parameters to use. The learning algorithm relies on a training data set. In record linkage, this would be a curated data set with true and false matches labeled as such. See <span class="citation">(Ventura, Nugent, and Fuchs <a href="#ref-ventura2015seeing">2015</a>)</span> for an example and a discussion of how a training data set was created for the problem of disambiguating inventors in the USPTO database. Once optimal parameters are computed from the training data, the predictive model can be applied to unlabeled data to find new links. The quality of the training data set is critical; the model is only as good as the data it is trained on.</p>
<div class="figure" style="text-align: center"><span id="fig:fig3-2"></span>
<img src="ChapterLinkage/figures/fig3-2.png" alt="Probabilistic (left) vs. machine learning (right) approaches to linking. Source: @kopcke2010evaluation" width="70%" />
<p class="caption">
Figure 3.2: Probabilistic (left) vs. machine learning (right) approaches to linking. Source: <span class="citation">Köpcke, Thor, and Rahm (<a href="#ref-kopcke2010evaluation">2010</a>)</span>
</p>
</div>
<p>As shown in Figure <a href="chap-link.html#fig:fig3-2">3.2</a>, a major difference between probabilistic and machine learning approaches is the need for labeled training data to implement the latter approach. Usually training data are created through a painstaking process of clerical review. After an initial round of record linkage, a sample of record pairs that are not clearly matches or nonmatches is given to a research assistant who makes the final determination. In some cases it is possible to create training data by automated means. For example, when there is a subset of the complete data that contains strongly identifying fields. Suppose that both of the candidate lists contain name and date of birth fields and that in the first list the date of birth data are complete, but in the second list only about 10% of records contain date of birth. For reasonably sized lists, name and date of birth together will be a nearly unique identifier. It is then possible to perform probabilistic record linkage on the subset of records with date of birth and be confident that the error rates would be small. If the subset of records with date of birth is representative of the complete data set, then the output from the probabilistic record linkage can be used as “truth” data.</p>
<p>Given a quality training data set, machine learning approaches may have advantages over probabilistic record linkage. There are many published studies on the effectiveness of random forests and other machine learning algorithms for record linkage. Christen and Ahmed et al. provide some pointers <span class="citation">(Christen <a href="#ref-christen2012survey">2012</a><a href="#ref-christen2012survey">a</a>; Elmagarmid, Ipeirotis, and Verykios <a href="#ref-elmagarmid2007duplicate">2007</a>)</span>.</p>
</div>
<div id="disambiguating-networks" class="section level3">
<h3><span class="header-section-number">3.5.4</span> Disambiguating networks</h3>
<p>The problem of disambiguating entities in a network is closely related to record linkage: in both cases the goal is to consolidate multiple records corresponding to the same entity. Rather than finding the same entity in two data sets, however, the goal in network disambiguation is to consolidate duplicate records in a network data set. By network we mean that the data set contains not only typical record fields like names and addresses but also information about how entities relate to one another: entities may be coauthors, coinventors, or simply friends in a social network.</p>
<p>The record linkage techniques that we have described in this chapter can be applied to disambiguate a network. To do so, one must convert the network to a form that can be used as input into a record linkage algorithm. For example, when disambiguating a social network one might define a field comparison whose output gives the fraction of friends in common between two records. Ventura et al. demonstrated the relative effectiveness of the probabilistic method and machine learning approaches to disambiguating a database of inventors in the USPTO database <span class="citation">(Ventura, Nugent, and Fuchs <a href="#ref-ventura2015seeing">2015</a>)</span>. Another approach is to apply clustering algorithms from the computer science literature to identify groups of records that are likely to refer to the same entity. Huang et al. <span class="citation">(<a href="#ref-HEG06">2006</a>)</span> have developed a successful method based on an efficient computation of distance between individuals in the network. These distances are then fed into the DBSCAN clustering algorithm to identify unique entities.</p>
</div>
</div>
<div id="classification" class="section level2">
<h2><span class="header-section-number">3.6</span> Classification</h2>
<p>Once the match score for a pair of records has been computed using the probabilistic or random forest method, a decision has to be made whether the pair should be linked. This requires classifying the pair as either a “true” or a “false” match. In most cases, a third classification is required—sending for manual review and classification.</p>
<div id="S:thresholds" class="section level3">
<h3><span class="header-section-number">3.6.1</span> Thresholds</h3>
<p>In the probabilistic and random forest approaches, both of which output a “match score” value, a classification is made by establishing a threshold <span class="math inline">\(T\)</span> such that all records with a match score greater than <span class="math inline">\(T\)</span> are declared to be links. Because of the way these algorithms are defined, the match scores are not meaningful by themselves and the threshold used for one linkage application may not be appropriate for another application. Instead, the classification threshold must be established by reviewing the model output.</p>
<p>Typically one creates an output file that includes pairs of records that were compared along with the match score. The file is sorted by match score and the reviewer begins to scan the file from the highest match scores to the lowest. For the highest match scores the record pairs will agree on all fields and there is usually no question about the records being linked. However, as the scores decrease the reviewer will see more record pairs whose match status is unclear (or that are clearly nonmatches) mixed in with the clear matches. There are a number of ways to proceed, depending on the resources available and the goal of the project.</p>
<p>Rather than set a single threshold, the reviewer may set two thresholds <span class="math inline">\(T_1 &gt; T_2\)</span>. Record pairs with a match score greater than <span class="math inline">\(T_1\)</span> are marked as matches and removed from further consideration. The set of record pairs with a match score between <span class="math inline">\(T_1\)</span> and <span class="math inline">\(T_2\)</span> are believed to contain significant numbers of matches and nonmatches. These are sent to clerical review, meaning that research assistants will make a final determination of match status. The final set of links will include clear matches with a score greater than <span class="math inline">\(T_1\)</span> as well as the record pairs that pass clerical review. If the resources are available for this approach and the initial threshold <span class="math inline">\(T_1\)</span> is set sufficiently high, then the resulting data set will contain a minimal number of false positive links. The collection of record pairs with match scores between <span class="math inline">\(T_1\)</span> and <span class="math inline">\(T_2\)</span> is sometimes referred to as the clerical review region.</p>
<p>The clerical review region generally contains many more pairs than the set of clear matches, and it can be expensive and time-consuming to review each pair. Therefore, a second approach is to establish tentative threshold <span class="math inline">\(T\)</span> and send only a sample of record pairs with scores in a neighborhood of <span class="math inline">\(T\)</span> to clerical review. This results in data on the relative numbers of true matches and true nonmatches at different score levels, as well as the characteristics of record pairs that appear at a given level. Based on the review and the relative tolerance for false positive errors and false negative errors, a final threshold <span class="math inline">\(T&#39;\)</span> is set such that pairs with a score greater than <span class="math inline">\(T&#39;\)</span> are considered to be matches.</p>
<p>After viewing the results of the clerical review, it may be determined that the parameters to the record linkage algorithm could be improved to create a clearer delineation between matches and nonmatches. For example, a research assistant may determine that many potential false positives appear near the tentative threshold because the current set of record linkage parameters is giving too much weight to agreement in first name. In this case the reviewer may decide to update the record linkage model to produce an improved set of match scores. The update may consist in an ad hoc adjustment of parameters, or the result of the clerical review may be used as training data and the parameter-fitting algorithm may be run again. An iterative approach like this is common when first linking two data sets because the clerical review process can improve one’s understanding of the data sets involved.</p>
<p>Setting the threshold value higher will reduce the number of false positives (record pairs for which a link is incorrectly predicted) while increasing the number of false negatives (record pairs that should be linked but for which a link is not predicted). The proper tradeoff between false positive and false negative error rates will depend on the particular application and the associated loss function, but there are some general concerns to keep in mind. Both types of errors create bias, which can impact the generalizability of analyses conducted on the linked data set. Consider a simple regression on the linked data that includes fields from both data sets. If the threshold is too high, then the linked data will be biased toward records with no data entry errors or missing values, and whose fields did not change over time. This set of records may not be representative of the population as a whole. If a low threshold is used, then the set of linked records will contain more pairs that are not true links and the variables measured in those records are independent of each other. Including these records in a regression amounts to adding statistical noise to the data.</p>
</div>
<div id="one-to-one-links" class="section level3">
<h3><span class="header-section-number">3.6.2</span> One-to-one links</h3>
<p>In the probabilistic and machine learning approaches to record linkage that we have described, each record pair is compared and a link is predicted independently of all other record pairs. Because of the independence of comparisons, one record in the first file may be predicted to link to multiple records in the second file. Under the assumption that each input file has been deduplicated, at most one of these predictions can correspond to a true link. For many applications it is preferable to extract a set of “best” links with the property that each record in one file links to at most one record in the second file. A set of links with this property is said to be one-to-one.</p>
<p>One possible definition of “best” is a set of one-to-one links such that the sum of the match scores of all included links is maximal. This is an example of what is known as the <em>assignment problem</em> in combinatorial optimization. In the linear case above, where we care about the sum of match scores, the problem can be solved exactly using the Hungarian algorithm <span class="citation">(Kuhn <a href="#ref-kuhn2005hungarian">2005</a>)</span>.</p>
</div>
</div>
<div id="record-linkage-and-data-protection" class="section level2">
<h2><span class="header-section-number">3.7</span> Record linkage and data protection</h2>
<p>In many social science applications data sets there is no need for data to include identifying fields like names and addresses. These fields may be left out intentionally out of concern for privacy<a href="#fn19" class="footnoteRef" id="fnref19"><sup>19</sup></a>, or they may simply be irrelevant to the research question. For record linkage, however, names and addresses are among the best possible identifiers. We describe two approaches to the problem of balancing needs for both effective record linkage and privacy.</p>
<p>The first approach is to establish a trusted third party or safe center. The concept of trusted third parties (TTPs) is well known in cryptography. In the case of record linkage, a third party takes a place between the data owners and the data users, and it is this third party that actually performs the linkage work. Both the data owners and data users trust the third party in the sense that it assumes responsibility for data protection (data owners) and data competence (data users) at the same time. No party other than the TTP learns about the private data of the other parties. After record linkage only the linked records are revealed, with no identifiers attached. The TTP ensures that the released linked data set cannot be relinked to any of the source data sets. Possible third parties are safe centers, which are operated by lawyers, or official trusted institutions like the US Census Bureau.</p>
<p>The second approach is known as privacy-preserving record linkage. The goal of this approach is to find the same individual in separate data files without revealing the identity of the individual <span class="citation">(Clifton et al. <a href="#ref-Clifton06">2006</a>)</span>. In privacy-preserving record linkage, cryptographic procedures are used to encrypt or hash identifiers before they are shared for record linkage. Many of these procedures require exact matching of the identifiers, however, and do not tolerate any errors in the original identifiers. This leads to information loss because it is not possible to account for typos or other small variations in hashed fields. To account for this, Schnell has developed a method to calculate string similarity of encrypted fields using bloom filters <span class="citation">(Schnell <a href="#ref-schnell2014efficient">2014</a>; Schnell, Bachteler, and Reiher <a href="#ref-schnell2009privacy">2009</a>)</span>.</p>
<p>In many countries these approaches are combined. For example, when the UK established the ADRN, the latter established the concept of trusted third parties. That third party is provided with data in which identifying fields have been hashed. This solves the challenge of trust between the different parties. Some authors argue that transparency of data use and informed consent will help to build trust. In the context of big data this is more challenging<a href="#fn20" class="footnoteRef" id="fnref20"><sup>20</sup></a>.</p>
</div>
<div id="summary-1" class="section level2">
<h2><span class="header-section-number">3.8</span> Summary</h2>
<p>Accurate record linkage is critical to creating high-quality data sets for analysis. However, outside of a few small centers for record linkage research, linking data sets historically relied on artisan approaches, particularly for parsing and cleaning data sets. As the creation and use of big data increases, so does the need for systematic record linkage. The history of record linkage is long by computer science standards, but new data challenges encourage the development of new approaches like machine learning methods, clustering algorithms, and privacy-preserving record linkage.</p>
<p>Record linkage stands on the boundary between statistics, information technology, and privacy. We are confident that there will continue to be exciting developments in this field in the years to come.</p>
</div>
<div id="resources" class="section level2">
<h2><span class="header-section-number">3.9</span> Resources</h2>
<p>Out of many excellent resources on the subject, we note the following:</p>
<ul>
<li><p>We strongly recommend Christen’s book <span class="citation">(Christen <a href="#ref-christen2012data">2012</a><a href="#ref-christen2012data">b</a>)</span>.</p></li>
<li><p>There is a wealth of information available on the ADRN website <span class="citation">(Economic and Social Research Council <a href="#ref-EconomicandSocialResearchCouncil2016">2016</a>)</span>.</p></li>
<li><p>Winkler has a series of high-quality survey articles <span class="citation">(Winkler <a href="#ref-WICS:WICS1317">2014</a>)</span>.</p></li>
<li><p>The German Record Linkage Center is a resource for research, software, and ongoing conference activities <span class="citation">(Schnell <a href="#ref-Schnell2016">2016</a>)</span>.</p></li>
<li><p>The <em>Record Linkage</em> workbook of Chapter <a href="chap-workbooks.html#chap:workbooks">Workbooks</a> provides an introduction to probabilistic record linkage with Python.<a href="#fn21" class="footnoteRef" id="fnref21"><sup>21</sup></a></p></li>
</ul>

<!-- % done -->
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-abowd2004integrated">
<p>Abowd, John M., John Haltiwanger, and Julia Lane. 2004. “Integrated Longitudinal Employer-Employee Data for the United States.” <em>American Economic Review</em> 94 (2). JSTOR: 224–29.</p>
</div>
<div id="ref-abowd2006final">
<p>Abowd, John M., Martha Stinson, and Gary Benedetto. 2006. “Final Report to the Social Security Administration on the SIPP/SSA/IRS Public Use File Project.” Suitland, MD: Census Bureau, Longitudinal Employer-Household Dynamics Program.</p>
</div>
<div id="ref-christen2012survey">
<p>Christen, Peter. 2012a. “A Survey of Indexing Techniques for Scalable Record Linkage and Deduplication.” <em>IEEE Transactions on Knowledge and Data Engineering</em> 24 (9). IEEE: 1537–55.</p>
</div>
<div id="ref-christen2012data">
<p>Christen, Peter. 2012b. <em>Data Matching: Concepts and Techniques for Record Linkage, Entity Resolution, and Duplicate Detection</em>. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-Clifton06">
<p>Clifton, C., M. Kantarcioglu, A. Doan, G. Schadow, J. Vaidya, A.K. Elmagarmid, and D. Suciu. 2006. “Privacy-Preserving Data Integration and Sharing.” In <em>9th Acm Sigmod Workshop on Research Issues in Data Mining and Knowledge Discovery</em>, edited by G. Das, B. Liu, and P. S. Yu, 19–26. ACM.</p>
</div>
<div id="ref-EconomicandSocialResearchCouncil2016">
<p>Economic and Social Research Council. 2016. “Administrative Data Research Network.”</p>
</div>
<div id="ref-elmagarmid2007duplicate">
<p>Elmagarmid, Ahmed K., Panagiotis G. Ipeirotis, and Vassilios S. Verykios. 2007. “Duplicate Record Detection: A Survey.” <em>IEEE Transactions on Knowledge and Data Engineering</em> 19 (1). IEEE: 1–16.</p>
</div>
<div id="ref-FS69">
<p>Fellegi, Ivan P., and Alan B. Sunter. 1969. “A Theory for Record Linkage.” <em>Journal of the American Statistical Association</em> 64 (328). Taylor &amp; Francis Group: 1183–1210.</p>
</div>
<div id="ref-Glennon2019">
<p>Glennon, Britta. 2019. “How Do Restrictions on High-Skilled Immigration Affect Offshoring? Evidence from the H-1b Program.” <a href="http://brittaglennon.com/research/" class="uri">http://brittaglennon.com/research/</a>.</p>
</div>
<div id="ref-herzog2007data">
<p>Herzog, Thomas N, Fritz J Scheuren, and William E Winkler. 2007. <em>Data Quality and Record Linkage Techniques</em>. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-HEG06">
<p>Huang, Jian, Seyda Ertekin, and C. Lee Giles. 2006. “Efficient Name Disambiguation for Large-Scale Databases.” In <em>Knowledge Discovery in Databases: PKDD 2006</em>, 536–44. Springer.</p>
</div>
<div id="ref-jarmin2002longitudinal">
<p>Jarmin, Ron S., and Javier Miranda. 2002. “The Longitudinal Business Database.” Available at SSRN 2128793.</p>
</div>
<div id="ref-kopcke2010evaluation">
<p>Köpcke, Hanna, Andreas Thor, and Erhard Rahm. 2010. “Evaluation of Entity Resolution Approaches on Real-World Match Problems.” <em>Proceedings of the VLDB Endowment</em> 3 (1–2). VLDB Endowment: 484–93.</p>
</div>
<div id="ref-kuhn2005hungarian">
<p>Kuhn, H. W. 2005. “The Hungarian Method for the Assignment Problem.” <em>Naval Research Logistics</em> 52 (1). Wiley Online Library: 7–21.</p>
</div>
<div id="ref-lahiri2005regression">
<p>Lahiri, Partha, and Michael D Larsen. 2005. “Regression Analysis with Linked Data.” <em>Journal of the American Statistical Association</em> 100 (469). Taylor &amp; Francis: 222–30.</p>
</div>
<div id="ref-NCHS2019">
<p>National Center for Health Statistics. 2019. “The Linkage of National Center for Health Statistics Survey Data to the National Death Index – 2015 Linked Mortality File (Lmf): Methodology Overview and Analytic Considerations.” <a href="https://www.cdc.gov/nchs/data-linkage/mortality-methods.htm" class="uri">https://www.cdc.gov/nchs/data-linkage/mortality-methods.htm</a>.</p>
</div>
<div id="ref-Rodolfa2020">
<p>Rodolfa, K., E. Salomon, L. Haynes, I. Mendieta, J. Larson, and R. Ghani. 2020. “Predictive Fairness to Reduce Misdemeanor Recidivism Through Social Service Interventions.” In <em>Proceedings of the Acm Conference on Fairness, Accountability, and Transparency (Acm Fat*) 2020</em>.</p>
</div>
<div id="ref-scheuren1993regression">
<p>Scheuren, Fritz, and William E. Winkler. 1993. “Regression Analysis of Data Files That Are Computer Matched.” <em>Survey Methodology</em> 19 (1): 39–58.</p>
</div>
<div id="ref-schnell2014efficient">
<p>Schnell, Rainer. 2014. “An Efficient Privacy-Preserving Record Linkage Technique for Administrative Data and Censuses.” <em>Statistical Journal of the IAOS</em> 30: 263–70.</p>
</div>
<div id="ref-Schnell2016">
<p>Schnell, Rainer. 2016. “German Record Linkage Center.”</p>
</div>
<div id="ref-schnell2009privacy">
<p>Schnell, Rainer, Tobias Bachteler, and Jörg Reiher. 2009. “Privacy-Preserving Record Linkage Using Bloom Filters.” <em>BMC Medical Informatics and Decision Making</em> 9 (1). BioMed Central Ltd: 41.</p>
</div>
<div id="ref-steorts2014smered">
<p>Steorts, Rebecca C, Rob Hall, and Stephen E Fienberg. 2014. “SMERED: A Bayesian Approach to Graphical Record Linkage and de-Duplication.” Preprint, arXiv 1403.0211.</p>
</div>
<div id="ref-forbesharvard">
<p>Tanner, Adam. 2013. “Harvard Professor Re-Identifies Anonymous Volunteers in DNA Study.” <em>Forbes</em>, <a href="http://www.forbes.com/sites/adamtanner/2013/04/25/harvard-professor-re-identifies-anonymous-volunteers-in-dna-study/#6cc7f6b43e39" class="uri">http://www.forbes.com/sites/adamtanner/2013/04/25/harvard-professor-re-identifies-anonymous-volunteers-in-dna-study/#6cc7f6b43e39</a>.</p>
</div>
<div id="ref-ventura2015seeing">
<p>Ventura, Samuel L., Rebecca Nugent, and Erica R. H. Fuchs. 2015. “Seeing the Non-Stars:(Some) Sources of Bias in Past Disambiguation Approaches and a New Public Tool Leveraging Labeled Records.” <em>Research Policy</em>. Elsevier.</p>
</div>
<div id="ref-whang2009entity">
<p>Whang, Steven Euijong, David Menestrina, Georgia Koutrika, Martin Theobald, and Hector Garcia-Molina. 2009. “Entity Resolution with Iterative Blocking.” In <em>Proceedings of the 2009 Acm Sigmod International Conference on Management of Data</em>, 219–32. ACM.</p>
</div>
<div id="ref-wick2013joint">
<p>Wick, Michael, Sameer Singh, Harshal Pandya, and Andrew McCallum. 2013. “A Joint Model for Discovering and Linking Entities.” In <em>Proceedings of the 2013 Workshop on Automated Knowledge Base Construction</em>, 67–72. ACM.</p>
</div>
<div id="ref-winkler09">
<p>Winkler, William E. 2009. “Record Linkage.” In <em>Handbook of Statistics 29a, Sample Surveys: Design, Methods and Applications</em>, edited by D. Pfeffermann and C. R. Rao, 351–80. Elsevier.</p>
</div>
<div id="ref-WICS:WICS1317">
<p>Winkler, William E. 2014. “Matching and Record Linkage.” <em>Wiley Interdisciplinary Reviews: Computational Statistics</em> 6 (5). John Wiley &amp; Sons, Inc.: 313–25. doi:<a href="https://doi.org/10.1002/wics.1317">10.1002/wics.1317</a>.</p>
</div>
<div id="ref-zolas2015wrapping">
<p>Zolas, Nikolas, Nathan Goldschlag, Ron Jarmin, Paula Stephan, Jason Owen-Smith, Rebecca F Rosen, Barbara McFadden Allen, Bruce A Weinberg, and Julia Lane. 2015. “Wrapping It up in a Person: Examining Employment and Earnings Outcomes for Ph.D. Recipients.” <em>Science</em> 350 (6266). American Association for the Advancement of Science: 1367–71.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="12">
<li id="fn12"><p>Other names associated with record linkage are entity disambiguation, entity resolution, co-reference resolution, matching, and data fusion, meaning that records which are linked or co-referent can be thought of as corresponding to the same underlying entity. The number of names is reflective of a vast literature in social science, statistics, computer science, and information sciences.<a href="chap-link.html#fnref12">↩</a></p></li>
<li id="fn13"><p>If you have examples from your own research using the methods we describe in this chapter, please submit a link to the paper (and/or code) here: <a href="https://textbook.coleridgeinitiative.org/submitexamples" class="uri">https://textbook.coleridgeinitiative.org/submitexamples</a><a href="chap-link.html#fnref13">↩</a></p></li>
<li id="fn14"><p>“Administrative data” typically refers to data generated by the administration of a government program, as distinct from deliberate survey collection.<a href="chap-link.html#fnref14">↩</a></p></li>
<li id="fn15"><p>This topic is discussed in more detail in Chapter <a href="chap-errors.html#chap:errors">Data Quality and Inference Errors</a>.<a href="chap-link.html#fnref15">↩</a></p></li>
<li id="fn16"><p>This topic (quality of data, preprocessing issues) is discussed in more detail in Chapter <a href="chap-intro.html#chap:intro">Introduction</a>.<a href="chap-link.html#fnref16">↩</a></p></li>
<li id="fn17"><p>This topic is discussed in more detail in Chapter <a href="chap-errors.html#chap:errors">Data Quality and Inference Errors</a>.<a href="chap-link.html#fnref17">↩</a></p></li>
<li id="fn18"><p>This topic is discussed in more detail in Chapter <a href="chap-ml.html#chap:ml">Machine Learning</a>.<a href="chap-link.html#fnref18">↩</a></p></li>
<li id="fn19"><p>See Chapter <a href="chap-privacy.html#chap:privacy">Privacy and Confidentiality</a>.<a href="chap-link.html#fnref19">↩</a></p></li>
<li id="fn20"><p>This topic is discussed in more detail in Chapter <a href="chap-privacy.html#chap:privacy">Privacy and Confidentiality</a>.<a href="chap-link.html#fnref20">↩</a></p></li>
<li id="fn21"><p>See <a href="https://workbooks.coleridgeinitiative.org" class="uri">https://workbooks.coleridgeinitiative.org</a>.<a href="chap-link.html#fnref21">↩</a></p></li>
</ol>
</div>
<div id="disqus_thread"></div>
<script>
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = '//big-data-and-social-science.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the
<a href="https://disqus.com/?ref_noscript">
  comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<a href="chap-web.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chap-db.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/Coleridge-Initiative/big-data-and-social-science/edit/master/03-ChapterLinkage.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["big-data-and-social-science.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
