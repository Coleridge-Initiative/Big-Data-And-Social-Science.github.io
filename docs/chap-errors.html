<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Big Data and Social Science</title>
  <meta name="description" content="Big Data and Social Science">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Big Data and Social Science" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="Coleridge-Initiative/big-data-and-social-science" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Big Data and Social Science" />
  
  
  

<meta name="author" content="Ian Foster">
<meta name="author" content="Rayid Ghani">
<meta name="author" content="Ron S. Jarmin">
<meta name="author" content="Frauke Kreuter">
<meta name="author" content="Julia Lane">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="chap-viz.html">
<link rel="next" href="chap-privacy.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Big Data and Social Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="chap-intro.html"><a href="chap-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-1"><i class="fa fa-check"></i><b>1.1</b> Why this book?</a></li>
<li class="chapter" data-level="1.2" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-2"><i class="fa fa-check"></i><b>1.2</b> Defining big data and its value</a></li>
<li class="chapter" data-level="1.3" data-path="chap-intro.html"><a href="chap-intro.html#sec:1.3"><i class="fa fa-check"></i><b>1.3</b> Social science, inference, and big data</a></li>
<li class="chapter" data-level="1.4" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-5"><i class="fa fa-check"></i><b>1.4</b> Social science, data quality, and big data</a></li>
<li class="chapter" data-level="1.5" data-path="chap-intro.html"><a href="chap-intro.html#new-tools-for-new-data"><i class="fa fa-check"></i><b>1.5</b> New tools for new data</a></li>
<li class="chapter" data-level="1.6" data-path="chap-intro.html"><a href="chap-intro.html#sec:1-6"><i class="fa fa-check"></i><b>1.6</b> The book’s “use case”</a></li>
<li class="chapter" data-level="1.7" data-path="chap-intro.html"><a href="chap-intro.html#the-structure-of-the-book"><i class="fa fa-check"></i><b>1.7</b> The structure of the book</a><ul>
<li class="chapter" data-level="1.7.1" data-path="chap-intro.html"><a href="chap-intro.html#part-i-capture-and-curation"><i class="fa fa-check"></i><b>1.7.1</b> Part I: Capture and curation</a></li>
<li class="chapter" data-level="1.7.2" data-path="chap-intro.html"><a href="chap-intro.html#part-ii-modeling-and-analysis"><i class="fa fa-check"></i><b>1.7.2</b> Part II: Modeling and analysis</a></li>
<li class="chapter" data-level="1.7.3" data-path="chap-intro.html"><a href="chap-intro.html#part-iii-inference-and-ethics"><i class="fa fa-check"></i><b>1.7.3</b> Part III: Inference and ethics</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="chap-intro.html"><a href="chap-intro.html#sec:intro:resources"><i class="fa fa-check"></i><b>1.8</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chap-web.html"><a href="chap-web.html"><i class="fa fa-check"></i><b>2</b> Working with Web Data and APIs</a><ul>
<li class="chapter" data-level="2.1" data-path="chap-web.html"><a href="chap-web.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="chap-web.html"><a href="chap-web.html#sec:4-1"><i class="fa fa-check"></i><b>2.2</b> Scraping information from the web</a><ul>
<li class="chapter" data-level="2.2.1" data-path="chap-web.html"><a href="chap-web.html#sec:4-1.1"><i class="fa fa-check"></i><b>2.2.1</b> Obtaining data from websites</a></li>
<li class="chapter" data-level="2.2.2" data-path="chap-web.html"><a href="chap-web.html#sec:4-1.2"><i class="fa fa-check"></i><b>2.2.2</b> Limits of scraping</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="chap-web.html"><a href="chap-web.html#sec:4-3"><i class="fa fa-check"></i><b>2.3</b> Application Programming Interfaces (APIs)</a><ul>
<li class="chapter" data-level="2.3.1" data-path="chap-web.html"><a href="chap-web.html#sec:4-3.1"><i class="fa fa-check"></i><b>2.3.1</b> Relevant APIs and resources</a></li>
<li class="chapter" data-level="2.3.2" data-path="chap-web.html"><a href="chap-web.html#sec:4-3.2"><i class="fa fa-check"></i><b>2.3.2</b> RESTful APIs, returned data, and Python wrappers</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="chap-web.html"><a href="chap-web.html#sec:4-4"><i class="fa fa-check"></i><b>2.4</b> Using an API</a></li>
<li class="chapter" data-level="2.5" data-path="chap-web.html"><a href="chap-web.html#sec:4-4.1"><i class="fa fa-check"></i><b>2.5</b> Another example: Using the ORCID API via a wrapper</a></li>
<li class="chapter" data-level="2.6" data-path="chap-web.html"><a href="chap-web.html#sec:4-6"><i class="fa fa-check"></i><b>2.6</b> Integrating data from multiple sources</a></li>
<li class="chapter" data-level="2.7" data-path="chap-web.html"><a href="chap-web.html#sec:4-9"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="chap-web.html"><a href="chap-web.html#acknowledgements-and-copyright"><i class="fa fa-check"></i><b>2.8</b> Acknowledgements and copyright</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chap-link.html"><a href="chap-link.html"><i class="fa fa-check"></i><b>3</b> Record Linkage</a><ul>
<li class="chapter" data-level="3.1" data-path="chap-link.html"><a href="chap-link.html#motivation"><i class="fa fa-check"></i><b>3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2" data-path="chap-link.html"><a href="chap-link.html#sec:recordlinkage"><i class="fa fa-check"></i><b>3.2</b> Introduction to record linkage</a></li>
<li class="chapter" data-level="3.3" data-path="chap-link.html"><a href="chap-link.html#preprocessing-data-for-record-linkage"><i class="fa fa-check"></i><b>3.3</b> Preprocessing data for record linkage</a></li>
<li class="chapter" data-level="3.4" data-path="chap-link.html"><a href="chap-link.html#S:indexing"><i class="fa fa-check"></i><b>3.4</b> Indexing and blocking</a></li>
<li class="chapter" data-level="3.5" data-path="chap-link.html"><a href="chap-link.html#matching"><i class="fa fa-check"></i><b>3.5</b> Matching</a><ul>
<li class="chapter" data-level="3.5.1" data-path="chap-link.html"><a href="chap-link.html#rule-based-approaches"><i class="fa fa-check"></i><b>3.5.1</b> Rule-based approaches</a></li>
<li class="chapter" data-level="3.5.2" data-path="chap-link.html"><a href="chap-link.html#probabilistic-record-linkage"><i class="fa fa-check"></i><b>3.5.2</b> Probabilistic record linkage</a></li>
<li class="chapter" data-level="3.5.3" data-path="chap-link.html"><a href="chap-link.html#machine-learning-approaches-to-linking"><i class="fa fa-check"></i><b>3.5.3</b> Machine learning approaches to linking</a></li>
<li class="chapter" data-level="3.5.4" data-path="chap-link.html"><a href="chap-link.html#disambiguating-networks"><i class="fa fa-check"></i><b>3.5.4</b> Disambiguating networks</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="chap-link.html"><a href="chap-link.html#classification"><i class="fa fa-check"></i><b>3.6</b> Classification</a><ul>
<li class="chapter" data-level="3.6.1" data-path="chap-link.html"><a href="chap-link.html#S:thresholds"><i class="fa fa-check"></i><b>3.6.1</b> Thresholds</a></li>
<li class="chapter" data-level="3.6.2" data-path="chap-link.html"><a href="chap-link.html#one-to-one-links"><i class="fa fa-check"></i><b>3.6.2</b> One-to-one links</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="chap-link.html"><a href="chap-link.html#record-linkage-and-data-protection"><i class="fa fa-check"></i><b>3.7</b> Record linkage and data protection</a></li>
<li class="chapter" data-level="3.8" data-path="chap-link.html"><a href="chap-link.html#summary"><i class="fa fa-check"></i><b>3.8</b> Summary</a></li>
<li class="chapter" data-level="3.9" data-path="chap-link.html"><a href="chap-link.html#resources"><i class="fa fa-check"></i><b>3.9</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chap-db.html"><a href="chap-db.html"><i class="fa fa-check"></i><b>4</b> Databases</a><ul>
<li class="chapter" data-level="4.1" data-path="chap-db.html"><a href="chap-db.html#sec:db:intro"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="chap-db.html"><a href="chap-db.html#sec:db:when"><i class="fa fa-check"></i><b>4.2</b> DBMS: When and why</a></li>
<li class="chapter" data-level="4.3" data-path="chap-db.html"><a href="chap-db.html#relational-dbmss"><i class="fa fa-check"></i><b>4.3</b> Relational DBMSs</a><ul>
<li class="chapter" data-level="4.3.1" data-path="chap-db.html"><a href="chap-db.html#structured-query-language-sql"><i class="fa fa-check"></i><b>4.3.1</b> Structured Query Language (SQL)</a></li>
<li class="chapter" data-level="4.3.2" data-path="chap-db.html"><a href="chap-db.html#sec:db:sql"><i class="fa fa-check"></i><b>4.3.2</b> Manipulating and querying data</a></li>
<li class="chapter" data-level="4.3.3" data-path="chap-db.html"><a href="chap-db.html#sec:db:schema"><i class="fa fa-check"></i><b>4.3.3</b> Schema design and definition</a></li>
<li class="chapter" data-level="4.3.4" data-path="chap-db.html"><a href="chap-db.html#loading-data"><i class="fa fa-check"></i><b>4.3.4</b> Loading data</a></li>
<li class="chapter" data-level="4.3.5" data-path="chap-db.html"><a href="chap-db.html#transactions-and-crash-recovery"><i class="fa fa-check"></i><b>4.3.5</b> Transactions and crash recovery</a></li>
<li class="chapter" data-level="4.3.6" data-path="chap-db.html"><a href="chap-db.html#sec:db:index"><i class="fa fa-check"></i><b>4.3.6</b> Database optimizations</a></li>
<li class="chapter" data-level="4.3.7" data-path="chap-db.html"><a href="chap-db.html#caveats-and-challenges"><i class="fa fa-check"></i><b>4.3.7</b> Caveats and challenges</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="chap-db.html"><a href="chap-db.html#linking-dbmss-and-other-tools"><i class="fa fa-check"></i><b>4.4</b> Linking DBMSs and other tools</a></li>
<li class="chapter" data-level="4.5" data-path="chap-db.html"><a href="chap-db.html#sec:db:nosql"><i class="fa fa-check"></i><b>4.5</b> NoSQL databases</a><ul>
<li class="chapter" data-level="4.5.1" data-path="chap-db.html"><a href="chap-db.html#challenges-of-scale-the-cap-theorem"><i class="fa fa-check"></i><b>4.5.1</b> Challenges of scale: The CAP theorem</a></li>
<li class="chapter" data-level="4.5.2" data-path="chap-db.html"><a href="chap-db.html#nosql-and-keyvalue-stores"><i class="fa fa-check"></i><b>4.5.2</b> NoSQL and key–value stores</a></li>
<li class="chapter" data-level="4.5.3" data-path="chap-db.html"><a href="chap-db.html#other-nosql-databases"><i class="fa fa-check"></i><b>4.5.3</b> Other NoSQL databases</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="chap-db.html"><a href="chap-db.html#sec:db:spatial"><i class="fa fa-check"></i><b>4.6</b> Spatial databases</a></li>
<li class="chapter" data-level="4.7" data-path="chap-db.html"><a href="chap-db.html#which-database-to-use"><i class="fa fa-check"></i><b>4.7</b> Which database to use?</a><ul>
<li class="chapter" data-level="4.7.1" data-path="chap-db.html"><a href="chap-db.html#relational-dbmss-1"><i class="fa fa-check"></i><b>4.7.1</b> Relational DBMSs</a></li>
<li class="chapter" data-level="4.7.2" data-path="chap-db.html"><a href="chap-db.html#nosql-dbmss"><i class="fa fa-check"></i><b>4.7.2</b> NoSQL DBMSs</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="chap-db.html"><a href="chap-db.html#summary-1"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
<li class="chapter" data-level="4.9" data-path="chap-db.html"><a href="chap-db.html#resources-1"><i class="fa fa-check"></i><b>4.9</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chap-parallel.html"><a href="chap-parallel.html"><i class="fa fa-check"></i><b>5</b> Scaling up through Parallel and Distributed Computing</a><ul>
<li class="chapter" data-level="5.1" data-path="chap-parallel.html"><a href="chap-parallel.html#introduction-1"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="chap-parallel.html"><a href="chap-parallel.html#sec:intro"><i class="fa fa-check"></i><b>5.2</b> MapReduce</a></li>
<li class="chapter" data-level="5.3" data-path="chap-parallel.html"><a href="chap-parallel.html#apache-hadoop-mapreduce"><i class="fa fa-check"></i><b>5.3</b> Apache Hadoop MapReduce</a><ul>
<li class="chapter" data-level="5.3.1" data-path="chap-parallel.html"><a href="chap-parallel.html#the-hadoop-distributed-file-system"><i class="fa fa-check"></i><b>5.3.1</b> The Hadoop Distributed File System</a></li>
<li class="chapter" data-level="5.3.2" data-path="chap-parallel.html"><a href="chap-parallel.html#hadoop-setup-bringing-compute-to-the-data"><i class="fa fa-check"></i><b>5.3.2</b> Hadoop Setup: Bringing compute to the data</a></li>
<li class="chapter" data-level="5.3.3" data-path="chap-parallel.html"><a href="chap-parallel.html#hardware-provisioning"><i class="fa fa-check"></i><b>5.3.3</b> Hardware provisioning</a></li>
<li class="chapter" data-level="5.3.4" data-path="chap-parallel.html"><a href="chap-parallel.html#programming-in-hadoop"><i class="fa fa-check"></i><b>5.3.4</b> Programming in Hadoop</a></li>
<li class="chapter" data-level="5.3.5" data-path="chap-parallel.html"><a href="chap-parallel.html#programming-language-support"><i class="fa fa-check"></i><b>5.3.5</b> Programming language support</a></li>
<li class="chapter" data-level="5.3.6" data-path="chap-parallel.html"><a href="chap-parallel.html#benefits-and-limitations-of-hadoop"><i class="fa fa-check"></i><b>5.3.6</b> Benefits and Limitations of Hadoop</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="chap-parallel.html"><a href="chap-parallel.html#other-mapreduce-implementations"><i class="fa fa-check"></i><b>5.4</b> Other MapReduce Implementations</a></li>
<li class="chapter" data-level="5.5" data-path="chap-parallel.html"><a href="chap-parallel.html#apache-spark"><i class="fa fa-check"></i><b>5.5</b> Apache Spark</a></li>
<li class="chapter" data-level="5.6" data-path="chap-parallel.html"><a href="chap-parallel.html#summary-2"><i class="fa fa-check"></i><b>5.6</b> Summary</a></li>
<li class="chapter" data-level="5.7" data-path="chap-parallel.html"><a href="chap-parallel.html#resources-2"><i class="fa fa-check"></i><b>5.7</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chap-ml.html"><a href="chap-ml.html"><i class="fa fa-check"></i><b>6</b> Machine Learning</a><ul>
<li class="chapter" data-level="6.1" data-path="chap-ml.html"><a href="chap-ml.html#introduction-2"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="chap-ml.html"><a href="chap-ml.html#what-is-machine-learning"><i class="fa fa-check"></i><b>6.2</b> What is machine learning?</a></li>
<li class="chapter" data-level="6.3" data-path="chap-ml.html"><a href="chap-ml.html#the-machine-learning-process"><i class="fa fa-check"></i><b>6.3</b> The machine learning process</a></li>
<li class="chapter" data-level="6.4" data-path="chap-ml.html"><a href="chap-ml.html#problem-formulation-mapping-a-problem-to-machine-learning-methods"><i class="fa fa-check"></i><b>6.4</b> Problem formulation: Mapping a problem to machine learning methods</a></li>
<li class="chapter" data-level="6.5" data-path="chap-ml.html"><a href="chap-ml.html#methods"><i class="fa fa-check"></i><b>6.5</b> Methods</a><ul>
<li class="chapter" data-level="6.5.1" data-path="chap-ml.html"><a href="chap-ml.html#unsupervised-learning-methods"><i class="fa fa-check"></i><b>6.5.1</b> Unsupervised learning methods</a></li>
<li class="chapter" data-level="6.5.2" data-path="chap-ml.html"><a href="chap-ml.html#sec:MLchapter:super"><i class="fa fa-check"></i><b>6.5.2</b> Supervised learning</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="chap-ml.html"><a href="chap-ml.html#evaluation"><i class="fa fa-check"></i><b>6.6</b> Evaluation</a><ul>
<li class="chapter" data-level="6.6.1" data-path="chap-ml.html"><a href="chap-ml.html#methodology"><i class="fa fa-check"></i><b>6.6.1</b> Methodology</a></li>
<li class="chapter" data-level="6.6.2" data-path="chap-ml.html"><a href="chap-ml.html#metrics"><i class="fa fa-check"></i><b>6.6.2</b> Metrics</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="chap-ml.html"><a href="chap-ml.html#practical-tips"><i class="fa fa-check"></i><b>6.7</b> Practical tips</a><ul>
<li class="chapter" data-level="6.7.1" data-path="chap-ml.html"><a href="chap-ml.html#features"><i class="fa fa-check"></i><b>6.7.1</b> Features</a></li>
<li class="chapter" data-level="6.7.2" data-path="chap-ml.html"><a href="chap-ml.html#machine-learning-pipeline"><i class="fa fa-check"></i><b>6.7.2</b> Machine learning pipeline</a></li>
<li class="chapter" data-level="6.7.3" data-path="chap-ml.html"><a href="chap-ml.html#multiclass-problems"><i class="fa fa-check"></i><b>6.7.3</b> Multiclass problems</a></li>
<li class="chapter" data-level="6.7.4" data-path="chap-ml.html"><a href="chap-ml.html#skewed-or-imbalanced-classification-problems"><i class="fa fa-check"></i><b>6.7.4</b> Skewed or imbalanced classification problems</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="chap-ml.html"><a href="chap-ml.html#how-can-social-scientists-benefit-from-machine-learning"><i class="fa fa-check"></i><b>6.8</b> How can social scientists benefit from machine learning?</a></li>
<li class="chapter" data-level="6.9" data-path="chap-ml.html"><a href="chap-ml.html#advanced-topics"><i class="fa fa-check"></i><b>6.9</b> Advanced topics</a></li>
<li class="chapter" data-level="6.10" data-path="chap-ml.html"><a href="chap-ml.html#summary-3"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
<li class="chapter" data-level="6.11" data-path="chap-ml.html"><a href="chap-ml.html#ml:res"><i class="fa fa-check"></i><b>6.11</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chap-text.html"><a href="chap-text.html"><i class="fa fa-check"></i><b>7</b> Text Analysis</a><ul>
<li class="chapter" data-level="7.1" data-path="chap-text.html"><a href="chap-text.html#understanding-what-people-write"><i class="fa fa-check"></i><b>7.1</b> Understanding what people write</a></li>
<li class="chapter" data-level="7.2" data-path="chap-text.html"><a href="chap-text.html#how-to-analyze-text"><i class="fa fa-check"></i><b>7.2</b> How to analyze text</a><ul>
<li class="chapter" data-level="7.2.1" data-path="chap-text.html"><a href="chap-text.html#processing-text-data"><i class="fa fa-check"></i><b>7.2.1</b> Processing text data</a></li>
<li class="chapter" data-level="7.2.2" data-path="chap-text.html"><a href="chap-text.html#how-much-is-a-word-worth"><i class="fa fa-check"></i><b>7.2.2</b> How much is a word worth?</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="chap-text.html"><a href="chap-text.html#sec:appapp"><i class="fa fa-check"></i><b>7.3</b> Approaches and applications</a><ul>
<li class="chapter" data-level="7.3.1" data-path="chap-text.html"><a href="chap-text.html#sec:lda"><i class="fa fa-check"></i><b>7.3.1</b> Topic modeling</a></li>
<li class="chapter" data-level="7.3.2" data-path="chap-text.html"><a href="chap-text.html#sec:ir"><i class="fa fa-check"></i><b>7.3.2</b> Information retrieval and clustering</a></li>
<li class="chapter" data-level="7.3.3" data-path="chap-text.html"><a href="chap-text.html#sec:other"><i class="fa fa-check"></i><b>7.3.3</b> Other approaches</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="chap-text.html"><a href="chap-text.html#sec:eval"><i class="fa fa-check"></i><b>7.4</b> Evaluation</a></li>
<li class="chapter" data-level="7.5" data-path="chap-text.html"><a href="chap-text.html#text-analysis-tools"><i class="fa fa-check"></i><b>7.5</b> Text analysis tools</a></li>
<li class="chapter" data-level="7.6" data-path="chap-text.html"><a href="chap-text.html#summary-4"><i class="fa fa-check"></i><b>7.6</b> Summary</a></li>
<li class="chapter" data-level="7.7" data-path="chap-text.html"><a href="chap-text.html#resources-3"><i class="fa fa-check"></i><b>7.7</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chap-networks.html"><a href="chap-networks.html"><i class="fa fa-check"></i><b>8</b> Networks: The Basics</a><ul>
<li class="chapter" data-level="8.1" data-path="chap-networks.html"><a href="chap-networks.html#introduction-3"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="chap-networks.html"><a href="chap-networks.html#network-data"><i class="fa fa-check"></i><b>8.2</b> Network data</a><ul>
<li class="chapter" data-level="8.2.1" data-path="chap-networks.html"><a href="chap-networks.html#forms-of-network-data"><i class="fa fa-check"></i><b>8.2.1</b> Forms of network data</a></li>
<li class="chapter" data-level="8.2.2" data-path="chap-networks.html"><a href="chap-networks.html#inducing-one-mode-networks-from-two-mode-data"><i class="fa fa-check"></i><b>8.2.2</b> Inducing one-mode networks from two-mode data</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="chap-networks.html"><a href="chap-networks.html#network-measures"><i class="fa fa-check"></i><b>8.3</b> Network measures</a><ul>
<li class="chapter" data-level="8.3.1" data-path="chap-networks.html"><a href="chap-networks.html#reachability"><i class="fa fa-check"></i><b>8.3.1</b> Reachability</a></li>
<li class="chapter" data-level="8.3.2" data-path="chap-networks.html"><a href="chap-networks.html#whole-network-measures"><i class="fa fa-check"></i><b>8.3.2</b> Whole-network measures</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="chap-networks.html"><a href="chap-networks.html#comparing-collaboration-networks"><i class="fa fa-check"></i><b>8.4</b> Comparing collaboration networks</a></li>
<li class="chapter" data-level="8.5" data-path="chap-networks.html"><a href="chap-networks.html#summary-5"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="chap-networks.html"><a href="chap-networks.html#resources-4"><i class="fa fa-check"></i><b>8.6</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chap-viz.html"><a href="chap-viz.html"><i class="fa fa-check"></i><b>9</b> Information Visualization</a><ul>
<li class="chapter" data-level="9.1" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-1"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2"><i class="fa fa-check"></i><b>9.2</b> Developing effective visualizations</a></li>
<li class="chapter" data-level="9.3" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-3"><i class="fa fa-check"></i><b>9.3</b> A data-by-tasks taxonomy</a><ul>
<li class="chapter" data-level="9.3.1" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.1"><i class="fa fa-check"></i><b>9.3.1</b> Multivariate data</a></li>
<li class="chapter" data-level="9.3.2" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.2"><i class="fa fa-check"></i><b>9.3.2</b> Spatial data</a></li>
<li class="chapter" data-level="9.3.3" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.4"><i class="fa fa-check"></i><b>9.3.3</b> Temporal data</a></li>
<li class="chapter" data-level="9.3.4" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.5"><i class="fa fa-check"></i><b>9.3.4</b> Hierarchical data</a></li>
<li class="chapter" data-level="9.3.5" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.6"><i class="fa fa-check"></i><b>9.3.5</b> Network data</a></li>
<li class="chapter" data-level="9.3.6" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-2.7"><i class="fa fa-check"></i><b>9.3.6</b> Text data</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4"><i class="fa fa-check"></i><b>9.4</b> Challenges</a><ul>
<li class="chapter" data-level="9.4.1" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4.1"><i class="fa fa-check"></i><b>9.4.1</b> Scalability</a></li>
<li class="chapter" data-level="9.4.2" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4.2"><i class="fa fa-check"></i><b>9.4.2</b> Evaluation</a></li>
<li class="chapter" data-level="9.4.3" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4.3"><i class="fa fa-check"></i><b>9.4.3</b> Visual impairment</a></li>
<li class="chapter" data-level="9.4.4" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-4.4"><i class="fa fa-check"></i><b>9.4.4</b> Visual literacy</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="chap-viz.html"><a href="chap-viz.html#sec:viz-5"><i class="fa fa-check"></i><b>9.5</b> Summary</a></li>
<li class="chapter" data-level="9.6" data-path="chap-viz.html"><a href="chap-viz.html#sec:mylabel4"><i class="fa fa-check"></i><b>9.6</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chap-errors.html"><a href="chap-errors.html"><i class="fa fa-check"></i><b>10</b> Errors and Inference</a><ul>
<li class="chapter" data-level="10.1" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-1"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-2"><i class="fa fa-check"></i><b>10.2</b> The total error paradigm</a><ul>
<li class="chapter" data-level="10.2.1" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-2.1"><i class="fa fa-check"></i><b>10.2.1</b> The traditional model</a></li>
<li class="chapter" data-level="10.2.2" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-2.2"><i class="fa fa-check"></i><b>10.2.2</b> Extending the framework to big data</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-3"><i class="fa fa-check"></i><b>10.3</b> Illustrations of errors in big data</a></li>
<li class="chapter" data-level="10.4" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-4"><i class="fa fa-check"></i><b>10.4</b> Errors in big data analytics</a><ul>
<li class="chapter" data-level="10.4.1" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-4.1"><i class="fa fa-check"></i><b>10.4.1</b> Errors resulting from volume, velocity, and variety, assuming perfect veracity</a></li>
<li class="chapter" data-level="10.4.2" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-4.2"><i class="fa fa-check"></i><b>10.4.2</b> Errors resulting from lack of veracity</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-5"><i class="fa fa-check"></i><b>10.5</b> Some methods for mitigating, detecting, and compensating for errors</a></li>
<li class="chapter" data-level="10.6" data-path="chap-errors.html"><a href="chap-errors.html#sec:10-6"><i class="fa fa-check"></i><b>10.6</b> Summary</a></li>
<li class="chapter" data-level="10.7" data-path="chap-errors.html"><a href="chap-errors.html#resources-5"><i class="fa fa-check"></i><b>10.7</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chap-privacy.html"><a href="chap-privacy.html"><i class="fa fa-check"></i><b>11</b> Privacy and Confidentiality</a><ul>
<li class="chapter" data-level="11.1" data-path="chap-privacy.html"><a href="chap-privacy.html#introduction-4"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="chap-privacy.html"><a href="chap-privacy.html#why-is-access-important"><i class="fa fa-check"></i><b>11.2</b> Why is access important?</a></li>
<li class="chapter" data-level="11.3" data-path="chap-privacy.html"><a href="chap-privacy.html#providing-access"><i class="fa fa-check"></i><b>11.3</b> Providing access</a></li>
<li class="chapter" data-level="11.4" data-path="chap-privacy.html"><a href="chap-privacy.html#the-new-challenges"><i class="fa fa-check"></i><b>11.4</b> The new challenges</a></li>
<li class="chapter" data-level="11.5" data-path="chap-privacy.html"><a href="chap-privacy.html#legal-and-ethical-framework"><i class="fa fa-check"></i><b>11.5</b> Legal and ethical framework</a></li>
<li class="chapter" data-level="11.6" data-path="chap-privacy.html"><a href="chap-privacy.html#summary-6"><i class="fa fa-check"></i><b>11.6</b> Summary</a></li>
<li class="chapter" data-level="11.7" data-path="chap-privacy.html"><a href="chap-privacy.html#resources-6"><i class="fa fa-check"></i><b>11.7</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chap-workbooks.html"><a href="chap-workbooks.html"><i class="fa fa-check"></i><b>12</b> Workbooks</a><ul>
<li class="chapter" data-level="12.1" data-path="chap-workbooks.html"><a href="chap-workbooks.html#introduction-5"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="chap-workbooks.html"><a href="chap-workbooks.html#environment"><i class="fa fa-check"></i><b>12.2</b> Environment</a><ul>
<li class="chapter" data-level="12.2.1" data-path="chap-workbooks.html"><a href="chap-workbooks.html#running-workbooks-locally"><i class="fa fa-check"></i><b>12.2.1</b> Running workbooks locally</a></li>
<li class="chapter" data-level="12.2.2" data-path="chap-workbooks.html"><a href="chap-workbooks.html#central-workbook-server"><i class="fa fa-check"></i><b>12.2.2</b> Central workbook server</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="chap-workbooks.html"><a href="chap-workbooks.html#workbook-details"><i class="fa fa-check"></i><b>12.3</b> Workbook details</a><ul>
<li class="chapter" data-level="12.3.1" data-path="chap-workbooks.html"><a href="chap-workbooks.html#social-media-and-apis"><i class="fa fa-check"></i><b>12.3.1</b> Social Media and APIs</a></li>
<li class="chapter" data-level="12.3.2" data-path="chap-workbooks.html"><a href="chap-workbooks.html#database-basics"><i class="fa fa-check"></i><b>12.3.2</b> Database basics</a></li>
<li class="chapter" data-level="12.3.3" data-path="chap-workbooks.html"><a href="chap-workbooks.html#data-linkage"><i class="fa fa-check"></i><b>12.3.3</b> Data Linkage</a></li>
<li class="chapter" data-level="12.3.4" data-path="chap-workbooks.html"><a href="chap-workbooks.html#machine-learning"><i class="fa fa-check"></i><b>12.3.4</b> Machine Learning</a></li>
<li class="chapter" data-level="12.3.5" data-path="chap-workbooks.html"><a href="chap-workbooks.html#text-analysis"><i class="fa fa-check"></i><b>12.3.5</b> Text Analysis</a></li>
<li class="chapter" data-level="12.3.6" data-path="chap-workbooks.html"><a href="chap-workbooks.html#networks"><i class="fa fa-check"></i><b>12.3.6</b> Networks</a></li>
<li class="chapter" data-level="12.3.7" data-path="chap-workbooks.html"><a href="chap-workbooks.html#visualization"><i class="fa fa-check"></i><b>12.3.7</b> Visualization</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="chap-workbooks.html"><a href="chap-workbooks.html#resources-7"><i class="fa fa-check"></i><b>12.4</b> Resources</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Big Data and Social Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chap:errors" class="section level1">
<h1><span class="header-section-number">Chapter 10</span> Errors and Inference</h1>
<p><strong>Paul P. Biemer</strong></p>
<p>This chapter deals with inference and the errors associated with big
data. Social scientists know only too well the cost associated with bad
data—we highlighted both the classic <em>Literary Digest</em> example and the
more recent Google Flu Trends problems in
Chapter <a href="chap-intro.html#chap:intro">Introduction</a>. Although the consequences are well understood,
the new types of data are so large and complex that their properties
often cannot be studied in traditional ways. In addition, the data
generating function is such that the data are often selective,
incomplete, and erroneous. Without proper data hygiene, the errors can
quickly compound. This chapter provides, for the first time, a
systematic way to think about the error framework in a big data setting.</p>
<div id="sec:10-1" class="section level2">
<h2><span class="header-section-number">10.1</span> Introduction</h2>
<p>The massive amounts of high-dimensional and unstructured data in big
data bring both new opportunities and new challenges to the data
analyst. Many of the problems with big data are well known (see, for
example, the AAPOR report by Japec et al. <span class="citation">(Japec et al. <a href="#ref-japec2015big">2015</a>)</span>). The volume,
variety, and velocity of big data can overwhelm traditional methods of
data analysis. Add to this complexity the fact that, as it is generated,
big data is often selective, incomplete, and erroneous. As it is
processed, new errors can be introduced in downstream operations.</p>
<p>Big data is typically aggregated from disparate sources at various
points in time and integrated to form data sets. These processes involve
linking records together, transforming them to form new attributes (or
variables), documenting the actions taken (although sometimes
inadequately), and interpreting the newly created features of the data.
These activities may introduce new errors into the data set: errors that
may be either <em>variable</em> (i.e., errors that create random noise
resulting in poor reliability) or <em>systematic</em> (i.e., errors that tend
to be directional, thus exacerbating biases). Using big data in
statistically valid ways is increasingly challenging in this
environment; however, it is important for data analysts to be aware of
the error risks and the potential effects of big data error on
inferences and decision-making. The massiveness, high dimensionality,
and accelerating pace of big data, combined with the risks of variable
and systematic data errors, requires new, robust approaches to data
analysis.</p>
<p>The core issue confronting big data veracity is that such data may not
be generated from instruments and methods designed to produce valid and
reliable data for scientific analysis and discovery. Rather, this is
data that are being repurposed for uses not originally intended. Big
data has been referred to as “found” data or “data exhaust” because it
is generated for purposes that often do not align with those of the data
analyst. In addition to inadvertent errors, there are also errors from
mischief in big data; for example, automated systems have been written
to generate bogus content in the social media that is indistinguishable
from legitimate or authentic data. Regardless of the source, many big
data generators have little or no regard for the quality of the data
that are cast off from their processes. Big data analysts must be keenly
aware of these limitations and should take the necessary steps to
understand and hopefully mitigate the effects of hidden errors on their
results.</p>
</div>
<div id="sec:10-2" class="section level2">
<h2><span class="header-section-number">10.2</span> The total error paradigm</h2>
<p>We now provide a framework for describing, mitigating, and interpreting
the errors in essentially any data set, be it structured or
unstructured, massive or small, static or dynamic. This framework has
been referred to as the total error framework or paradigm. We begin by
reviewing the traditional paradigm, acknowledging its limitations for
truly big data sets, and we suggest how this framework can be extended
to encompass the new error structures often associated with big data.</p>
<div id="sec:10-2.1" class="section level3">
<h3><span class="header-section-number">10.2.1</span> The traditional model</h3>
<p>Dealing with the risks that errors introduce in big data analysis can be
facilitated through a better understanding of the sources and nature of
those errors. Such knowledge is gained through in-depth understanding of
the data generating mechanism, the data processing/transformation
infrastructure, and the approaches used to create a specific data set or
the estimates derived from it. For survey data, this knowledge is
embodied in the well-known <em>total survey error</em> (TSE) framework that
identifies all the major sources of error contributing to data validity
and estimator accuracy
<span class="citation">(Groves <a href="#ref-groves2004survey">2004</a>; Biemer and Lyberg <a href="#ref-biemer2003">2003</a>; Biemer <a href="#ref-biemer2010total">2010</a>)</span>. The TSE framework
attempts to describe the nature of the error sources and what they may
suggest about how the errors could affect inference. The framework
parses the total error into bias and variance components that, in turn,
may be further subdivided into subcomponents that map the specific types
of errors to unique components of the total mean squared error. It
should be noted that, while our discussion on issues regarding inference
has quantitative analyses in mind, some of the issues discussed here are
also of interest to more qualitative uses of big data.</p>
<p>For surveys, the TSE framework provides useful insights regarding how
the many steps in the data generating, reformatting, and file
preparation processes affect estimation and inference, and may also
suggest methods for either reducing the errors at their source or
adjusting for their effects in the final data products to produce
inferences of higher quality. The AAPOR Task Force Report on big data
referenced above <span class="citation">(Japec et al. <a href="#ref-japec2015big">2015</a>)</span> describes the concept of data quality
for big data and provides a total error framework for big data that we
consider and further extend in this chapter. This approach was closely
modeled after the TSE framework since, as we shall see, both share a
number of common error sources. However, the big data total error (BDTE)
framework, as it is called in the AAPOR report, necessarily includes
additional error sources that are unique to big data and can create
substantial biases and uncertainties in big data products. Like the TSE
framework, the BDTE framework aids our understanding of the limitations
of the data, leading to better-informed analyses and applications of the
results. It may also inform a research agenda for reducing the effects
of error on big data analytics.</p>
<p>In Figure <a href="chap-errors.html#fig:fig10-1">10.1</a>, a
canonical data file is represented as an array consisting of rows
(records) and columns (variables), with their size denoted by <span class="math inline">\(N\)</span> and
<span class="math inline">\(p\)</span>, respectively. Many data sets derived from big data can be
represented in this way, at least conceptually. Later, we will consider
data sets that do not conform to this rectangular structure and may even
be unstructured.</p>
<p>Many administrative data sets have a simple tabular structure, as do
survey sampling frames, population registers, and accounting
spreadsheets. Let us assume that the data set is intended to some target
population to which inference will be made in the subsequent analysis.
Thus, the rows are typically aligned with units or elements of this
target population, the columns represent characteristics, variables (or
features) of the row elements, and the cells correspond to values of the
column features for elements on the rows.</p>
<div class="figure" style="text-align: center"><span id="fig:fig10-1"></span>
<img src="ChapterError/figures/fig10-1.png" alt="A typical rectangular data file format" width="70%" />
<p class="caption">
Figure 10.1: A typical rectangular data file format
</p>
</div>
<p>The total error for this data set may be expressed by the following
heuristic formula:
<span class="math display">\[\text{Total error } =\text{ Row error } + \text{ Column error }
+ \text{ Cell error}.\]</span></p>
<p><strong>Row error</strong></p>
<p>For the situations considered in this chapter, the row errors may be of
three types:</p>
<ul>
<li><p><strong>Omissions</strong>: Some rows are missing, which implies that elements in the target
population are not represented on the file.</p></li>
<li><p><strong>Duplications</strong>: Some population elements occupy more than one row.</p></li>
<li><p><strong>Erroneous inclusions</strong>: Some rows contain elements or entities that are not part of the
target population.</p></li>
</ul>
<p>For survey sample data sets, omissions include members of the target
population that are either inadvertently or deliberately absent from the
frame, as well as nonsampled frame members. For big data, the
selectivity of the capture mechanism is a common form of omissions. For
example, a data set consisting of persons who conducted a Google search
in the past week necessarily excludes persons not satisfying that
criterion who may have quite different characteristics from those who
do. Such exclusions can therefore be viewed as a source of selectivity
bias if inference is to be made to the general population. For one,
persons who do not have access to the Internet are excluded from the
data set. These exclusions may be biasing in that persons with Internet
access may have quite different demographic characteristics from persons
who do not have Internet access. The selectivity of big data capture is
similar to frame noncoverage in survey sampling and can bias inferences
when researchers fail to consider it and compensate for it in their
analyses.</p>
<hr />
<p><strong>Example: Google searches</strong></p>
<p>As an example, in the United States, the word “Jewish” is included in
3.2 times more Google searches than “Mormon” <span class="citation">(Stephens-Davidowitz and Varian <a href="#ref-SDV2015">2015</a>)</span>. This does not
mean that the Jewish population is 3.2 times larger than the Mormon
population. Another possible explanation is that Jewish people use the
Internet in higher proportions or have more questions that require using
the word “Jewish.” Thus Google search data are more useful for relative
comparisons than for estimating absolute levels.</p>
<hr />
<p>A well-known formula in the survey literature provides a useful
expression for the so-called <em>coverage bias</em> in the mean of some
variable, <span class="math inline">\(V\)</span>. Denote the mean by <span class="math inline">\(\bar{V}\)</span>, and let <span class="math inline">\(\bar{V}_T\)</span> denote
the (possibly hypothetical because it may not be observable) mean of the
target population of <span class="math inline">\(N_{T}\)</span> elements, including the <span class="math inline">\(N_{T}-N\)</span> elements
that are missing from the observed data set. Then the bias due to this
<em>noncoverage</em> is <span class="math inline">\(B_{NC} = \bar{V} - \bar{V}_T = (1 - N / N_T )(\bar{V}_C - \bar{V}_{NC})\)</span>, where <span class="math inline">\(\bar{V}_C\)</span> is the mean of
the <em>covered</em> elements (i.e., the elements in the observed data set) and
<span class="math inline">\(\bar{V}_{NC}\)</span> is the mean of the <span class="math inline">\(N_{T}-N\)</span> <em>noncovered</em> elements. Thus
we see that, to the extent that the difference between the covered and
noncovered elements is large or the fraction of missing elements
<span class="math inline">\((1 - N / N_T)\)</span> is large, the bias in the descriptive statistic will also be
large. As in survey research, often we can only speculate about the
sizes of these two components of bias. Nevertheless, speculation is
useful for understanding and interpreting the results of data analysis
and cautioning ourselves regarding the risks of false inference.</p>
<p>We can also expect that big data sets, such as a data set containing
Google searches during the previous week, could have the same person
represented many times. People who conducted many searches during the
data capture period would be disproportionately represented relative to
those who conducted fewer searchers. If the rows of the data set
correspond to tweets in a Twitter feed, duplication can arise when the
same tweet is retweeted or when some persons are quite active in
tweeting while others lurk and tweet much less frequently. Whether such
duplications should be regarded as “errors” depends upon the goals of
the analysis.</p>
<p>For example, if inference is to be made to a population of persons,
persons who tweet multiple times on a topic would be overrepresented. If
inference is to be made to the population of tweets, including retweets,
then such duplication does not bias inference.</p>
<p>When it is a problem, it still may not be possible to identify
duplications in the data. Failing to account for them could generate
duplication biases in the analysis. If these unwanted duplications can
be identified, they can either be removed from the data file (i.e.,
deduplication). Alternatively, if a certain number of rows, say <span class="math inline">\(d\)</span>,
correspond to the same population unit, those row values can be weighted
by <span class="math inline">\(1/d\)</span> to correct the estimates for the duplications.</p>
<p>Erroneous inclusions can also create biases. For example, Google
searches or tweets may not be generated by a person but rather by a
computer either maliciously or as part of an information-gathering or
publicity-generating routine. Likewise, some rows may not satisfy the
criteria for inclusion in an analysis—for example, an analysis by age
or gender includes some row elements not satisfying the criteria. If the
criteria can be applied accurately, the rows violating the criteria can
be excluded prior to analysis. However, with big data, some out-of-scope
elements may still be included as a result of missing or erroneous
information, and these inclusions will bias inference.</p>
<p><strong>Column error</strong></p>
<p>The most common type of column error in survey data analysis is caused
by inaccurate or erroneous labeling of the column data—an example of
metadata error. In the TSE framework, this is referred to as a
<em>specification</em> error. For example, a business register may include a
column labeled “number of employees,” defined as the number of persons
in the company who received a payroll check in the month preceding.
Instead the column contains the number of persons on the payroll whether
or not they received a check in the prior month, thus including, for
example, persons on leave without pay.</p>
<p>For big data analysis, such errors would seem to be quite common because
of the complexities involved in producing a data set. For example, data
generated from a source, such as an individual tweet, may undergo a
number of transformations before it is included in the analysis data
set. This transformative process can be quite complex, involving parsing
phrases, identifying words, and classifying them as to subject matter
and then perhaps further classifying them as either positive or negative
expressions about some phenomenon like the economy or a political
figure. There is considerable risk of the resulting variables being
either inaccurately defined or misinterpreted by the data analyst.</p>
<hr />
<p><strong>Example: Specification error with Twitter data</strong></p>
<p>As an example, consider a Twitter data set where the rows correspond to
tweets and one of the columns supposedly contains an indicator of
whether the tweet contained one of the following key words: marijuana,
pot, cannabis, weed, hemp, ganja, or THC. Instead, the indicator
actually corresponds to whether the tweet contained a shorter list of
words; say, either marijuana or pot. The mislabeled column is an example
of specification error which could be a biasing factor in an analysis.
For example, estimates of marijuana use based upon the indicator could
be underestimates.</p>
<hr />
<p><strong>Cell errors</strong></p>
<p>Finally, cell errors can be of three types: content error, specification
error, or missing data. A content error occurs when the value in a cell
satisfies the column definition but still deviates from the true value,
whether or not the true value is known. For example, the value satisfies
the definition of “number of employees” but is outdated because it does
not agree with the current number of employees. Errors in sensitive data
such as drug use, prior arrests, and sexual misconduct may be
deliberate. Thus, content errors may be the result of the measurement
process, a transcription error, a data processing error (e.g., keying,
coding, editing), an imputation error, or some other cause.</p>
<p>Specification error is just as described for column error but applied to
a cell. For example, the column is correctly defined and labeled;
however, a few companies provided values that, although otherwise highly
accurate, were nevertheless inconsistent with the required definition.
Missing data, as the name implies, are just empty cells that should be
filled. As described in Kreuter and Peng <span class="citation">(Kreuter and Peng <a href="#ref-kreuter201412">2014</a>)</span>, data sets
derived from big data are notoriously affected by all three types of
cell error, particularly missing or incomplete data, perhaps because
that is the most obvious deficiency.</p>
<p>The traditional TSE framework is quite general in that it can be applied
to essentially any data set that conform to the format in Figure
<a href="chap-errors.html#fig:fig10-1">10.1</a>. However, in
most practical situations it is quite limited because it makes no
attempt to describe how the processes that the data may have contributed
to what could be construed as data errors. In some cases, these
processes constitute a “black box,” and the best approach is to attempt
to evaluate the quality of the end product. For survey data, the TSE
framework provides a fairly complete description of the error-generating
processes for survey data and survey frames <span class="citation">(Biemer <a href="#ref-biemer2010total">2010</a>)</span>. In
addition, there has been some effort to describe these processes for
population registers and administrative data <span class="citation">(Wallgren and Wallgren <a href="#ref-wallgren2007register">2007</a>)</span>.
But at this writing, little effort has been devoted to enumerating the
error sources and the error generating processes for big data.</p>
<p>As previously noted, missing data can take two forms: missing
information in a cell of a data matrix (referred to as <em>item
missingness</em>) or missing rows (referred to as <em>unit missingness</em>), with
the former being readily observable whereas the latter can be completely
hidden from the analyst. Much is known from the survey research
literature about how both types of missingness affect data analysis
(see, for example, Little and Rubin
<span class="citation">(Little and Rubin <a href="#ref-little2014statistical">2014</a>; Rubin <a href="#ref-rubin1976">1976</a>)</span>). Rubin <span class="citation">(Rubin <a href="#ref-rubin1976">1976</a>)</span> introduced the
term <em>missing completely at random (MCAR)</em> to describe data where the
data that are available (say, the rows of a data set) can be considered
as a simple random sample of the inferential population (i.e., the
population to which inferences from the data analysis will be made).
Since the data set represents the population, MCAR data provide results
that are generalizable to this population.</p>
<p>A second possibility also exists for the reasons why data are missing.
For example, students who have high absenteeism may be missing because
they were ill on the day of the test. They may otherwise be average
performers on the test so, in this case, it has little to do with how
they would score. Thus, the values are missing for reasons related to
another variable, health, that may be available in the data set and
completely observed. Students with poor health tend to be missing test
scores, regardless of those student’s performance on the test. Rubin
<span class="citation">(Rubin <a href="#ref-rubin1976">1976</a>)</span> uses the term <em>missing at random (MAR)</em> to describe data
that are missing for reasons related to completely observed variables in
the data set. It is possible to compensate for this type of missingness
in statistical inferences by modeling the missing data mechanism.</p>
<p>However, most often, missing data may be related to factors that are not
represented in the data set and, thus, the missing data mechanism cannot
be adequately modeled. For example, there may be a tendency for test
scores to be missing from school administrative data files for students
who are poor academic performers. Rubin calls this form of missingness
<em>nonignorable</em>. With nonignorable missing data, the reasons for the
missing observations depend on the values that are missing. When we
suspect a nonignorable missing data mechanism, we need to use procedures
much more complex than will be described here. Little and Rubin
<span class="citation">(Little and Rubin <a href="#ref-little2014statistical">2014</a>)</span> and Schafer <span class="citation">(Schafer <a href="#ref-schafer1997analysis">1997</a>)</span> discuss
methods that can be used for nonignorable missing data. Ruling out a
nonignorable response mechanism can simplify the analysis considerably.</p>
<p>In practice, it is quite difficult to obtain empirical evidence about
whether or not the data are MCAR or MAR. Understanding the data
generation process is invaluable for specifying models that
appropriately represent the missing data mechanism and that will then be
successful in compensating for missing data in an analysis. (Schafer and
Graham <span class="citation">(Schafer and Graham <a href="#ref-schafer2002missing">2002</a>)</span> provide a more thorough discussion of this
issue.)</p>
<p>One strategy for ensuring that the missing data mechanism can be
successfully modeled is to have available on the data set many variables
that may be causally related to missing data. For example, features such
as personal income are subject to high item missingness, and often the
missingness is related to income. However, less sensitive, surrogate
variables such as years of education or type of employment may be less
subject to missingness. The statistical relationship between income and
other income-related variables increases the chance that information
lost in missing variables is supplemented by other completely observed
variables. Model-based methods use the multivariate relationship between
variables to handle the missing data. Thus, the more informative the
data set, the more measures we have on important constructs, the more
successfully we can compensate for missing data using model-based
approaches.</p>
</div>
<div id="sec:10-2.2" class="section level3">
<h3><span class="header-section-number">10.2.2</span> Extending the framework to big data</h3>
<p>The processes involved in generating big data are as varied as big data
itself. This diversity substantially complicates the goal of providing a
detailed error framework that is applicable to all big data.
Nevertheless, some progress can be made by considering the steps that
are typically involved in creating a data set from big data, which often
includes three stages: (a) data generation (see
Chapter <a href="chap-web.html#chap:web">Working with Web Data and APIs</a>), (b) extract, transform and load (ETL), and (c)
analysis (see, for example,
Chapter <a href="chap-text.html#chap:text">Text Analysis</a>). As noted there, this mapping of the process is
oversimplified for some applications. For example, data that flow
continuously from their sources may not be directed through an ETL
process at all. Rather it may be gathered in real time and processed on
an ad hoc basis. Likewise, the ETL processes may be recursive in that,
as errors are identified, the processes may be altered and refined. In
addition, the transform stage may generate new data (e.g., proxy
variables). The analysis stage may also involve language transformations
such as translation and character set transformations. Finally, there
may downstream processes that lead to additional iterations of the
stages in the process map. Thus, the model outline here is but an
initial attempt to capture the tremendous complexities that may be
involved in the big data life cycle.</p>
<p>Figure <a href="chap-errors.html#fig:fig10-2">10.2</a>
graphically depicts the flow of data along the major steps in the
process. One might imagine many more arrows among the boxes within the
ETL and analysis stages that represent the recursive nature of those
stages. The severity of the errors that arise from these processes will
depend on the specific data sources and analytic goals involved.
Nevertheless, we can still consider how each stage might create errors
in a more generic fashion.</p>
<div class="figure" style="text-align: center"><span id="fig:fig10-2"></span>
<img src="ChapterError/figures/fig10-2.png" alt="Big data process map" width="70%" />
<p class="caption">
Figure 10.2: Big data process map
</p>
</div>

<p>For example, data generation error is somewhat analogous to errors
arising in survey data collection. Like surveys, the data-generating
process for big data is subject to erroneous, incomplete, and missing
data. In addition, the data-generating sources may be selective in that
the data collected may not represent a well-defined population or one
that is representative of a target population of inference in an
analysis. Thus, data generation errors include low signal-to-noise
ratio, lost signals, incomplete or missing values, systematic errors,
selective elements, and metadata that are lacking, absent, or erroneous.</p>
<p>ETL processes may be quite similar to various data processing stages for
surveys. These may include creating or enhancing metadata, record
matching, variable coding, editing, data munging (or scrubbing), and
data integration (i.e., linking and merging records and files across
disparate systems). ETL errors include specification errors (including
errors in metadata), matching errors, coding errors, editing errors,
data munging errors, and data integration errors.</p>
<p>Finally, the analysis of big data introduces risks for a number of
errors that will be described in more detail in the next section. These
risks may be due to noise accumulation, coincidental correlations, and
incidental endogeneity. As we shall see, these errors arise as a
consequence of big data volume and variety even when the big data itself
is infallible. Erroneous data compound these problems and can lead to
other issues that are described in some detail below. For example, big
data can be subject to sampling errors when the data are filtered,
sampled, or otherwise reduced to form more manageable or representative
data sets. So-called nonsampling errors can arise when the analysis
involves further transforming the data and weighting the data elements,
as well as can be errors due to modeling and estimation. The latter
errors are similar to modeling and estimation errors in surveys and may
include inadequate or erroneous adjustments for representativeness,
improper or erroneous weighting, computation and algorithmic errors,
model misspecification, and so on.</p>
</div>
</div>
<div id="sec:10-3" class="section level2">
<h2><span class="header-section-number">10.3</span> Illustrations of errors in big data</h2>
<p>A well-known example of the risks of big data error is provided by the
Google Flu Trends series that uses Google searches on flu symptoms,
remedies, and other related key words to provide near-real-time
estimates of flu activity in the USA and 24 other countries<a href="#fn56" class="footnote-ref" id="fnref56"><sup>56</sup></a>. Compared to
CDC data, the Google Flu Trends provided remarkably accurate indicators
of flu incidence in the USA between 2009 and 2011. However, for the
2012–2013 flu seasons, Google Flu Trends predicted more than double the
proportion of doctor visits for flu-like symptoms compared to the CDC
<span class="citation">(Butler <a href="#ref-butler2013google">2013</a>)</span>. Lazer et al. <span class="citation">(Lazer et al. <a href="#ref-lazer2014parable">2014</a>)</span> cite two causes of
this error: big data hubris and algorithm dynamics.</p>
<p>Hubris occurs when the big data researcher believes that the volume of
the data compensates for any of its deficiencies, thus obviating the
need for traditional, scientific analytic approaches. As Lazer et
al. <span class="citation">(Lazer et al. <a href="#ref-lazer2014parable">2014</a>)</span> note, big data hubris fails to recognize that
“quantity of data does not mean that one can ignore foundational issues
of measurement and construct validity and reliability.”</p>
<p>Algorithm dynamics refers to properties of algorithms that allow them to
adapt and “learn” as the processes generating the data change over time.
Although explanations vary, the fact remains that Google Flu Trends was
too high and by considerable margins for 100 out of 108 weeks starting
in July 2012. Lazer et al. <span class="citation">(Lazer et al. <a href="#ref-lazer2014parable">2014</a>)</span> also blame “blue team
dynamics,” which arises when the data generating engine is modified in
such a way that the formerly highly predictive search terms eventually
failed to work. For example, when a Google user searched on “fever” or
“cough,” Google’s other programs started recommending searches for flu
symptoms and treatments—the very search terms the algorithm used to
predict flu. Thus, flu-related searches artificially spiked as a result
of these changes to the algorithm and the impact these changes had on
user behavior. In survey research, this is similar to the measurement
biases induced by interviewers who suggest to respondents who are
coughing that they might have flu, then ask the same respondents if they
think they might have flu.</p>
<p>Algorithm dynamic issues are not limited to Google. Platforms such as
Twitter and Facebook are also frequently modified to improve the user
experience. A key lesson provided by Google Flu Trends is that
successful analyses using big data today may fail to produce good
results tomorrow. All these platforms change their methodologies more or
less frequently, with ambiguous results for any kind of long-term study
unless highly nuanced methods are routinely used. Recommendation engines
often exacerbate effects in a certain direction, but these effects are
hard to tease out. Furthermore, other sources of error may affect Google
Flu Trends to an unknown extent. For example, selectivity may be an
important issue because the demographics of people with Internet access
are quite different from the demographic characteristics related to flu
incidence <span class="citation">(Thompson, Comanor, and Shay <a href="#ref-thompson2006epidemiology">2006</a>)</span>. Thus, the “at risk” population
for influenza and the implied population based on Google searches do not
correspond. This illustrates just one type of representativeness issue
that often plagues big data analysis. In general it is an issue that
algorithms are not (publicly) measured for accuracy, since they are
often proprietary. Google Flu Trends is special in that it publicly
failed. From what we have seen, most models fail privately and often
without anyone at all noticing.</p>
<p>In the next section, we consider the impact of errors on some forms of
analysis that are common in the big data literature. Due to space
limitations, our focus is limited to the effects of content errors on
data analysis. However, there are numerous resources available for
studying and mitigating the effects of missing data on analysis; in
particular, the books by Little and Rubin <span class="citation">(Little and Rubin <a href="#ref-little2014statistical">2014</a>)</span>,
Schafer <span class="citation">(Schafer <a href="#ref-schafer1997analysis">1997</a>)</span>, and Allison <span class="citation">(Allison <a href="#ref-allison2001missing">2001</a>)</span> are
quite relevant to big data analytics.</p>
</div>
<div id="sec:10-4" class="section level2">
<h2><span class="header-section-number">10.4</span> Errors in big data analytics</h2>
<p>In this section, we consider some common types of errors in big data
analytics. The next section considers analytic errors caused by the
so-called “three Vs” of big data, namely, volume, velocity, and variety,
under the assumption of perfect “veracity.” If we relax the assumption
of perfect veracity, the errors that afflict big data as a result of the
three Vs can be exacerbated in largely unpredictable ways. Section
<a href="chap-errors.html#sec:10-4.2">Errors resulting from lack of veracity</a> considers three common types of analysis when data lack veracity:
classification, correlation, and regression. While the scope of our
review is necessarily limited, hopefully the discussion will lay the
foundation for further study regarding the effects of errors on big data
analytics.</p>
<div id="sec:10-4.1" class="section level3">
<h3><span class="header-section-number">10.4.1</span> Errors resulting from volume, velocity, and variety, assuming perfect veracity</h3>
<p>Data deficiencies represent only one set of challenges for the big data
analyst. Other challenges arise solely as a result of the massive size,
rapid generation, and vast dimensionality of the data. As a consequence
of these so-called three Vs of big data, Fan et al. <span class="citation">(Fan, Han, and Liu <a href="#ref-fan2014challenges">2014</a>)</span>
identify three issues— noise accumulation, spurious correlations, and
incidental endogeneity—which will be briefly discussed in this
section. These issues should concern big data analysts even if the data
could be regarded as infallible. Content errors, missing data, and other
data deficiencies will only exacerbate</p>
<hr />
<p><strong>Example: Noise accumulation</strong></p>
<p>To illustrate noise accumulation, Fan et al. <span class="citation">(Fan, Han, and Liu <a href="#ref-fan2014challenges">2014</a>)</span>
consider the following scenario. Suppose an analyst is interested in
classifying individuals into two categories, <span class="math inline">\(C_{1}\)</span> and <span class="math inline">\(C_{2}\)</span>, based
upon the values of 1,000 variables in a big data set. Suppose further
that, unknown to the researcher, the mean value for persons in <span class="math inline">\(C_{1}\)</span>
is 0 on all 1,000 variables while persons in <span class="math inline">\(C_{2}\)</span> have a mean of 3 on
the first 10 variables and 0 on all other variables. Since we are
assuming the data are error-free, a classification rule based upon the
first <span class="math inline">\(m \le 10\)</span> variables performs quite well, with little
classification error. However, as more and more variables are included
in the rule, classification error increases because the uninformative
variables (i.e., the 990 variables having no discriminating power)
eventually overwhelm the informative signals (i.e., the first 10
variables). In the Fan et al. <span class="citation">(Fan, Han, and Liu <a href="#ref-fan2014challenges">2014</a>)</span> example, when
<span class="math inline">\(m &gt; 200\)</span>, the accumulated noise exceeds the signal embedded in the
first 10 variables and the classification rule becomes equivalent to a
coin-flip classification rule.</p>
<hr />
<p>High dimensionality can also introduce coincidental (or <em>spurious</em>)
correlations in that many unrelated variables may be highly correlated
simply by chance, resulting in false discoveries and erroneous
inferences. The phenomenon depicted in Figure
<a href="chap-errors.html#fig:fig10-3">10.3</a>, is an
illustration of this. Many more examples can be found on a website and
in a book devoted to the topic <span class="citation">(Vigen, <a href="#ref-spurious">n.d.</a>, <a href="#ref-spurious2">2015</a>)</span>. Fan et
al. <span class="citation">(Fan, Han, and Liu <a href="#ref-fan2014challenges">2014</a>)</span> explain this phenomenon using simulated
populations and relatively small sample sizes. They illustrate how, with
800 independent (i.e., uncorrelated) variables, the analyst has a 50%
chance of observing an absolute correlation that exceeds 0.4. Their
results suggest that there are considerable risks of false inference
associated with a purely empirical approach to predictive analytics
using high-dimensional data.</p>
<div class="figure" style="text-align: center"><span id="fig:fig10-3"></span>
<img src="ChapterError/figures/fig10-3.png" alt="An illustration of coincidental correlation between two variables: stork die-off linked to human birth decline [@sies1988new]" width="70%" />
<p class="caption">
Figure 10.3: An illustration of coincidental correlation between two variables: stork die-off linked to human birth decline <span class="citation">(Sies <a href="#ref-sies1988new">1988</a>)</span>
</p>
</div>
<p>Finally, turning to incidental endogeneity, a key assumption in
regression analysis is that the model covariates are uncorrelated with
the residual error; endogeneity refers to a violation of this
assumption. For high-dimensional models, this can occur purely by
chance—a phenomenon Fan and Liao <span class="citation">(Fan and Liao <a href="#ref-fan2014endogeneity">2014</a>)</span> call
<em>incidental endogeneity</em>. Incidental endogeneity leads to the modeling
of spurious variation in the outcome variables resulting in errors in
the model selection process and biases in the model predictions. The
risks of incidental endogeneity increase as the number of variables in
the model selection process grows large. Thus it is a particularly
important concern for big data analytics.</p>
<p>Fan et al. <span class="citation">(Fan, Han, and Liu <a href="#ref-fan2014challenges">2014</a>)</span> as well as a number of other authors
<span class="citation">(Stock and Watson <a href="#ref-stock2002forecasting">2002</a>; Fan, Samworth, and Wu <a href="#ref-fan2009ultrahigh">2009</a>)</span> (see, for example, Hall and
Miller <span class="citation">(P. Hall and Miller <a href="#ref-HallMiller2009">2009</a>)</span>; Fan and Liao, <span class="citation">(Fan and Liao <a href="#ref-FanLiao2012">2012</a>)</span>) suggest robust
statistical methods aimed at mitigating the risks of noise accumulation,
spurious correlations, and incidental endogeneity. However, as
previously noted, these issues and others are further compounded when
data errors are present in a data set. Biemer and Trewin
<span class="citation">(Biemer and Trewin <a href="#ref-biemer1997review">1997</a>)</span> show that data errors will bias the results of
traditional data analysis and inflate the variance of estimates in ways
that are difficult to evaluate or mitigate in the analysis process.</p>
</div>
<div id="sec:10-4.2" class="section level3">
<h3><span class="header-section-number">10.4.2</span> Errors resulting from lack of veracity</h3>
<p>The previous section examined some of the issues big data analysts face
as either <span class="math inline">\(N\)</span> or <span class="math inline">\(p\)</span> in Figure <a href="chap-errors.html#fig:fig10-1">10.1</a> becomes extremely large. When row, column, and
cell errors are added into the mix, these <em>volumatic</em> problems can be
further exacerbated. For example, noise accumulation can be expected to
accelerate when random noise (i.e., content errors) afflicts the data.
Spurious correlations that give rise to both incidental endogeneity and
coincidental correlations can render correlation analysis meaningless if
the error levels in big data are not mitigated<a href="#fn57" class="footnote-ref" id="fnref57"><sup>57</sup></a>. In this section, we
consider some of the issues that arise in classification, correlation,
and regression analysis as a result of content errors that may be either
variable or systematic. The current literature on big data error
acknowledges that the data may be noisy, that is, subject to variable
errors. However, there appears to be little recognition of the problems
associated with systematic or correlated errors, particularly those
introduced when data are combined from disparate sources that may be
subject to source-specific errors.</p>
<p>There are various important findings in this section. First, for rare
classes, even small levels of error can impart considerable biases in
classification analysis. Second, variable errors will attenuate
correlations and regression slope coefficients; however, these effects
can be mitigated by forming meaningful aggregates of the data and
substituting these aggregates for the individual units in these
analyses. Third, unlike random noise, systematic errors can bias
correlation and regression analysis is unpredictable ways, and these
biases cannot be effectively mitigated by aggregating the data. Finally,
multilevel modeling can be an important mitigation strategy for dealing
with systematic errors emanating from multiple data sources. These
issues will be examined in some detail in the remainder of this section.</p>
<div id="sec:10-4.2.1" class="section level4">
<h4><span class="header-section-number">10.4.2.1</span> Variable and correlated error</h4>
<p>Error models are essential for understanding the effects of error on
data sets and the estimates that may be derived from them. They allow us
to concisely and precisely communicate the nature of the errors that are
being considered, the general conditions that give rise to them, how
they affect the data, how they may affect the analysis of these data,
and how their effects can be evaluated and mitigated. In the remainder
of this chapter, we focus primarily on content errors and consider two
types of error, variable errors and correlated errors, the latter a
subcategory of systematic errors.</p>
<p>Variable errors are sometimes referred to as <em>random noise</em> or
<em>uncorrelated</em> errors. For example, administrative databases often
contain errors from a myriad of random causes, including mistakes in
keying or other forms of data capture, errors on the part of the persons
providing the data due to confusion about the information requested,
difficulties in recalling information, the vagaries of the terms used to
request the inputs, and other system deficiencies.</p>
<p>Correlated errors, on the other hand, carry a systematic effect that
results in a nonzero covariance between the errors of two distinct
units. For example, quite often, an analysis data set may combine
multiple data sets from different sources and each source may impart
errors that follow a somewhat different distribution. As we shall see,
these differences in error distributions can induce correlated errors in
the merged data set. It is also possible that correlated errors are
induced from a single source as a result of different operators (e.g.,
computer programmers, data collection personnel, data editors, coders,
data capture mechanisms) handling the data. Differences in the way these
operators perform their tasks have the potential to alter the error
distributions so that data elements handled by the same operator have
errors that are correlated <span class="citation">(Biemer and Lyberg <a href="#ref-biemer2003">2003</a>)</span>.</p>
<p>These concepts may be best expressed by a simple error model. Let
<span class="math inline">\(y_{rc}\)</span> denote the cell value for variable <span class="math inline">\(c\)</span> on the <span class="math inline">\(r\)</span>th unit in the
data set, and let <span class="math inline">\(\varepsilon_{rc}\)</span> denote the error associated with
this value. Suppose it can be assumed that there is a true value
underlying <span class="math inline">\(y_{rc}\)</span>, which is denoted by <span class="math inline">\(\mu_{rc}\)</span>. Then we can write
<span class="math display">\[\label{eq:10-1.1}
y_{rc} = \mu_{rc} + \varepsilon_{rc}.\]</span></p>
<p>At this point, <span class="math inline">\(\varepsilon_{rc}\)</span> is not stochastic in nature because a
statistical process for generating the data has not yet been assumed.
Therefore, it is not clear what <em>correlated error</em> really means. To
remedy this problem, we can consider the hypothetical situation where
the processes generating the data set can be repeated under the same
general conditions (i.e., at the same point in time with the same
external and internal factors operating). Each time the processes are
repeated, a different set of errors may be realized. Thus, it is assumed
that although the true values, <span class="math inline">\(\mu_{rc}\)</span>, are fixed, the errors,
<span class="math inline">\(\varepsilon_{rc}\)</span>, can vary across the hypothetical, infinite
repetitions of the data set generating process. Let <span class="math inline">\(\mbox{E}(\cdot)\)</span>
denote the expected value over all these hypothetical repetitions, and
define the variance, <span class="math inline">\(\mathrm{Var}(\cdot)\)</span>, and covariance,
<span class="math inline">\(\mathrm{Cov}(\cdot)\)</span>, analogously.</p>
<p>For the present, error correlations between variables are not
considered, and thus the subscript, <span class="math inline">\(c\)</span>, is dropped to simplify the
notation. For the uncorrelated data model, we assume that
<span class="math inline">\({\rm E}(y_r \vert r) = \mu_r\)</span>, <span class="math inline">\({\rm Var}(y_r \vert r) = \sigma_\varepsilon^2\)</span>, and <span class="math inline">\({\rm Cov}(y_r ,y_s \vert r,s) = 0\)</span>, for
<span class="math inline">\(r \ne s\)</span>. For the correlated data model, the latter assumption is
relaxed. To add a bit more structure to the model, suppose the data set
is the product of combining data from multiple sources (or operators)
denoted by <span class="math inline">\(j = 1, 2, \ldots, J\)</span>, and let <span class="math inline">\(b_j\)</span> denote the systematic effect of the <span class="math inline">\(j\)</span>th source. Here
we also assume that, with each hypothetical repetition of the data set
generating process, these systematic effects can vary stochastically.
(It is also possible to assume the systematic effects are fixed. See,
for example, Biemer and Stokes <span class="citation">(Biemer and Stokes <a href="#ref-BiemerStokes1991">1991</a>)</span> for more details on
this model.) Thus, we assume that <span class="math inline">\({\rm E}(b_j ) = 0\)</span>,
<span class="math inline">\({\rm Var}(b_j ) = \sigma_b^2\)</span>, and <span class="math inline">\({\rm Cov}(b_j ,b_k ) = 0\)</span> for <span class="math inline">\(j \ne k\)</span>.</p>
<p>Finally, for the <span class="math inline">\(r\)</span>th unit within the <span class="math inline">\(j\)</span>th source, let
<span class="math inline">\(\varepsilon_{rj} = b_j + e_{rj}\)</span>. Then it follows that
<span class="math display">\[\label{eq:10-1.2}
\begin{array}{lcl@{\quad}l}
\mathrm{Cov}(\varepsilon_{rj} ,\varepsilon_{sk}) =
\begin{cases}
\sigma_b^2 + \sigma_\varepsilon^2 &amp; \text{for } r = s,j = k, \\
%&amp; =&amp;
\sigma_\varepsilon^2&amp;\text{for } r = s,j \ne k,  \\
% &amp;=&amp;
 0 &amp; \text{for } r \ne s,j \ne k.
 \end{cases}
\end{array}\]</span> The case where <span class="math inline">\(\sigma_b^2 = 0\)</span> corresponds to the
uncorrelated error model (i.e., <span class="math inline">\(b_j = 0\)</span>) and thus <span class="math inline">\(\varepsilon_{rj}\)</span>
is purely random noise.</p>
<hr />
<p><strong>Example: Speed sensor</strong></p>
<p>Suppose that, due to calibration error, the <span class="math inline">\(j\)</span>th speed sensor in a
traffic pattern study underestimates the speed of vehicle traffic on a
highway by an average of 4 miles per hour. Thus, the model for this
sensor is that the speed for the <span class="math inline">\(r\)</span>th vehicle recorded by this sensor
<span class="math inline">\((y_{rj})\)</span> is the vehicle’s true speed <span class="math inline">\((\mu_{rj})\)</span> minus 4 mph
(<span class="math inline">\(b_{j}\)</span>) plus a random departure from <span class="math inline">\(-4\)</span> for the <span class="math inline">\(r\)</span>th vehicle
(<span class="math inline">\(\varepsilon_{rj}\)</span>). Note that to the extent that <span class="math inline">\(b_{j}\)</span> varies across
sensors <span class="math inline">\(j = 1,\ldots ,J\)</span> in the study, <span class="math inline">\(\sigma_b^2\)</span> will be large.
Further, to the extent that ambient noise in the readings for <span class="math inline">\(j\)</span>th
sensor causes variation around the values <span class="math inline">\(\mu_{rc} + b_j\)</span>, then
<span class="math inline">\(\sigma_\varepsilon^2\)</span> will be large. Both sources of variation will
reduce the reliability of the measurements. However, as shown in Section
<a href="chap-errors.html#sec:10-4.2.4">Correlation analysis</a>,
the systematic error component is particularly problematic for many
types of analysis.</p>
<hr />
</div>
<div id="sec:10-4.2.2" class="section level4">
<h4><span class="header-section-number">10.4.2.2</span> Models for categorical data</h4>
<p>For variables that are categorical, the model of the previous section is
not appropriate because the assumptions it makes about the error
structure do not hold. For example, consider the case of a binary
(<span class="math inline">\(0/1\)</span>) variable. Since both <span class="math inline">\(y_r\)</span> and <span class="math inline">\(\mu_r\)</span> should be either 1 or 0,
the error in equation (10.1) must assume the values of <span class="math inline">\(-1\)</span>, <span class="math inline">\(0\)</span>, or <span class="math inline">\(1\)</span>. A
more appropriate model is the misclassification model described by
Biemer <span class="citation">(Biemer <a href="#ref-biemer2011latent">2011</a>)</span>, which we summarize here.</p>
<p>Let <span class="math inline">\(\phi_r\)</span> denote the probability of a false positive error (i.e.,
<span class="math inline">\(\phi_r = \Pr (y_r = 1\vert \mu_r = 0)\)</span>), and let <span class="math inline">\(\theta_r\)</span> denote the
probability of a false negative error (i.e.,
<span class="math inline">\(\theta_r =\Pr (y_r = 0\vert \mu_r = 1)\)</span>). Thus, the probability that
the value for row <span class="math inline">\(r\)</span> is correct is <span class="math inline">\(1 - \theta_r\)</span> if the true value is
<span class="math inline">\(1\)</span>, and <span class="math inline">\(1 - \phi_r\)</span> if the true value is <span class="math inline">\(0\)</span>.</p>
<p>As an example, suppose an analyst wishes to compute the proportion,
<span class="math inline">\(P = \sum_r {y_r / N}\)</span>, of the units in the file that are classified as
<span class="math inline">\(1\)</span>, and let <span class="math inline">\(\pi = \sum_r {\mu_r / N}\)</span> denote the true proportion. Then
under the assumption of uncorrelated error, Biemer <span class="citation">(Biemer <a href="#ref-biemer2011latent">2011</a>)</span>
shows that <span class="math display">\[\label{eq:10-1.3}
P = \pi (1 - \theta ) + (1 - \pi )\phi,\]</span> where
<span class="math inline">\(\theta = \sum_r {\theta_r / N}\)</span> and <span class="math inline">\(\phi = \sum_r {\phi_r / N}\)</span>.</p>
<p>In the classification error literature, the sensitivity of a classifier
is defined as <span class="math inline">\(1 - \theta\)</span>, that is, the probability that a true
positive is correctly classified. Correspondingly, <span class="math inline">\(1 - \phi\)</span> is
referred to as the specificity of the classifier, that is, the
probability that a true negative is correctly classified. Two other
quantities that will be useful in our study of misclassification error
are the positive predictive value (PPV) and negative predictive value
(NPV) given by <span class="math display">\[\label{eq:10-1.4}
\mathrm{PPV} = \Pr (\mu_r = 1\vert y_r = 1),\quad\mathrm{NPV} = \Pr
(\mu_r = 0\vert y_r = 0).\]</span> The PPV (NPV) is the probability that a
positive (negative) classification is correct.</p>
</div>
<div id="sec:10-4.2.3" class="section level4">
<h4><span class="header-section-number">10.4.2.3</span> Misclassification and rare classes</h4>
<p>Fan et al. <span class="citation">(Fan, Han, and Liu <a href="#ref-fan2014challenges">2014</a>)</span> and many others have stated that one of
the strengths of big data is the ability to study rare population groups
that seldom show up in large enough numbers in designed studies such as
surveys and clinical trials. While this is true in theory, in practice
content errors can quickly overwhelm such an analysis, rendering the
data useless for this purpose. We illustrate this using the following
contrived and somewhat amusing example. The results in this section are
particularly relevant to the approaches considered in
Chapter <a href="chap-ml.html#chap:ml">Machine Learning</a>.</p>
<hr />
<p><strong>Example: Thinking about probabilities</strong></p>
<p>Suppose, using big data and other resources, we construct a terrorist
detector and boast that the detector is 99.9% accurate. In other words,
both the probability of a false negative (i.e., classifying a terrorist
as a nonterrorist, <span class="math inline">\(\theta\)</span>) and the probability of a false positive
(i.e., classifying a nonterrorist as a terrorist, <span class="math inline">\(\phi\)</span>) are 0.001.
Assume that about <span class="math inline">\(1\)</span> person in a million in the population is a
terrorist, that is, <span class="math inline">\(\pi = 0.000001\)</span> (hopefully, somewhat of an
overestimate). Your friend, Terry, steps into the machine and, to
Terry’s chagrin (and your surprise) the detector declares that he is a
terrorist! What are the odds that the machine is right? The surprising
answer is only about 1 in 1000. That is, 999 times out of 1,000 times
the machine classifies a person as a terrorist, the machine will be
wrong!</p>
<hr />
<p>How could such an accurate machine be wrong so often in the terrorism
example? Let us do the math.</p>
<p>The relevant probability is the PPV of the machine: given that the
machine classifies an individual (Terry) as a terrorist, what is the
probability the individual is truly a terrorist? Using the notation in
Section <a href="chap-errors.html#sec:10-4.2.2">Models for categorical data</a> and Bayes’ rule, we can derive the PPV as
<span class="math display">\[\begin{aligned}
\Pr (\mu_r = 1\vert y_r = 1) &amp;=  \frac{\Pr (y_r = 1\vert \mu_r =
1)\Pr(\mu_r = 1)}{\Pr (y_r = 1)} \\
&amp;= \frac{(1 - \theta )\pi }{\pi (1 - \theta ) + (1 - \pi )\phi } \\
&amp;=  \frac{0.999\times 0.000001}{0.000001\times 0.999 + 0.99999\times 0.001} \\
 &amp;\approx  0.001.\end{aligned}\]</span></p>
<p>This example calls into question whether security surveillance using
emails, phone calls, etc. can ever be successful in finding rare threats
such as terrorism since to achieve a reasonably high PPV (say, 90%)
would require a sensitivity and specificity of at least <span class="math inline">\(1-10^{-7}\)</span>, or
less than 1 chance in 10 million of an error.</p>
<p>To generalize this approach, note that any population can be regarded as
a <em>mixture</em> of subpopulations. Mathematically, this can be written as
<span class="math display">\[\label{eq:10-1.5}
f(y\vert \mathbf{x};{\boldsymbol \eth}) = \pi_1 f(y\vert
\mathbf{x};\eth_1 ) + \pi_2 f(y\vert \mathbf{x};\eth_2 ) +
\ldots + \pi_K f(y\vert \mathbf{x};\eth_K ),\]</span> where
<span class="math inline">\(f(y\vert \mathbf{x}; {\boldsymbol \theta})\)</span> denotes the population
distribution of <span class="math inline">\(y\)</span> given the vector of explanatory variables
<span class="math inline">\(\mathbf{x}\)</span> and the parameter vector <span class="math inline">\({\boldsymbol \theta } = (\theta_1 ,\theta_2, \ldots, \theta_K )\)</span>, <span class="math inline">\(\pi _k\)</span> is the proportion
of the population in the <span class="math inline">\(k\)</span>th subgroup, and <span class="math inline">\(f(y\vert \mathbf{x};\theta_k)\)</span> is the distribution of <span class="math inline">\(y\)</span> in the <span class="math inline">\(k\)</span>th subgroup.
A rare subgroup is one where <span class="math inline">\(\pi_k\)</span> is quite small (say, less than
0.01).</p>
<p>Table <a href="chap-errors.html#tab:table10-1">10.1</a> shows the PPV for a range of rare subgroup sizes
when the sensitivity is perfect (i.e., no misclassification of true
positives) and specificity is not perfect but still high. This table
reveals the fallacy of identifying rare population subgroups using
fallible classifiers unless the accuracy of the classifier is
appropriately matched to the rarity of the subgroup. As an example, for
a 0.1% subgroup, the specificity should be at least 99.99%, even with
perfect sensitivity, to attain a 90% PPV.</p>
<table>
<caption><span id="tab:table10-1">Table 10.1: </span> Positive predictive value (%) for rare subgroups, high specificity, and perfect sensitivity</caption>
<thead>
<tr class="header">
<th><strong><span class="math inline">\(\pi_k\)</span></strong></th>
<th align="center"></th>
<th align="center"><strong>Specificity</strong></th>
<th align="center"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td align="center"><em>99%</em></td>
<td align="center"><em>99.9%</em></td>
<td align="center"><em>99.99%</em></td>
</tr>
<tr class="even">
<td>0.1</td>
<td align="center">91.70</td>
<td align="center">99.10</td>
<td align="center">99.90</td>
</tr>
<tr class="odd">
<td>0.01</td>
<td align="center">50.30</td>
<td align="center">91.00</td>
<td align="center">99.00</td>
</tr>
<tr class="even">
<td>0.001</td>
<td align="center">9.10</td>
<td align="center">50.00</td>
<td align="center">90.90</td>
</tr>
<tr class="odd">
<td>0.0001</td>
<td align="center">1.00</td>
<td align="center">9.10</td>
<td align="center">50.00</td>
</tr>
</tbody>
</table>
</div>
<div id="sec:10-4.2.4" class="section level4">
<h4><span class="header-section-number">10.4.2.4</span> Correlation analysis</h4>
<p>In Section <a href="chap-errors.html#sec:10-4.1">Errors resulting from volume, velocity, and variety, assuming perfect veracity</a>, we considered the problem of incidental
correlation that occurs when an analyst correlates pairs of variables
selected from big data stores containing thousands of variables. In this
section, we discuss how errors in the data can exacerbate this problem
or even lead to failure to recognize strong associations among the
variables. We confine the discussion to the continuous variable model of
Section <a href="chap-errors.html#sec:10-4.2.1">Variable and correlated error</a> and begin with theoretical results that help
explain what happens in correlation analysis when the data are subject
to variable and systematic errors.</p>
<p>For any two variables in the data set, <span class="math inline">\(c\)</span> and <span class="math inline">\(d\)</span>, define the
covariance between <span class="math inline">\(y_{rc}\)</span> and <span class="math inline">\(y_{rd}\)</span> as <span class="math display">\[\label{eq:10-1.6}
\sigma_{y\vert cd} = \frac{\sum\nolimits_r {\mbox{E}(y_{rc} -
\bar{y}_c )(y_{rd} - \bar {y}_d )} }{N},\]</span> where the expectation is with
respect to the error distributions and the sum extends over all rows in
the data set. Let
<span class="math display">\[\sigma_{\mu \vert cd} = \frac{\sum\nolimits_r {(\mu_{rc} - \bar{\mu
}_c } )(\mu_{rd} - \bar{\mu }_d )}{N}\]</span> denote the <em>population</em>
covariance. (The population is defined as the set of all units
corresponding to the rows of the data set.) For any variable <span class="math inline">\(c\)</span>, define
the variance components <span class="math display">\[\sigma_{y\vert c}^2 = \frac{\sum_r {(y_{rc} -
\bar{y}_c )^2}}{N},\quad \sigma_{\mu \vert c}^2 =\frac{%
\sum_r {(\mu_{rc} - \bar {\mu }_c } )^2}{N},\]</span> and let <span class="math display">\[R_c
= \frac{\sigma_{\mu \vert c}^2}{\sigma_{\mu \vert c}^2 +
\sigma_{b\vert c}^2 + \sigma_{\varepsilon \vert c}^2},\quad
\rho_c = \frac{\sigma_{b\vert c}^2}{\sigma_{\mu \vert c}^2 +
\sigma_{b\vert c}^2 + \sigma _{\varepsilon \vert c}^2},\]</span> with analogous
definitions for <span class="math inline">\(d\)</span>. The ratio <span class="math inline">\(R_{c}\)</span> is known as the <em>reliability
ratio</em>, and <span class="math inline">\(\rho_c\)</span> will be referred to as the <em>intra-source
correlation</em>. Note that the reliability ratio is the proportion of total
variance that is due to the variation of true values in the data set. If
there were no errors, either variable or systematic, then this ratio
would be 1. To the extent that errors exist in the data, <span class="math inline">\(R_{c}\)</span> will be
less than 1.</p>
<p>Likewise, <span class="math inline">\(\rho_c\)</span> is also a ratio of variance components that reflects
the proportion of total variance that is due to systematic errors with
biases that vary by data source. A value of <span class="math inline">\(\rho_c\)</span> that exceeds 0
indicates the presence of systematic error variation in the data. As we
shall see, even small values of <span class="math inline">\(\rho_c\)</span> can cause big problems in
correlation analysis.</p>
<p>Using the results in Biemer and Trewin <span class="citation">(Biemer and Trewin <a href="#ref-biemer1997review">1997</a>)</span>, it can be
shown that the correlation between <span class="math inline">\(y_{rc}\)</span> and <span class="math inline">\(y_{rd}\)</span>, defined as
<span class="math inline">\(\rho_{y\vert cd} = \sigma_{y\vert cd} / \sigma_{y\vert c} \sigma_{y\vert d}\)</span>, can be expressed as
<span class="math display">\[\label{eq:10-1.7} \rho_{y\vert cd} = \sqrt {R_c R_d } \rho_{\mu \vert cd} + \sqrt {\rho_c \rho_d }.\]</span>
Note that if there are no errors (i.e., when
<span class="math inline">\(\sigma_{b\vert c}^2 = \sigma_{\varepsilon \vert c}^2 = 0\)</span>), then <span class="math inline">\(R_c = 1\)</span>,
<span class="math inline">\(\rho_c =0\)</span>, and the correlation between <span class="math inline">\(y_{c}\)</span> and <span class="math inline">\(y_{d}\)</span> is just the
population correlation.</p>
<p>Let us consider the implications of these results first without
systematic errors (i.e., only variable errors) and then with the effects
of systematic errors.</p>
<p><strong>Variable errors only</strong></p>
<p>If the only errors are due to random noise, then the additive term on
the right in equation (10.2) is 0 and <span class="math inline">\(\rho_{y\vert cd} = \sqrt {R_c R_d } \rho _{\mu \vert cd}\)</span>, which says that the correlation is attenuated by
the product of the root reliability ratios. For example, suppose
<span class="math inline">\(R_c = R_d = 0.8\)</span>, which is considered excellent reliability. Then the
observed correlation in the data will be about 80% of the true
correlation; that is, correlation is attenuated by random noise. Thus,
<span class="math inline">\(\sqrt {R_c R_d }\)</span> will be referred to as the <em>attenuation factor</em> for
the correlation between two variables.</p>
<p>Quite often in the analysis of big data, the correlations being explored
are for aggregate measures, as in Figure
<a href="chap-errors.html#fig:fig10-3">10.3</a>. Therefore,
suppose that, rather than being a single element, <span class="math inline">\(y_{rc}\)</span> and <span class="math inline">\(y_{rd}\)</span>
are the means of <span class="math inline">\(n_{rc}\)</span> and <span class="math inline">\(n_{rd}\)</span> independent elements,
respectively. For example, <span class="math inline">\(y_{rc}\)</span> and <span class="math inline">\(y_{rd}\)</span> may be the average rate
of inflation and the average price of oil, respectively, for the <span class="math inline">\(r\)</span>th
year, for <span class="math inline">\(r = 1,\ldots ,N\)</span> years. Aggregated data are less affected by variable errors
because, as we sum up the values in a data set, the positive and
negative values of the random noise components combine and cancel each
other under our assumption that <span class="math inline">\(\mathrm{E}(\varepsilon_{rc} ) = 0\)</span>. In
addition, the variance of the mean of the errors is of order
<span class="math inline">\(O(n_{rc}^{ - 1} )\)</span>.</p>
<p>To simplify the result for the purposes of our discussion, suppose
<span class="math inline">\(n_{rc} = n_c\)</span>, that is, each aggregate is based upon the same sample
size. It can be shown that
equation (10.2) still applies if we replace <span class="math inline">\(R_c\)</span> by its
aggregated data counterpart denoted by
<span class="math inline">\(R_c^A = \sigma_{\mu \vert c}^2 / (\sigma_{\mu \vert c}^2 + \sigma_{\varepsilon \vert c}^2 / n_c )\)</span>. Note that <span class="math inline">\(R_c^A\)</span>
converges to 1 as <span class="math inline">\(n_c\)</span> increases, which means that <span class="math inline">\(\rho _{y\vert cd}\)</span>
will converge to <span class="math inline">\(\rho_{\mu \vert cd}\)</span>. Figure
<a href="chap-errors.html#fig:fig10-4">10.4</a> illustrates
the speed at which this convergence occurs.</p>
<p>In this figure, we assume <span class="math inline">\(n_c = n_d = n\)</span> and vary <span class="math inline">\(n\)</span> from 0 to 60. We
set the reliability ratios for both variables to 0.5 (which is
considered to be a “fair” reliability) and assume a population
correlation of <span class="math inline">\(\rho_{\mu \vert cd} = 0.5\)</span>. For <span class="math inline">\(n\)</span> in the range
<span class="math inline">\([2,10]\)</span>, the attenuation is pronounced. However, above 10 the
correlation is quite close to the population value. Attenuation is
negligible when <span class="math inline">\(n &gt; 30\)</span>. These results suggest that variable error can
be mitigated by aggregating like elements that can be assumed to have
independent errors.</p>
<div class="figure" style="text-align: center"><span id="fig:fig10-4"></span>
<img src="ChapterError/figures/fig10-4.png" alt="Correlation as a function of sample size" width="70%" />
<p class="caption">
Figure 10.4: Correlation as a function of sample size
</p>
</div>
<p><strong>Both variable and systematic errors</strong></p>
<p>If both systematic and variable errors contaminate the data, the
additive term on the right in
equation (10.2) is positive. For aggregate data, the reliability
ratio takes the form <span class="math display">\[\label{eq:10-1.8}
R_c^A = \frac{{\sigma_{\mu |c}^2}}{{\sigma_{\mu |c}^2 + \sigma
_{b|c}^2 + n_c^{ - 1}\sigma_{\varepsilon |c}^2}},\]</span> which converges not
to 1 as in the case of variable error only, but to
<span class="math inline">\(\sigma_{\mu \vert c}^2 / (\sigma_{\mu \vert c}^2 + \sigma_{b\vert c}^2)\)</span>, which will be less than 1. Thus, some attenuation
is possible regardless of the number of elements in the aggregate. In
addition, the intra-source correlation takes the form
<span class="math display">\[\label{eq:10-1.9}
\rho_c^A = \frac{{\sigma_{b|c}^2}}{{\sigma_{\mu |c}^2 +
\sigma_{b|c}^2 + n_c^{ - 1}\sigma_{\varepsilon |c}^2}},\]</span> which
converges to <span class="math inline">\(\rho_c^A = \sigma_{b|c}^2/(\sigma_{\mu |c}^2 + \sigma _{b|c}^2)\)</span>, which converges to <span class="math inline">\(1 - R_c^A\)</span>. Thus, the
systematic effects may still operate for correlation analysis without
regard to the number of elements comprising the aggregates.</p>
<p>For example, consider the illustration in
Figure <a href="chap-errors.html#fig:fig10-4">10.4</a> with
<span class="math inline">\(n_c = n_d = n\)</span>, reliability ratios (excluding systematic effects) set at <span class="math inline">\(0.5\)</span> and
population correlation at <span class="math inline">\(\rho_{\mu \vert cd} = 0.5\)</span>. In this scenario,
let <span class="math inline">\(\rho_c = \rho_d = 0.25\)</span>. Figure
<a href="chap-errors.html#fig:fig10-5">10.5</a> shows the
correlation as a function of the sample size with systematic errors
(dotted line) compared to the correlation without systematic errors
(solid line). Correlation with systematic errors is both inflated and
attenuated. However, at the assumed level of intra-source variation, the
inflation factor overwhelms the attenuation factors and the result is a
much inflated value of the correlation across all aggregate sizes.</p>
<div class="figure" style="text-align: center"><span id="fig:fig10-5"></span>
<img src="ChapterError/figures/fig10-5.png" alt="Correlation as a function of sample size" width="70%" />
<p class="caption">
Figure 10.5: Correlation as a function of sample size
</p>
</div>
<p>To summarize these findings, correlation analysis is attenuated by
variable errors, which can lead to null findings when conducting a
correlation analysis and the failure to identify associations that exist
in the data. Combined with systematic errors that may arise when data
are extracted and combined from multiple sources, correlation analysis
can be unpredictable because both attenuation and inflation of
correlations can occur. Aggregating data mitigates the effects of
variable error but may have little effect on systematic errors.</p>
</div>
<div id="sec:10-4.2.5" class="section level4">
<h4><span class="header-section-number">10.4.2.5</span> Regression analysis</h4>
<p>The effects of variable errors on regression coefficients are well known
<span class="citation">(Cochran <a href="#ref-cochran1968errors">1968</a>; Fuller <a href="#ref-fuller1991regression">1991</a>; Biemer and Trewin <a href="#ref-biemer1997review">1997</a>)</span>. The
effects of systematic errors on regression have been less studied. We
review some results for both types of errors in this section.</p>
<p>Consider the simple situation where we are interested in computing the
population slope and intercept coefficients given by
<span class="math display">\[\label{eq:10-1.10}
b = \frac{\sum_r {(y_r - \bar{y})(x_r - \bar{x})} }{\sum_r {(x_r
- \bar{x})^2} }\quad\mbox{and}\quad b_0 = \bar{y} - b\bar{x},\]</span> where,
as before, the sum extends over all rows in the data set. When <span class="math inline">\(x\)</span> is
subject to variable errors, it can be shown that the observed regression
coefficient will be attenuated from its error-free counterpart. Let
<span class="math inline">\(R_x\)</span> denote the reliability ratio for <span class="math inline">\(x\)</span>. Then <span class="math display">\[\label{eq:10-1.11}
b = R_x B,\]</span> where
<span class="math inline">\(B = \sum_r {(y_r - \bar{y})(\mu_{r\vert x} - \bar{\mu }_x )} / \sum_r {(\mu_{r\vert x} - \bar{\mu }_x )^2}\)</span> is the population
slope coefficient, with <span class="math inline">\(x_r = \mu_{r\vert x} + \varepsilon_{r\vert x}\)</span>, where <span class="math inline">\(\varepsilon_{r\vert x}\)</span> is the variable
error with mean 0 and variance <span class="math inline">\(\sigma_{\varepsilon\vert x}^2\)</span>. It can
also be shown that <span class="math inline">\(\mbox{Bias}(b_0 ) \approx B(1 - R_x )\bar{\mu }_x\)</span>.</p>
<p>As an illustration of these effects, consider the regressions displayed
in Figure <a href="chap-errors.html#fig:fig10-6">10.6</a>, which are based upon contrived data. The
regression on the left is the population (true) regression with a slope
of <span class="math inline">\(1.05\)</span> and an intercept of <span class="math inline">\(-0.61\)</span>. The regression on the left uses
the same <span class="math inline">\(y\)</span>- and <span class="math inline">\(x\)</span>-values. The only difference is that normal error
was added to the <span class="math inline">\(x\)</span>-values, resulting in a reliability ratio of <span class="math inline">\(0.73\)</span>.
As the theory predicted, the slope was attenuated toward <span class="math inline">\(0\)</span> in direct
proportion to the reliability, <span class="math inline">\(R_{x}\)</span>. As random error is added to the
<span class="math inline">\(x\)</span>-values, reliability is reduced and the fitted slope will approach
<span class="math inline">\(0\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:fig10-6"></span>
<img src="ChapterError/figures/fig10-6.png" alt="Regression of *y* on *x* with and without variable error. On the left is the population regression with no error in the *x* variable. On the right, variable error was added to the *x*-values with a reliability ratio of 0.73. Note its attenuated slope, which is very near the theoretical value of 0.77" width="70%" />
<p class="caption">
Figure 10.6: Regression of <em>y</em> on <em>x</em> with and without variable error. On the left is the population regression with no error in the <em>x</em> variable. On the right, variable error was added to the <em>x</em>-values with a reliability ratio of 0.73. Note its attenuated slope, which is very near the theoretical value of 0.77
</p>
</div>

<p>When the dependent variable, <span class="math inline">\(y\)</span>, only is subject to variable error, the
regression deteriorates, but the expected values of the slope and
intercept coefficients are still equal to true to their population
values. To see this, suppose <span class="math inline">\(y_r = \mu_{y\vert r} + \varepsilon_{y\vert r}\)</span>, where <span class="math inline">\(\mu_{r\vert y}\)</span> denotes the
error-free value of <span class="math inline">\(y_r\)</span> and <span class="math inline">\(\varepsilon_{r\vert y}\)</span> is the associated
variable error with variance <span class="math inline">\(\sigma _{\varepsilon \vert y}^2\)</span>. The regression of <span class="math inline">\(y\)</span> on <span class="math inline">\(x\)</span> can now be rewritten as
<span class="math display">\[\label{eq:10-1.12}
\mu_{y\vert r} = b_0 + bx_r + e_r - \varepsilon _{r\vert y},\]</span> where
<span class="math inline">\(e_r\)</span> is the usual regression residual error with mean <span class="math inline">\(0\)</span> and variance
<span class="math inline">\(\sigma_e^2\)</span>, which is assumed to be uncorrelated with
<span class="math inline">\(\varepsilon_{r\vert y}\)</span>. Letting <span class="math inline">\({e}&#39; = e_r - \varepsilon_{r\vert y}\)</span>, it follows that the regression in
equation (10.3) is equivalent to the previously considered
regression of <span class="math inline">\(y\)</span> on <span class="math inline">\(x\)</span> where <span class="math inline">\(y\)</span> is not subject to error, but now the
residual variance is increased by the additive term, that is,
<span class="math inline">\(\sigma_{e}^{\prime2} = \sigma_{\varepsilon \vert y}^2 + \sigma_e^2\)</span>.</p>
<p>Chai <span class="citation">(Chai <a href="#ref-chai1971correlated">1971</a>)</span> considers the case of systematic errors in
the regression variables that may induce correlations both within and
between variables in the regression. He shows that, in the presence of
systematic errors in the independent variable, the bias in the slope
coefficient may either attenuate the slope or increase its magnitude in
ways that cannot be predicted without extensive knowledge of the error
properties. Thus, like the results from correlation analysis, systematic
errors greatly increase the complexity of the bias effects and their
effects on inference can be quite severe.</p>
<p>One approach for dealing with systematic error at the source level in
regression analysis is to model it using, for example, random effects
<span class="citation">(Hox <a href="#ref-hox2010multilevel">2010</a>)</span>. In brief, a random effects model specifies
<span class="math inline">\(y_{ijk} = \beta_{0i}^\ast + \beta x_{ijk} + \varepsilon_{ijk}\)</span>, where
<span class="math inline">\({\varepsilon }&#39;_{ijk} = b_i + \varepsilon_{ijk}\)</span> and
<span class="math inline">\(\mathrm{Var}({\varepsilon }&#39;_{ijk} ) = \sigma_b^2 + \sigma_{\varepsilon \vert j}^2\)</span>. The next section considers other mitigation strategies that
attempt to eliminate the error rather than model it.</p>

</div>
</div>
</div>
<div id="sec:10-5" class="section level2">
<h2><span class="header-section-number">10.5</span> Some methods for mitigating, detecting, and compensating for errors</h2>
<p>For survey data and other <em>designed</em> data collections, error mitigation<a href="#fn58" class="footnote-ref" id="fnref58"><sup>58</sup></a>
begins at the data generation stage by incorporating design strategies
that generate high-quality data that are at least adequate for the
purposes of the data users. For example, missing data can be mitigated
by repeated follow-up of nonrespondents, questionnaires can be perfected
through pretesting and experimentation, interviewers can be trained in
the effective methods for obtaining highly accurate responses, and
computer-assisted interviewing instruments can be programmed to correct
errors in the data as they are generated. For big data, the data
generation process is often outside the purview of the data collectors,
as noted in Section <a href="chap-errors.html#sec:10-1">Introduction</a>, and there is limited opportunity to address
deficiencies in the data generation process. This is because big data is
often a by-product of systems designed with little or no thought given
to the potential secondary uses of the data. Instead, error mitigation
must necessarily begin at the data processing stage, which is the focus
of this chapter, particularly with regard to data editing and cleaning.</p>
<p>In survey collections, data processing may involve data entry, coding
textual responses, creating new variables, editing the data, imputing
missing cells, weighting the observations, and preparing the file,
including the application of disclosure- limiting processes. For big
data, many of these same operations could be needed to some extent,
depending upon requirements of the data, its uses, and the prevailing
statutory requirements<a href="#fn59" class="footnote-ref" id="fnref59"><sup>59</sup></a>. Of all the operations comprising data
processing, the area that has the greatest potential to both alter data
quality and consume vast resources is data editing. For example, Biemer
and Lyberg <span class="citation">(Biemer and Lyberg <a href="#ref-biemer2003">2003</a>)</span> note that national statistical offices may
spend as much as 40% or more of their production budgets on data editing
for some surveys. Recent computer technologies have automated many
formerly manual processes, which has greatly reduced these expenditures.
Nevertheless, editing remains a key component of the quality improvement
process for big data. Therefore, the remainder of this chapter will
discuss the issue of editing big data.</p>
<p>Biemer and Lyberg <span class="citation">(Biemer and Lyberg <a href="#ref-biemer2003">2003</a>)</span> define data editing as a set of
methodologies for identifying and correcting (or transforming) anomalies
in the data. It often involves verifying that various relationships
among related variables of the data set are plausible and, if they are
not, attempting to make them so. Editing is typically a rule-based
approach where rules can apply to a particular variable, a combination
of variables, or an aggregate value that is the sum over all the rows or
a subset of the rows in a data set.</p>
<p>In small data sets, data editing usually starts as a <em>bottom-up</em> process
that applies various rules to the cell values—often referred to as
<em>micro-editing</em>. The rules specify that the values for some variable or
combinations of variables (e.g., a ratio or difference between two
values) should be within some specified range. Editing may reveal
impossible values (so-called <em>fatal</em> edits) as well as values that are
simply highly suspect (leading to <em>query</em> edits). For example, a
pregnant male would generate a fatal edit, while a property that sold
for $500,000 in a low-income neighborhood might generate a query edit.
Once the anomalous values are identified, the process attempts to deduce
more accurate values based upon other variables in the data set or,
perhaps, from an external data set.</p>
<p>The identification of anomalies can be handled quite efficiently and
automatically using editing software. Recently, data mining and machine
learning techniques have been applied to data editing with excellent
results (see Chandola et al. <span class="citation">(Chandola, Banerjee, and Kumar <a href="#ref-chandola2009anomaly">2009</a>)</span> for a review).
Tree-based methods such as classification and regression trees and
random forests are particularly useful for creating editing rules for
anomaly identification and resolution <span class="citation">(Petrakos et al. <a href="#ref-petrakos2004new">2004</a>)</span>. However, some
human review may be necessary to resolve the most complex situations.</p>
<p>For big data, the identification of data anomalies could result in
possibly billions of edit failures. Even if only a tiny proportion of
these required some form of manual review for resolution, the task could
still require the inspection of tens or hundreds of thousands of query
edits, which would be infeasible for most applications. Thus,
micro-editing must necessarily be a completely automated process unless
it can be confined to a relatively small subset of the data. As an
example, a representative (random) subset of the data set could be
edited using manual editing for purposes of evaluating the error levels
for the larger data set, or possibly to be used as a training data set,
benchmark, or reference distribution for further processing, including
recursive learning.</p>
<p>To complement fully automated micro-editing, big data editing usually
involves <em>top-down</em> or <em>macro-editing</em> approaches. For such approaches,
analysts and systems inspect aggregated data for conformance to some
benchmark values or data distributions that are known from either
training data or prior experience. When unexpected or suspicious
aggregates are identified, the analyst can “drill down” into the data to
discover and, if possible, remove the discrepancy by either altering the
value at the source (usually a micro-data element) or delete the
edit-failed value.</p>

<p>Note that an aggregate that is not flagged as suspicious in
macro-editing passes the edit and is deemed correct. However, because
serious offsetting errors can be masked by aggregation, there is no
guarantee that the elements comprising the aggregate are indeed
accurate. Recalling the discussion in
Section <a href="chap-errors.html#sec:10-4.2.1">Variable and correlated error</a> regarding random noise, we know that the data
may be quite noisy while the aggregates may appear to be essentially
error-free. However, macro-editing can be an effective control for
systematic errors and egregious and some random errors that create
outliers and other data anomalies.</p>
<p>Given the volume, velocity, and variety of big data, even macro-editing
can be challenging when thousands of variables and billions of records
are involved. In such cases, the <em>selective editing</em> strategies
developed in the survey research literature can be helpful
<span class="citation">(Granquist and Kovar <a href="#ref-granquist1997editing">1997</a>; De Waal, Pannekoek, and Scholtus <a href="#ref-de2011handbook">2011</a>)</span>. Using selective macro-editing,
query edits are selected based upon the importance of the variable for
the analysis under study, the severity of the error, and the cost or
level of effort involved in investigating the suspicious aggregate.
Thus, only extreme errors, or less extreme errors on critical variables,
would be investigated.</p>
<p>There are a variety of methods that may be effective in macro-editing.
Some of these are based upon data mining <span class="citation">(Natarajan, Li, and Koronios <a href="#ref-natarajan2010data">2010</a>)</span>, machine
learning <span class="citation">(Clarke <a href="#ref-Clarke2014">2014</a>)</span>, cluster analysis
<span class="citation">(Duan et al. <a href="#ref-duan2009cluster">2009</a>; He, Xu, and Deng <a href="#ref-he2003discovering">2003</a>)</span>, and various data visualization
tools such as treemaps
<span class="citation">(Johnson and Shneiderman <a href="#ref-johnson1991tree">1991</a>; Shneiderman <a href="#ref-shneiderman1992tree">1992</a>; Tennekes and Jonge <a href="#ref-tennekes2011top">2011</a>)</span> and
tableplots <span class="citation">(Tennekes, Jonge, and Daas <a href="#ref-tennekes2013visualizing">2013</a>, <a href="#ref-Tennekes2012">2012</a>; Puts, Daas, and Waal <a href="#ref-puts2015finding">2015</a>)</span>.
The tableplot seems particularly well suited for the three Vs of big
data and will be discussed in some detail.</p>
<p>Like other visualization techniques examined in
Chapter <a href="chap-viz.html#chap:viz">Information Visualization</a>, the tableplot has the ability to summarize a
large multivariate data set in a single plot <span class="citation">(Malik, Unwin, and Gribov <a href="#ref-malik2010interactive">2010</a>)</span>. In
editing big data, it can be used to detect outliers and unusual data
patterns. Software for implementing this technique has been written in R
and is available without cost from the Comprehensive R Archive Network
(<a href="https://cran.r-project.org/" class="uri">https://cran.r-project.org/</a>). Figure
<a href="chap-errors.html#fig:fig10-7">10.7</a> shows an example. The key idea is that micro-aggregates of two related
variables should have similar data patterns. Inconsistent data patterns
may signal errors in one of the aggregates that can be investigated and
corrected in the editing process to improve data quality. The tableplot
uses bar charts created for the micro-aggregates to identify these
inconsistent data patterns.</p>
<p>Each column in the tableplot represents some variable in the data table,
and each row is a “bin” containing a subset of the data. A statistic
such as the mean or total is computed for the values in a bin and is
displayed as a bar (for continuous variables) or as a stacked bar for
categorical variables.</p>
<div class="figure" style="text-align: center"><span id="fig:fig10-7"></span>
<img src="ChapterError/figures/fig10-7.png" alt="Comparison of tableplots for the Dutch Structural Business Statistics Survey for five variables before and after editing. Row bins with high missing and unknown numeric values are represented by lighter colored bars" width="70%" />
<p class="caption">
Figure 10.7: Comparison of tableplots for the Dutch Structural Business Statistics Survey for five variables before and after editing. Row bins with high missing and unknown numeric values are represented by lighter colored bars
</p>
</div>

<p>The sequence of steps typically involved in producing a tableplot is as
follows:</p>
<ol style="list-style-type: decimal">
<li><p>Sort the records in the data set by the key variable.</p></li>
<li><p>Divide the sorted data set into <span class="math inline">\(B\)</span> bins containing the same number
of rows.</p></li>
<li><p>For continuous variables, compute the statistic to be compared
across variables for each row bin, say <span class="math inline">\(T_{b}\)</span>, for <span class="math inline">\(b = 1,\ldots ,B\)</span>, for each continuous variable, <span class="math inline">\(V\)</span>, ignoring missing
values. The level of missingness for <span class="math inline">\(V\)</span> may be represented by the
color or brightness of the bar. For categorical variables with <span class="math inline">\(K\)</span>
categories, compute the proportion in the <span class="math inline">\(k\)</span>th category, denoted by
<span class="math inline">\(P_{bk}\)</span>. Missing values are assigned to a new (<span class="math inline">\(K+1\)</span>)th category
(“missing”).</p></li>
<li><p>For continuous variables, plot the <span class="math inline">\(B\)</span> values <span class="math inline">\(T_{b}\)</span> as a bar
chart. For categorical variables, plot the <span class="math inline">\(B\)</span> proportions <span class="math inline">\(P_{bk}\)</span>
as astacked bar chart.</p></li>
</ol>
<p>Typically, <span class="math inline">\(T_{b}\)</span> is the mean, but other statistics such as the median
or range could be plotted if they aid in the outlier identification
process. For highly skewed distributions, Tennekes and de Jonge
<span class="citation">(Tennekes and Jonge <a href="#ref-tennekes2011top">2011</a>)</span> suggest transforming <span class="math inline">\(T_{b}\)</span> by the log function to
better capture the range of values in the data set. In that case,
negative values can be plotted as <span class="math inline">\(\log(-T_{b})\)</span> to the left of the
origin and zero values can be plotted on the origin line. For
categorical variables, each bar in the stack should be displayed using
contrasting colors so that the divisions between categories are
apparent.</p>
<p>Tableplots appear to be well suited for studying the distributions of
variable values, the correlation between variables, and the occurrence
and selectivity of missing values. Because they can help visualize
massive, multivariate data sets, they seem particularly well suited for
big data. Currently, the R implementation of tableplot is limited to <span class="math inline">\(2\)</span>
billion records.</p>
<p>The tableplot in Figure <a href="chap-errors.html#fig:fig10-7">10.7</a> is taken from Tennekes and de Jonge
<span class="citation">(Tennekes and Jonge <a href="#ref-tennekes2011top">2011</a>)</span> for the annual Dutch Structural Business Statistics
survey, a survey of approximately 58,000 business units annually. Topics
covered in the questionnaire include turnover, number of employed
persons, total purchases, and financial results. Figure
<a href="chap-errors.html#fig:fig10-7">10.7</a> was
created by sorting on the first column, viz., log(turnover), and
dividing the 57,621 observed units into 100 bins, so that each row bin
contains approximately 576 records. To aid the comparisons between
unedited and edited data, the two tableplots are displayed side by side,
with the unedited graph on the left and the edited graph on the right.
All variables were transformed by the log function.</p>
<p>The unedited tableplot reveals that all four of the variables in the
comparison with log(turnover) show some distortion by large values for
some row bins. In particular, log(employees) has some fairly large
nonconforming bins with considerable discrepancies. In addition, that
variable suffers from a large number of missing values, as indicated by
the brightness of the bar color. All in all, there are obvious data
quality issues in the unprocessed data set for all four of these
variables that should be dealt with in the subsequent processing steps.</p>
<p>The edited tableplot reveals the effect of the data checking and editing
strategy used in the editing process. Notice the much darker color for
the number of employees for the graph on the left compared to same graph
on the right. In addition, the lack of data in the lowest part of the
turnover column has been somewhat improved. The distributions for the
graph on the right appear smoother and are less jagged.</p>
</div>
<div id="sec:10-6" class="section level2">
<h2><span class="header-section-number">10.6</span> Summary</h2>
<p>This review of big data errors should dispel the misconception that the
volume of the data can somehow compensate for other data deficiencies.
To think otherwise is big data “hubris,” according to Lazer et
al. <span class="citation">(Lazer et al. <a href="#ref-lazer2014parable">2014</a>)</span>. In fact, the errors in big data are at least as
severe and impactful for data analysis as are the errors in designed
data collections. For the latter, there is a vast literature that
developed during the last century on mitigating and adjusting for the
error effects on data analysis. But for the former, the corresponding
literature is still in its infancy. While the survey literature can form
a foundation for the treatment of errors in big data, its utility is
also quite limited because, unlike for smaller data sets, it is often
quite difficult to eliminate all but the most egregious errors from big
data due to its size. Indeed, volume, velocity, and variety may be
regarded as the three curses of big data veracity.</p>
<p>This chapter considers only a few types of data analysis that are common
in big data analytics: classification, correlation, and regression
analysis. We show that even when there is veracity, big data analytics
can result in poor inference as a result of the three Vs. These issues
include noise accumulation, spurious correlations, and incidental
endogeneity. However, relaxing the assumption of veracity compounds
these inferential issues and adds many new ones. Analytics can suffer in
unpredictable ways.</p>
<p>One solution we propose is to clean up the data before analysis. Another
option that was not discussed is the possibility of using analytical
techniques that attempt to model errors and compensate for them in the
analysis. Such techniques include the use of latent class analysis for
classification error <span class="citation">(Biemer <a href="#ref-biemer2011latent">2011</a>)</span>, multilevel modeling of
systematic errors from multiple sources <span class="citation">(Hox <a href="#ref-hox2010multilevel">2010</a>)</span>, and
Bayesian statistics for partitioning massive data sets across multiple
machines and then combining the results
<span class="citation">(Ibrahim and Chen <a href="#ref-ibrahim2000power">2000</a>; Scott et al. <a href="#ref-scott2013bayes">2013</a>)</span>.</p>
<p>The BDTE perspective can be helpful when we consider using a big data
source. In the end, it is the responsibility of big data analysts to be
aware of the many limitations of the data and to take the necessary
steps to limit the effects of big data error on analytical results.</p>
<p>Finally, we note that the BDTE paradigm focuses on the accuracy
dimension of Total Quality (writ large). Accuracy may also impinge on
other quality dimensions such as timeliness, comparability, coherence,
and relevance that we have not considered in this chapter. However,
those other dimensions are equally important to some consumers of big
data analytics. For example, timeliness often competes with accuracy
because achieving acceptable levels of the latter often requires greater
expenditures of resources and time. In fact, some consumers prefer
analytic results that are less accurate for the sake of timeliness.
Biemer and Lyberg <span class="citation">(Biemer and Lyberg <a href="#ref-biemer2003">2003</a>)</span> discuss these and other issues in some
detail.</p>

</div>
<div id="resources-5" class="section level2">
<h2><span class="header-section-number">10.7</span> Resources</h2>
<p>The American Association of Public Opinion Research has a number of
resources on its website <span class="citation">(AAPOR, <a href="#ref-aaporWeb">n.d.</a>)</span>. See, in particular, its report on
big data <span class="citation">(Japec et al. <a href="#ref-japec2015big">2015</a>)</span>.</p>
<p>The <em>Journal of Official Statistics</em> <span class="citation">(JOS, <a href="#ref-JOSweb">n.d.</a>)</span> is a standard resource
with many relevant articles. There is also an annual international
conference on the total survey error framework, supported by major
survey organizations <span class="citation">(TSE15, <a href="#ref-TSEweb">n.d.</a>)</span>.</p>


</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-japec2015big">
<p>Japec, Lilli, Frauke Kreuter, Marcus Berg, Paul Biemer, Paul Decker, Cliff Lampe, Julia Lane, Cathy O’Neil, and Abe Usher. 2015. “Big Data in Survey Research: AAPOR Task Force Report.” <em>Public Opinion Quarterly</em> 79 (4). AAPOR: 839–80.</p>
</div>
<div id="ref-groves2004survey">
<p>Groves, Robert M. 2004. <em>Survey Errors and Survey Costs</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-biemer2003">
<p>Biemer, Paul P., and Lars E. Lyberg. 2003. <em>Introduction to Survey Quality</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-biemer2010total">
<p>Biemer, Paul P. 2010. “Total Survey Error: Design, Implementation, and Evaluation.” <em>Public Opinion Quarterly</em> 74 (5). AAPOR: 817–48.</p>
</div>
<div id="ref-SDV2015">
<p>Stephens-Davidowitz, S., and H. Varian. 2015. “A Hands-on Guide to Google Data.” <a href="http://people.ischool.berkeley.edu/~hal/Papers/2015/primer.pdf" class="uri">http://people.ischool.berkeley.edu/~hal/Papers/2015/primer.pdf</a>. Accessed October 12.</p>
</div>
<div id="ref-kreuter201412">
<p>Kreuter, Frauke, and Roger D. Peng. 2014. “Extracting Information from Big Data: Issues of Measurement, Inference, and Linkage.” In <em>Privacy, Big Data, and the Public Good: Frameworks for Engagement</em>, edited by Julia Lane, Victoria Stodden, Stefan Bender, and Helen Nissenbaum, 257–75. Cambridge University Press.</p>
</div>
<div id="ref-wallgren2007register">
<p>Wallgren, Anders, and Britt Wallgren. 2007. <em>Register-Based Statistics: Administrative Data for Statistical Purposes</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-little2014statistical">
<p>Little, Roderick J. A., and Donald B Rubin. 2014. <em>Statistical Analysis with Missing Data</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-rubin1976">
<p>Rubin, Donald B. 1976. “Inference and Missing Data.” <em>Biometrika</em> 63: 581–92.</p>
</div>
<div id="ref-schafer1997analysis">
<p>Schafer, Joseph L. 1997. <em>Analysis of Incomplete Multivariate Data</em>. CRC Press.</p>
</div>
<div id="ref-schafer2002missing">
<p>Schafer, Joseph L, and John W Graham. 2002. “Missing Data: Our View of the State of the Art.” <em>Psychological Methods</em> 7 (2). American Psychological Association: 147.</p>
</div>
<div id="ref-butler2013google">
<p>Butler, Declan. 2013. “When Google Got Flu Wrong.” <em>Nature</em> 494 (7436): 155.</p>
</div>
<div id="ref-lazer2014parable">
<p>Lazer, David, Ryan Kennedy, Gary King, and Alessandro Vespignani. 2014. “The Parable of Google Flu: Traps in Big Data Analysis.” <em>Science</em> 343 (14 March).</p>
</div>
<div id="ref-thompson2006epidemiology">
<p>Thompson, William W., Lorraine Comanor, and David K. Shay. 2006. “Epidemiology of Seasonal Influenza: Use of Surveillance Data and Statistical Models to Estimate the Burden of Disease.” <em>Journal of Infectious Diseases</em> 194 (Supplement 2). Oxford University Press: S82–S91.</p>
</div>
<div id="ref-allison2001missing">
<p>Allison, Paul D. 2001. <em>Missing Data</em>. Sage Publications.</p>
</div>
<div id="ref-fan2014challenges">
<p>Fan, Jianqing, Fang Han, and Han Liu. 2014. “Challenges of Big Data Analysis.” <em>National Science Review</em> 1 (2). Oxford University Press: 293–314.</p>
</div>
<div id="ref-spurious">
<p>Vigen, Tyler. n.d. “Spurious Correlations.” <a href="http://www.tylervigen.com/spurious-correlations" class="uri">http://www.tylervigen.com/spurious-correlations</a>. Accessed February 1, 2016.</p>
</div>
<div id="ref-spurious2">
<p>Vigen, Tyler. 2015. <em>Spurious Correlations</em>. Hachette Books.</p>
</div>
<div id="ref-sies1988new">
<p>Sies, Helmut. 1988. “A New Parameter for Sex Education.” <em>Nature</em> 332 (495). Nature Publishing Group.</p>
</div>
<div id="ref-fan2014endogeneity">
<p>Fan, Jianqing, and Yuan Liao. 2014. “Endogeneity in High Dimensions.” <em>Annals of Statistics</em> 42 (3). NIH Public Access: 872.</p>
</div>
<div id="ref-stock2002forecasting">
<p>Stock, James H., and Mark W. Watson. 2002. “Forecasting Using Principal Components from a Large Number of Predictors.” <em>Journal of the American Statistical Association</em> 97 (460). Taylor &amp; Francis: 1167–79.</p>
</div>
<div id="ref-fan2009ultrahigh">
<p>Fan, Jianqing, Richard Samworth, and Yichao Wu. 2009. “Ultrahigh Dimensional Feature Selection: Beyond the Linear Model.” <em>Journal of Machine Learning Research</em> 10. JMLR. org: 2013–38.</p>
</div>
<div id="ref-HallMiller2009">
<p>Hall, P., and H. Miller. 2009. “Using Generalized Correlation to Effect Variable Selection in Very High Dimensional Problems.” <em>Journal of Computational and Graphical Statistics</em> 18: 533–50.</p>
</div>
<div id="ref-FanLiao2012">
<p>Fan, Jianqing, and Yuan Liao. 2012. “Endogeneity in Ultrahigh Dimension.” Princeton University.</p>
</div>
<div id="ref-biemer1997review">
<p>Biemer, Paul P., and Dennis Trewin. 1997. “A Review of Measurement Error Effects on the Analysis of Survey Data.” In <em>Survey Measurement and Process Quality</em>, edited by L. Lyberg, P. Biemer, M. Collins, E. De Leeuw, C. Dippo, N. Schwarz, and D. Trewin, 601–32. John Wiley &amp; Sons.</p>
</div>
<div id="ref-BiemerStokes1991">
<p>Biemer, Paul P., and S. L. Stokes. 1991. “Approaches to Modeling Measurement Error.” In <em>Measurement Errors in Surveys</em>, edited by Paul P. Biemer, R. Groves, L. Lyberg, N. Mathiowetz, and S. Sudman, 54–68. John Wiley.</p>
</div>
<div id="ref-biemer2011latent">
<p>Biemer, Paul P. 2011. <em>Latent Class Analysis of Survey Error</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-cochran1968errors">
<p>Cochran, William G. 1968. “Errors of Measurement in Statistics.” <em>Technometrics</em> 10 (4). Taylor &amp; Francis Group: 637–66.</p>
</div>
<div id="ref-fuller1991regression">
<p>Fuller, Wayne A. 1991. “Regression Estimation in the Presence of Measurement Error.” In <em>Measurement Errors in Surveys</em>, edited by Paul P. Biemer, Robert M. Groves, Lars E. Lyberg, Nancy A. Mathiowetz, and Seymour Sudman, 617–35. John Wiley &amp; Sons.</p>
</div>
<div id="ref-chai1971correlated">
<p>Chai, John J. 1971. “Correlated Measurement Errors and the Least Squares Estimator of the Regression Coefficient.” <em>Journal of the American Statistical Association</em> 66 (335). Taylor &amp; Francis Group: 478–83.</p>
</div>
<div id="ref-hox2010multilevel">
<p>Hox, Joop. 2010. <em>Multilevel Analysis: Techniques and Applications</em>. Routledge.</p>
</div>
<div id="ref-chandola2009anomaly">
<p>Chandola, Varun, Arindam Banerjee, and Vipin Kumar. 2009. “Anomaly Detection: A Survey.” <em>ACM Computing Surveys</em> 41 (3). ACM: 15.</p>
</div>
<div id="ref-petrakos2004new">
<p>Petrakos, George, Claudio Conversano, Gregory Farmakis, Francesco Mola, Roberta Siciliano, and Photis Stavropoulos. 2004. “New Ways of Specifying Data Edits.” <em>Journal of the Royal Statistical Society, Series A</em> 167 (2). Wiley Online Library: 249–74.</p>
</div>
<div id="ref-granquist1997editing">
<p>Granquist, Leopold, and John G. Kovar. 1997. “Editing of Survey Data: How Much Is Enough?” In <em>Survey Measurement and Process Quality</em>, edited by L. Lyberg, P. Biemer, M. Collins, E. De Leeuw, C. Dippo, N. Schwarz, and D. Trewin, 415–35. John Wiley &amp; Sons.</p>
</div>
<div id="ref-de2011handbook">
<p>De Waal, Ton, Jeroen Pannekoek, and Sander Scholtus. 2011. <em>Handbook of Statistical Data Editing and Imputation</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-natarajan2010data">
<p>Natarajan, Kalaivany, Jiuyong Li, and Andy Koronios. 2010. <em>Data Mining Techniques for Data Cleaning</em>. Springer.</p>
</div>
<div id="ref-Clarke2014">
<p>Clarke, Claire. 2014. “Editing Big Data with Machine Learning Methods.” Paper presented at the Australian Bureau of Statistics Symposium, Canberra.</p>
</div>
<div id="ref-duan2009cluster">
<p>Duan, Lian, Lida Xu, Ying Liu, and Jun Lee. 2009. “Cluster-Based Outlier Detection.” <em>Annals of Operations Research</em> 168 (1). Springer: 151–68.</p>
</div>
<div id="ref-he2003discovering">
<p>He, Zengyou, Xiaofei Xu, and Shengchun Deng. 2003. “Discovering Cluster-Based Local Outliers.” <em>Pattern Recognition Letters</em> 24 (9). Elsevier: 1641–50.</p>
</div>
<div id="ref-johnson1991tree">
<p>Johnson, Brian, and Ben Shneiderman. 1991. “Tree-Maps: A Space-Filling Approach to the Visualization of Hierarchical Information Structures.” In <em>Proceedings of the Ieee Conference on Visualization</em>, 284–91. IEEE.</p>
</div>
<div id="ref-shneiderman1992tree">
<p>Shneiderman, Ben. 1992. “Tree Visualization with Tree-Maps: 2-D Space-Filling Approach.” <em>ACM Transactions on Graphics</em> 11 (1). ACM: 92–99.</p>
</div>
<div id="ref-tennekes2011top">
<p>Tennekes, Martijn, and Edwin de Jonge. 2011. “Top-down Data Analysis with Treemaps.” In <em>Proceedings of the International Conference on Imaging Theory and Applications and International Conference on Information Visualization Theory and Applications</em>, 236–41. SciTePress.</p>
</div>
<div id="ref-tennekes2013visualizing">
<p>Tennekes, Martijn, Edwin de Jonge, and Piet J. H. Daas. 2013. “Visualizing and Inspecting Large Datasets with Tableplots.” <em>Journal of Data Science</em> 11 (1). ???????? 43–58.</p>
</div>
<div id="ref-Tennekes2012">
<p>Tennekes, M., E. de Jonge, and P. Daas. 2012. “Innovative Visual Tools for Data Editing.” Presented at the United Nations Economic Commission for Europe Work Session on Statistical Data. Available online at <a href="http://www.pietdaas.nl/beta/pubs/pubs/30_Netherlands.pdf" class="uri">http://www.pietdaas.nl/beta/pubs/pubs/30_Netherlands.pdf</a>.</p>
</div>
<div id="ref-puts2015finding">
<p>Puts, Marco, Piet Daas, and Ton de Waal. 2015. “Finding Errors in Big Data.” <em>Significance</em> 12 (3). Wiley Online Library: 26–29.</p>
</div>
<div id="ref-malik2010interactive">
<p>Malik, Waqas Ahmed, Antony Unwin, and Alexander Gribov. 2010. “An Interactive Graphical System for Visualizing Data Quality–Tableplot Graphics.” In <em>Classification as a Tool for Research</em>, 331–39. Springer.</p>
</div>
<div id="ref-ibrahim2000power">
<p>Ibrahim, Joseph G., and Ming-Hui Chen. 2000. “Power Prior Distributions for Regression Models.” <em>Statistical Science</em> 15 (1). JSTOR: 46–60.</p>
</div>
<div id="ref-scott2013bayes">
<p>Scott, Steven L., Alexander W. Blocker, Fernando V. Bonassi, H. Chipman, E. George, and R. McCulloch. 2013. “Bayes and Big Data: The consensus Monte Carlo Algorithm.” In <em>EFaBBayes 250 Conference</em>. Vol. 16.</p>
</div>
<div id="ref-aaporWeb">
<p>AAPOR. n.d. “American Association for Public Opinion Research Website.” <a href="http://www.aapor.org" class="uri">http://www.aapor.org</a>. Accessed February 1, 2016.</p>
</div>
<div id="ref-JOSweb">
<p>JOS. n.d. “Journal of Official Statistics Website.” Journal of Statistics Sweden; <a href="http://www.jos.nu" class="uri">http://www.jos.nu</a>. Accessed February 1, 2016.</p>
</div>
<div id="ref-TSEweb">
<p>TSE15. n.d. “2015 International Total Survey Error Conference Website.” <a href="https://www.tse15.org" class="uri">https://www.tse15.org</a>. Accessed February 1, 2016.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="56">
<li id="fn56"><p>See the discussion in
Section 1.3.<a href="chap-errors.html#fnref56" class="footnote-back">↩</a></p></li>
<li id="fn57"><p>The volume, velocity, and
variety of big data make it
challenging to analyze because of noise accumulation, spurious correlations,
and incidental endogeneity.
But, the three Vs are also
the scourge of veracity—
i.e., errors in big data.<a href="chap-errors.html#fnref57" class="footnote-back">↩</a></p></li>
<li id="fn58"><p>Data errors further complicate analysis and exacerbate the analytical problems. There are essentially
three solutions: prevention,
remediation, and the choice
of analysis methodology.<a href="chap-errors.html#fnref58" class="footnote-back">↩</a></p></li>
<li id="fn59"><p>For example, Title 13 in
the US code is explicit in
terms of data statutory protections and—as you will
see in the next chapter—
can have substantial impact
on the quality of subsequent
inference.<a href="chap-errors.html#fnref59" class="footnote-back">↩</a></p></li>
</ol>
</div>
<div id="disqus_thread"></div>
<script>
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = '//big-data-and-social-science.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the
<a href="https://disqus.com/?ref_noscript">
  comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<a href="chap-viz.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chap-privacy.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/Coleridge-Initiative/big-data-and-social-science/edit/master/10-ChapterError.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
